<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="">
<meta name="description" content="Have you ever dreamed of building AI-native applications that can leverage the power of large language models (LLMs) without relying on expensive cloud services or complex infrastructure? If so, you’re not alone. Many developers are looking for ways to create and deploy AI-powered solutions that are fast, flexible, and cost-effective, or just experiment locally. In this blog post, I’m going to show you how you can use three amazing tools and a language model like gpt4all to : LangChain, LocalAI, and Chroma." />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="https://mudler.pm/posts/localai-question-answering/" />


    <title>
        
            Question Answering on Documents locally with LangChain, LocalAI, Chroma, and GPT4All :: Mudler blog  — Place where I write about stuff
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="/main.3d89495748aff88bbfbb2a665d8e1260b3aa63445cd77cc810fb6477aba343dd.css">




    <script src="https://kit.fontawesome.com/bdd86d0a63.js" crossorigin="anonymous"></script>

    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="">
    <link rel="shortcut icon" href="/favicon.ico">
    <meta name="msapplication-TileColor" content="">


<meta itemprop="name" content="Question Answering on Documents locally with LangChain, LocalAI, Chroma, and GPT4All">
<meta itemprop="description" content="Have you ever dreamed of building AI-native applications that can leverage the power of large language models (LLMs) without relying on expensive cloud services or complex infrastructure? If so, you’re not alone. Many developers are looking for ways to create and deploy AI-powered solutions that are fast, flexible, and cost-effective, or just experiment locally. In this blog post, I’m going to show you how you can use three amazing tools and a language model like gpt4all to : LangChain, LocalAI, and Chroma."><meta itemprop="datePublished" content="2023-05-12T00:00:00+00:00" />
<meta itemprop="dateModified" content="2023-05-12T00:00:00+00:00" />
<meta itemprop="wordCount" content="1979"><meta itemprop="image" content="https://mudler.pm"/>
<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://mudler.pm"/>

<meta name="twitter:title" content="Question Answering on Documents locally with LangChain, LocalAI, Chroma, and GPT4All"/>
<meta name="twitter:description" content="Have you ever dreamed of building AI-native applications that can leverage the power of large language models (LLMs) without relying on expensive cloud services or complex infrastructure? If so, you’re not alone. Many developers are looking for ways to create and deploy AI-powered solutions that are fast, flexible, and cost-effective, or just experiment locally. In this blog post, I’m going to show you how you can use three amazing tools and a language model like gpt4all to : LangChain, LocalAI, and Chroma."/>



    <meta property="og:title" content="Question Answering on Documents locally with LangChain, LocalAI, Chroma, and GPT4All" />
<meta property="og:description" content="Have you ever dreamed of building AI-native applications that can leverage the power of large language models (LLMs) without relying on expensive cloud services or complex infrastructure? If so, you’re not alone. Many developers are looking for ways to create and deploy AI-powered solutions that are fast, flexible, and cost-effective, or just experiment locally. In this blog post, I’m going to show you how you can use three amazing tools and a language model like gpt4all to : LangChain, LocalAI, and Chroma." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mudler.pm/posts/localai-question-answering/" /><meta property="og:image" content="https://mudler.pm"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-05-12T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-05-12T00:00:00+00:00" /><meta property="og:site_name" content="Mudler blog" />







    <meta property="article:published_time" content="2023-05-12 00:00:00 &#43;0000 UTC" />





<link href="https://mudler.pmfeed.json" rel="alternate"
    type="application/json" title="Mudler blog" />






    </head>

    
        <body>
    
    
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text">mudler blog</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <style>

  
.search-container {
    position: relative;
    display: flex;
    align-items: center;
    border-radius: 20px;
    padding: 5px;
    background-color: rgba(255, 255, 255, 0.5);
  }

  .search-box {
    border: none;
    background-color: transparent;
    color: #333;
    flex: 1;
    padding: 8px 8px 8px 28px;
    border-radius: 20px;
    font-size: 16px;
  }

  .search-button {
    border: none;
    background-color: #fff;
    color: #333;
    padding: 8px;
    border-radius: 20px;
  }

  .search-icon {
    position: absolute;
    left: 8px;
    top: 50%;
    transform: translateY(-50%);
    opacity: 0.7;
  }

  .search-icon i {
    font-size: 16px;
  }

  .search-button:hover {
    background-color: #555;
    color: #fff;
    cursor: pointer;
  }
  </style>

<nav class="menu">
    <ul class="menu__inner">
            
            <div class="submenu">
                <li class="dropdown">
                    <a href="/about">
                        About
                    </a>
                </li>
            </div>
            
        
            
            <div class="submenu">
                <li class="dropdown">
                    <a href="/posts">
                        Blog
                    </a>
                </li>
            </div>
            
        

    
        <div class="submenu">
            <form action="/search" method="GET">
                <div class="search-container">
                  <div class="search-icon">
                    <i class="fas fa-search"></i>
                  </div>
                  <input type="search" name="q" id="search-query" placeholder="Search..." class="search-box">
                  <button type="submit" class="search-button">
                    <i class="fas fa-search"></i>
                  </button>
                </div>
              </form>
        </div>
    </ul>

    
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            
                <span class="theme-toggle not-selectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
   <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
   3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
   13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
 </svg></span>
        </span>
    </span>
</header>


            <div class="content">
                
  <main class="post">

    <div class="post-info">
      <p>
        <i class="fa-solid fa-clock"></i>
        10 minutes
        <i class="fa-solid fa-clipboard"></i>
        1979 Words
        
      </p>
      <p>
        <i class="fa-solid fa-calendar-days"></i>
        
          2023-05-12 00:00
        

         
          
        
      </p>
      <p>
      
      
      </p>
    </div>

    <article>
      <h1 class="post-title">
        <a href="https://mudler.pm/posts/localai-question-answering/">Question Answering on Documents locally with LangChain, LocalAI, Chroma, and GPT4All</a>
      </h1>

      

      

      

      <div class="post-content">
        <p>Have you ever dreamed of building AI-native applications that can leverage the power of large language models (LLMs) without relying on expensive cloud services or complex infrastructure? If so, you’re not alone. Many developers are looking for ways to create and deploy AI-powered solutions that are fast, flexible, and cost-effective, or just experiment locally. In this blog post, I’m going to show you how you can use three amazing tools and a language model like gpt4all to : LangChain, LocalAI, and Chroma.</p>
<ul>
<li>
<p><a href="https://www.trychroma.com/">Chroma</a> is a vector store and embedding database designed for AI workloads.</p>
</li>
<li>
<p><a href="https://gpt4all.io/index.html">Gpt4all</a> is an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue.</p>
</li>
<li>
<p><a href="https://github.com/go-skynet/LocalAI">LocalAI</a> is a self-hosted, community-driven, local OpenAI-compatible API that can run on CPU with consumer-grade hardware. It enables you to run models locally or on-prem without the need for internet connectivity or external servers.</p>
</li>
<li>
<p><a href="https://python.langchain.com/en/latest/">LangChain</a> is a modular and flexible framework for developing AI-native applications using LLMs.</p>
</li>
</ul>
<p>Together, these four tools form a powerful combination that can help you create and deploy AI-native applications with ease and efficiency. In the following sections, I’ll show you how to use them in practice to do question answering on a document.</p>
<h2 id="localai">LocalAI</h2>
<p><img src="https://user-images.githubusercontent.com/2420543/233147843-88697415-6dbf-4368-a862-ab217f9f7342.jpeg" alt=""></p>
<p><a href="https://github.com/go-skynet/LocalAI">LocalAI</a> is a drop-in replacement REST API compatible with OpenAI for local CPU inferencing. It allows you to run models locally or on-prem with consumer grade hardware, supporting multiple models families. LocalAI is a community-driven project, focused on making the AI accessible to anyone.</p>
<p>LocalAI uses C++ bindings for optimizing speed and performance. It is based on <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> gpt4all, rwkv.cpp, ggml, whisper.cpp for audio transcriptions, and bert.cpp for embedding. LocalAI also supports GPT4ALL-J which is licensed under Apache 2.0, and MosaicLM PT models which are also usable for commercial applications. (see <a href="https://github.com/go-skynet/LocalAI#model-compatibility-table">here the table of supported models</a>)</p>
<p>To use LocalAI, you need to install it on your machine and run it as a service, or either on the cloud or in a dedicated environment. You can then use the same API endpoints as OpenAI to interact with the models. For example, you can use the <code>/v1/models</code> endpoint to list the available models, or the <code>/v1/completions</code> endpoint to generate text completions, but computation is executed locally, with CPU-compatible models. You can download models from <a href="https://gpt4all.io/index.html">Gpt4all</a>.</p>
<p>LocalAI also supports various ranges of configuration and prompt templates, which are predefined prompts that can help you generate specific outputs with the models. For example, you can use the summarizer template to generate summaries of texts, or the sentiment-analyzer template to analyze the sentiment of texts. You can find more <a href="https://github.com/go-skynet/LocalAI/tree/master/examples">examples</a> and <a href="https://github.com/go-skynet/LocalAI/tree/master/prompt-templates">prompt templates</a> in the <a href="https://github.com/go-skynet/LocalAI">LocalAI</a> repository.</p>
<h2 id="langchain-and-chroma">Langchain and Chroma</h2>
<p><img src="https://blog.langchain.dev/content/images/size/w1000/2023/02/langchain-chroma-light.png" alt=""></p>
<p><a href="https://www.trychroma.com/">Chroma</a> is a vector store and embedding database designed for AI workloads. It allows you to store and work with embeddings, which are the AI-native way to represent any kind of data. It also offers high performance and flexibility for working with different types of embeddings and algorithms. Chroma was founded to build tools that leverage the power of embeddings. Embeddings are the perfect fit for working with all kinds of AI-powered tools and algorithms, such as LLMs, semantic search, example selection, and more.</p>
<p><a href="https://python.langchain.com/en/latest/">LangChain</a> is a modular and flexible framework for developing AI-native applications using LLMs. It allows you to easily prototype and experiment with different models, data sources, and use cases, such as chat bots, question answering services, and agents. LangChain also provides a rich ecosystem of integrations with other tools and platforms, such as Notion, PDFs, ClearML, CerebriumAI, and more.</p>
<p>Together, those are powerful and convenient tools that can help you store and work with embeddings in a simple and efficient way. It can also help you enhance your LLM applications with pluggable knowledge, facts, and skills. In the next section, I’ll show you how to use LangChain and Chroma together with LocalAI to create and deploy AI-native applications locally.</p>
<h2 id="question-answering-with-localai-chromadb-and-langchain">Question answering with LocalAI, ChromaDB and Langchain</h2>
<p>In this example, I&rsquo;ll show you how to use <code>LocalAI</code> with the <code>gpt4all</code> models with <code>LangChain</code> and <code>Chroma</code> to enable question answering on a set of documents. We’ll use the state of the union speeches from different US presidents as our data source, and we’ll use the <code>ggml-gpt4all-j</code> model served by LocalAI to generate answers. Note, you can use any model compatible with <code>LocalAI</code>.</p>
<p>To run this example, you’ll need to have LocalAI, LangChain, and Chroma installed on your machine. You’ll also need to download the models and the data files from the links provided below. Alternatively, you can use the docker-compose file to start the LocalAI API and the Chroma service with the models and data already loaded.</p>
<p>The example consists of two steps: creating a storage and querying the storage. In the first step, we’ll use LangChain and Chroma to create a local vector database from our document set. This will allow us to perform semantic search on the documents using embeddings. In the second step, we’ll use LangChain and LocalAI to query the storage using natural language questions. We’ll use the gpt4all model served by LocalAI using the <em>OpenAI</em> api and python client to generate answers based on the most relevant documents. The key aspect here is that we will configure the python client to use the LocalAI API endpoint instead of OpenAI.</p>
<h3 id="step-1-start-localai">Step 1: Start LocalAI</h3>
<p>To start LocalAI, we can either build it locally or use <code>docker-compose</code>. Note the steps here are for <code>Linux</code> machines. If you are on <code>Mac</code> you need to build the binary manually:</p>
<pre tabindex="0"><code># Clone LocalAI
git clone https://github.com/go-skynet/LocalAI

cd LocalAI/examples/langchain-chroma

# Download models
wget https://huggingface.co/skeskinen/ggml/resolve/main/all-MiniLM-L6-v2/ggml-model-q4_0.bin -O models/bert
wget https://gpt4all.io/models/ggml-gpt4all-j.bin -O models/ggml-gpt4all-j

docker-compose up
</code></pre><p>This will start the LocalAI server locally, with the models required for embeddings (<code>bert</code>) and for question answering (<code>gpt4all</code>).</p>
<p>Note: The example contains a <code>models</code> folder with the configuration for <code>gpt4all</code> and the <code>embeddings</code> models already prepared. LocalAI will map gpt4all to <code>gpt-3.5-turbo</code> model, and <code>bert</code> to the embeddings endpoints.</p>
<h3 id="step-2-create-a-vector-database">Step 2: Create a vector database</h3>
<p>To create a vectore database, we’ll use a script which uses LangChain and Chroma to create a collection of documents and their embeddings. The script takes a text file as input, where each line is a document. In our case, we’ll use the <code>state_of_the_union.txt</code> file, which we will use to ask it questions later for.</p>
<p>Download the data:</p>
<pre tabindex="0"><code># Download data used for training
wget https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt
</code></pre><p>The python OpenAI client allows to set an API key and a API target host with those two environment variables:</p>
<ul>
<li><code>OPENAI_API_BASE</code>: The base URL of the OpenAI API. If we run LocalAI locally, we set it to http://localhost:8080/v1</li>
<li><code>OPENAI_API_KEY</code>: The API key for the OpenAI API. We have to set it to something, but doesn&rsquo;t really matter</li>
</ul>
<p>Our script will do the following:</p>
<ul>
<li>It imports the necessary modules from the langchain library and the os module.</li>
<li>It sets the base_path variable to the OpenAI API endpoint, which is used to access the OpenAI embeddings model.</li>
<li>It creates a <code>TextLoader</code> object that loads the text file and returns a list of documents (each document is a string).</li>
<li>It creates a <code>CharacterTextSplitter</code> object that splits each document into smaller chunks of 300 characters with an overlap of 70 characters.
<ul>
<li>Alternatively, it can use a <code>TokenTextSplitter</code> object that splits each document into tokens (words or punctuation marks) or <code>RecursiveCharacterTextSplitter</code>.</li>
</ul>
</li>
<li>It creates an OpenAIEmbeddings object that uses the <code>text-embedding-ada-002</code> model to generate embeddings (numeric representations) for each chunk of text. Since we set <code>OPENAI_API_BASE</code> it will use LocalAI instead.</li>
<li>It creates a Chroma object that stores the embeddings in a vector database. It also specifies a persist_directory where the embeddings are saved on disk.</li>
<li>It calls the persist method to save the embeddings.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.vectorstores <span style="color:#f92672">import</span> Chroma
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.embeddings <span style="color:#f92672">import</span> OpenAIEmbeddings
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.text_splitter <span style="color:#f92672">import</span> RecursiveCharacterTextSplitter,TokenTextSplitter,CharacterTextSplitter
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.llms <span style="color:#f92672">import</span> OpenAI
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.chains <span style="color:#f92672">import</span> VectorDBQA
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.document_loaders <span style="color:#f92672">import</span> TextLoader
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>base_path <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>environ<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#39;OPENAI_API_BASE&#39;</span>, <span style="color:#e6db74">&#39;http://localhost:8080/v1&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load and process the text</span>
</span></span><span style="display:flex;"><span>loader <span style="color:#f92672">=</span> TextLoader(<span style="color:#e6db74">&#39;state_of_the_union.txt&#39;</span>)
</span></span><span style="display:flex;"><span>documents <span style="color:#f92672">=</span> loader<span style="color:#f92672">.</span>load()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>text_splitter <span style="color:#f92672">=</span> CharacterTextSplitter(chunk_size<span style="color:#f92672">=</span><span style="color:#ae81ff">300</span>, chunk_overlap<span style="color:#f92672">=</span><span style="color:#ae81ff">70</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e">#text_splitter = TokenTextSplitter()</span>
</span></span><span style="display:flex;"><span>texts <span style="color:#f92672">=</span> text_splitter<span style="color:#f92672">.</span>split_documents(documents)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Embed and store the texts</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Supplying a persist_directory will store the embeddings on disk</span>
</span></span><span style="display:flex;"><span>persist_directory <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;db&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>embedding <span style="color:#f92672">=</span> OpenAIEmbeddings(model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;text-embedding-ada-002&#34;</span>)
</span></span><span style="display:flex;"><span>vectordb <span style="color:#f92672">=</span> Chroma<span style="color:#f92672">.</span>from_documents(documents<span style="color:#f92672">=</span>texts, embedding<span style="color:#f92672">=</span>embedding, persist_directory<span style="color:#f92672">=</span>persist_directory)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>vectordb<span style="color:#f92672">.</span>persist()
</span></span><span style="display:flex;"><span>vectordb <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span></code></pre></div><p>To run the script, execute the following commands:</p>
<pre tabindex="0"><code># Set environment variables
export OPENAI_API_BASE=http://localhost:8080/v1
export OPENAI_API_KEY=sk-

# Run store.py script
python store.py
</code></pre><p>After it finishes, a directory named <code>db</code> will be created with the vector index database.</p>
<h3 id="step-3-query-the-storage">Step 3: Query the storage</h3>
<p>Now we can query our vector database with <code>gpt4all</code>.</p>
<p>To query the storage, we’ll use a python script which uses LangChain and LocalAI to generate answers from natural language questions.</p>
<p>Our script will do the following:</p>
<ul>
<li>It imports the necessary modules from LangChain and os.</li>
<li>It calculate embeds the question using the OpenAIEmbeddings class, which uses the OpenAI API to generate embeddings for each text chunk. - However, since we set <code>OPENAI_API_BASE</code> it will use <strong>LocalAI</strong> instead. It also uses the Chroma class, which is a vector store that can persist the embeddings on disk for later use.</li>
<li>It creates a question answering system using the VectorDBQA class, which can query the vector store using natural language questions. It also uses the OpenAI class, which is a wrapper for the OpenAI API that can specify parameters such as temperature and model name. In this case, it uses the <code>gpt-3.5-turbo</code> model, and LocalAI is configured to redirect requests to the <code>gpt4all</code> model instead.</li>
<li>Finally, it runs a sample query on the question answering system, asking “What the president said about taxes ?” and prints the answer.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.vectorstores <span style="color:#f92672">import</span> Chroma
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.embeddings <span style="color:#f92672">import</span> OpenAIEmbeddings
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.llms <span style="color:#f92672">import</span> OpenAI
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.chains <span style="color:#f92672">import</span> VectorDBQA
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>base_path <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>environ<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#39;OPENAI_API_BASE&#39;</span>, <span style="color:#e6db74">&#39;http://localhost:8080/v1&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load and process the text</span>
</span></span><span style="display:flex;"><span>embedding <span style="color:#f92672">=</span> OpenAIEmbeddings()
</span></span><span style="display:flex;"><span>persist_directory <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;db&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Now we can load the persisted database from disk, and use it as normal. </span>
</span></span><span style="display:flex;"><span>vectordb <span style="color:#f92672">=</span> Chroma(persist_directory<span style="color:#f92672">=</span>persist_directory, embedding_function<span style="color:#f92672">=</span>embedding)
</span></span><span style="display:flex;"><span>qa <span style="color:#f92672">=</span> VectorDBQA<span style="color:#f92672">.</span>from_chain_type(llm<span style="color:#f92672">=</span>OpenAI(temperature<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, model_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;gpt-3.5-turbo&#34;</span>, openai_api_base<span style="color:#f92672">=</span>base_path), chain_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;stuff&#34;</span>, vectorstore<span style="color:#f92672">=</span>vectordb)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>query <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;What the president said about taxes ?&#34;</span>
</span></span><span style="display:flex;"><span>print(qa<span style="color:#f92672">.</span>run(query))
</span></span></code></pre></div><p>To run the script, execute the following commands:</p>
<pre tabindex="0"><code># Set environment variables
export OPENAI_API_BASE=http://localhost:8080/v1
export OPENAI_API_KEY=sk-

# Run query.py
python query.py
# President Trump recently stated during a press conference regarding tax reform legislation that &#34;we&#39;re getting rid of all these loopholes.&#34; He also mentioned that he wants to simplify the system further through changes such as increasing the standard deduction amount and making other adjustments aimed at reducing taxpayers&#39; overall burden.    
</code></pre><h2 id="conclusions">Conclusions:</h2>
<p>In this blog post, we showed you how to use LangChain and Chroma together with LocalAI to enable question answering on a set of documents. We used the state of the union speeches from different US presidents as our data source, and we used the ggml-gpt4all-j model from LocalAI to generate answers. We also used Chroma as a vector store and embedding database to perform semantic search on the documents using embeddings.</p>
<p>This is just one example of how you can use these three amazing tools to create and deploy AI-native applications with ease and efficiency. You can also use them for other use cases, such as chat bots, agents, summarizers, sentiment analyzers, and more. You can also use different models, data sources, and integrations with other tools and platforms.</p>
<p>We hope you enjoyed this blog post and learned something new. If you want to try it out for yourself, you can find the code and the instructions in <a href="https://github.com/go-skynet/LocalAI/tree/master/examples/langchain-chroma">this GitHub repo</a>. You can also find more resources and documentation for LangChain, LocalAI, and Chroma in the links below.</p>
<ul>
<li>Chroma: <a href="https://docs.trychroma.com/">https://docs.trychroma.com/</a></li>
<li>GPT4all: <a href="https://github.com/nomic-ai/gpt4all">https://github.com/nomic-ai/gpt4all</a></li>
<li>LangChain: <a href="https://python.langchain.com/en/latest/">https://python.langchain.com/en/latest/</a></li>
<li>LocalAI: <a href="https://github.com/go-skynet/LocalAI">https://github.com/go-skynet/LocalAI</a></li>
</ul>
<p>We’d love to hear your feedback and suggestions on how to improve these tools and make them more useful for you. You can join our Discord channels and chat with us and other developers:</p>
<h2 id="links">Links</h2>
<ul>
<li>Full example code: <a href="https://github.com/go-skynet/LocalAI/tree/master/examples/langchain-chroma">https://github.com/go-skynet/LocalAI/tree/master/examples/langchain-chroma</a></li>
<li>LocalAI examples: <a href="https://github.com/go-skynet/LocalAI/tree/master/examples">https://github.com/go-skynet/LocalAI/tree/master/examples</a></li>
<li>Github: <a href="https://github.com/go-skynet/LocalAI">https://github.com/go-skynet/LocalAI</a></li>
<li>Follow us on Twitter: <a href="https://twitter.com/LocalAI_API">https://twitter.com/LocalAI_API</a></li>
<li>Upvote on Hacker news: <a href="https://news.ycombinator.com/item?id=35726934">https://news.ycombinator.com/item?id=35726934</a></li>
<li>Join our Discord: <a href="https://discord.gg/uJAeKSAGDy">https://discord.gg/uJAeKSAGDy</a></li>
</ul>
<p>Thank you for reading and happy coding!</p>

      </div>
    </article>
      <hr />
    <div class="post-info">
      
    <div class="pagination">
        
        <div class="pagination__title">
            <span class="pagination__title-h">Read other posts</span>
            <hr />
        </div>
        

        <div class="pagination__buttons">
            
            <span class="button previous">
                <a href="https://mudler.pm/posts/smart-slackbot-for-teams/">
                    <span class="button__icon">←</span>
                    <span class="button__text">Create a Question answering bot on your data, locally with LangChain, LocalAI, Chroma, and GPT4All</span>
                </a>
            </span>
            

            
            <span class="button next">
                <a href="https://mudler.pm/posts/localai/">
                    <span class="button__text">LocalAI updates</span>
                    <span class="button__icon">→</span>
                </a>
            </span>
            
        </div>
    </div>

    </div>
      <div class="sharing-buttons">
        
<a class="resp-sharing-button__link" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmudler.pm%2fposts%2flocalai-question-answering%2f" target="_blank" rel="noopener" aria-label="" title="Share on facebook">
  <div class="resp-sharing-button resp-sharing-button--facebook resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M18 2h-3a5 5 0 0 0-5 5v3H7v4h3v8h4v-8h3l1-4h-4V7a1 1 0 0 1 1-1h3z"></path></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://twitter.com/intent/tweet/?url=https%3a%2f%2fmudler.pm%2fposts%2flocalai-question-answering%2f" target="_blank" rel="noopener" aria-label="" title="Share on twitter">
  <div class="resp-sharing-button resp-sharing-button--twitter resp-sharing-button--small">
      <div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://www.tumblr.com/widgets/share/tool?posttype=link&amp;title=Question%20Answering%20on%20Documents%20locally%20with%20LangChain%2c%20LocalAI%2c%20Chroma%2c%20and%20GPT4All&amp;caption=Question%20Answering%20on%20Documents%20locally%20with%20LangChain%2c%20LocalAI%2c%20Chroma%2c%20and%20GPT4All&amp;canonicalUrl=https%3a%2f%2fmudler.pm%2fposts%2flocalai-question-answering%2f" target="_blank" rel="noopener" aria-label="" title="Share on tumblr">
  <div class="resp-sharing-button resp-sharing-button--tumblr resp-sharing-button--small">
    <div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" stroke="none" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M14.563 24c-5.093 0-7.031-3.756-7.031-6.411V9.747H5.116V6.648c3.63-1.313 4.512-4.596 4.71-6.469C9.84.051 9.941 0 9.999 0h3.517v6.114h4.801v3.633h-4.82v7.47c.016 1.001.375 2.371 2.207 2.371h.09c.631-.02 1.486-.205 1.936-.419l1.156 3.425c-.436.636-2.4 1.374-4.156 1.404h-.178l.011.002z"/></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="mailto:?subject=Question%20Answering%20on%20Documents%20locally%20with%20LangChain%2c%20LocalAI%2c%20Chroma%2c%20and%20GPT4All&amp;body=https%3a%2f%2fmudler.pm%2fposts%2flocalai-question-answering%2f" target="_self" rel="noopener" aria-label="" title="Share via email">
  <div class="resp-sharing-button resp-sharing-button--email resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path><polyline points="22,6 12,13 2,6"></polyline></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://pinterest.com/pin/create/button/?url=https%3a%2f%2fmudler.pm%2fposts%2flocalai-question-answering%2f&amp;media=https%3a%2f%2fmudler.pm%2fposts%2flocalai-question-answering%2f;description=Question%20Answering%20on%20Documents%20locally%20with%20LangChain%2c%20LocalAI%2c%20Chroma%2c%20and%20GPT4All" target="_blank" rel="noopener" aria-label="" title="Share on pinterest">
  <div class="resp-sharing-button resp-sharing-button--pinterest resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" stroke="none"><path d="M12.017 0C5.396 0 .029 5.367.029 11.987c0 5.079 3.158 9.417 7.618 11.162-.105-.949-.199-2.403.041-3.439.219-.937 1.406-5.957 1.406-5.957s-.359-.72-.359-1.781c0-1.663.967-2.911 2.168-2.911 1.024 0 1.518.769 1.518 1.688 0 1.029-.653 2.567-.992 3.992-.285 1.193.6 2.165 1.775 2.165 2.128 0 3.768-2.245 3.768-5.487 0-2.861-2.063-4.869-5.008-4.869-3.41 0-5.409 2.562-5.409 5.199 0 1.033.394 2.143.889 2.741.099.12.112.225.085.345-.09.375-.293 1.199-.334 1.363-.053.225-.172.271-.401.165-1.495-.69-2.433-2.878-2.433-4.646 0-3.776 2.748-7.252 7.92-7.252 4.158 0 7.392 2.967 7.392 6.923 0 4.135-2.607 7.462-6.233 7.462-1.214 0-2.354-.629-2.758-1.379l-.749 2.848c-.269 1.045-1.004 2.352-1.498 3.146 1.123.345 2.306.535 3.55.535 6.607 0 11.985-5.365 11.985-11.987C23.97 5.39 18.592.026 11.985.026L12.017 0z"/></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmudler.pm%2fposts%2flocalai-question-answering%2f&amp;title=Question%20Answering%20on%20Documents%20locally%20with%20LangChain%2c%20LocalAI%2c%20Chroma%2c%20and%20GPT4All&amp;summary=Question%20Answering%20on%20Documents%20locally%20with%20LangChain%2c%20LocalAI%2c%20Chroma%2c%20and%20GPT4All&amp;source=https%3a%2f%2fmudler.pm%2fposts%2flocalai-question-answering%2f" target="_blank" rel="noopener" aria-label="" title="Share on linkedin">
  <div class="resp-sharing-button resp-sharing-button--linkedin resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect x="2" y="9" width="4" height="12"></rect><circle cx="4" cy="4" r="2"></circle></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://reddit.com/submit/?url=https%3a%2f%2fmudler.pm%2fposts%2flocalai-question-answering%2f&amp;resubmit=true&amp;title=Question%20Answering%20on%20Documents%20locally%20with%20LangChain%2c%20LocalAI%2c%20Chroma%2c%20and%20GPT4All" target="_blank" rel="noopener" aria-label="" title="Share on reddit">
  <div class="resp-sharing-button resp-sharing-button--reddit resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" stroke="none"><path d="M12 0A12 12 0 0 0 0 12a12 12 0 0 0 12 12 12 12 0 0 0 12-12A12 12 0 0 0 12 0zm5.01 4.744c.688 0 1.25.561 1.25 1.249a1.25 1.25 0 0 1-2.498.056l-2.597-.547-.8 3.747c1.824.07 3.48.632 4.674 1.488.308-.309.73-.491 1.207-.491.968 0 1.754.786 1.754 1.754 0 .716-.435 1.333-1.01 1.614a3.111 3.111 0 0 1 .042.52c0 2.694-3.13 4.87-7.004 4.87-3.874 0-7.004-2.176-7.004-4.87 0-.183.015-.366.043-.534A1.748 1.748 0 0 1 4.028 12c0-.968.786-1.754 1.754-1.754.463 0 .898.196 1.207.49 1.207-.883 2.878-1.43 4.744-1.487l.885-4.182a.342.342 0 0 1 .14-.197.35.35 0 0 1 .238-.042l2.906.617a1.214 1.214 0 0 1 1.108-.701zM9.25 12C8.561 12 8 12.562 8 13.25c0 .687.561 1.248 1.25 1.248.687 0 1.248-.561 1.248-1.249 0-.688-.561-1.249-1.249-1.249zm5.5 0c-.687 0-1.248.561-1.248 1.25 0 .687.561 1.248 1.249 1.248.688 0 1.249-.561 1.249-1.249 0-.687-.562-1.249-1.25-1.249zm-5.466 3.99a.327.327 0 0 0-.231.094.33.33 0 0 0 0 .463c.842.842 2.484.913 2.961.913.477 0 2.105-.056 2.961-.913a.361.361 0 0 0 .029-.463.33.33 0 0 0-.464 0c-.547.533-1.684.73-2.512.73-.828 0-1.979-.196-2.512-.73a.326.326 0 0 0-.232-.095z"/></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://www.xing.com/app/user?op=share;url=https%3a%2f%2fmudler.pm%2fposts%2flocalai-question-answering%2f;title=Question%20Answering%20on%20Documents%20locally%20with%20LangChain%2c%20LocalAI%2c%20Chroma%2c%20and%20GPT4All" target="_blank" rel="noopener" aria-label="" title="Share on xing">
  <div class="resp-sharing-button resp-sharing-button--xing resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" stroke="none"><path d="M18.188 0c-.517 0-.741.325-.927.66 0 0-7.455 13.224-7.702 13.657.015.024 4.919 9.023 4.919 9.023.17.308.436.66.967.66h3.454c.211 0 .375-.078.463-.22.089-.151.089-.346-.009-.536l-4.879-8.916c-.004-.006-.004-.016 0-.022L22.139.756c.095-.191.097-.387.006-.535C22.056.078 21.894 0 21.686 0h-3.498zM3.648 4.74c-.211 0-.385.074-.473.216-.09.149-.078.339.02.531l2.34 4.05c.004.01.004.016 0 .021L1.86 16.051c-.099.188-.093.381 0 .529.085.142.239.234.45.234h3.461c.518 0 .766-.348.945-.667l3.734-6.609-2.378-4.155c-.172-.315-.434-.659-.962-.659H3.648v.016z"/></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="whatsapp://send?text=Question%20Answering%20on%20Documents%20locally%20with%20LangChain%2c%20LocalAI%2c%20Chroma%2c%20and%20GPT4All%20https%3a%2f%2fmudler.pm%2fposts%2flocalai-question-answering%2f" target="_blank" rel="noopener" aria-label="" title="Share on whatsapp">
  <div class="resp-sharing-button resp-sharing-button--whatsapp resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" stroke="none" stroke-width="1" stroke-linecap="round" stroke-linejoin="round"><path d="M17.472 14.382c-.297-.149-1.758-.867-2.03-.967-.273-.099-.471-.148-.67.15-.197.297-.767.966-.94 1.164-.173.199-.347.223-.644.075-.297-.15-1.255-.463-2.39-1.475-.883-.788-1.48-1.761-1.653-2.059-.173-.297-.018-.458.13-.606.134-.133.298-.347.446-.52.149-.174.198-.298.298-.497.099-.198.05-.371-.025-.52-.075-.149-.669-1.612-.916-2.207-.242-.579-.487-.5-.669-.51-.173-.008-.371-.01-.57-.01-.198 0-.52.074-.792.372-.272.297-1.04 1.016-1.04 2.479 0 1.462 1.065 2.875 1.213 3.074.149.198 2.096 3.2 5.077 4.487.709.306 1.262.489 1.694.625.712.227 1.36.195 1.871.118.571-.085 1.758-.719 2.006-1.413.248-.694.248-1.289.173-1.413-.074-.124-.272-.198-.57-.347m-5.421 7.403h-.004a9.87 9.87 0 01-5.031-1.378l-.361-.214-3.741.982.998-3.648-.235-.374a9.86 9.86 0 01-1.51-5.26c.001-5.45 4.436-9.884 9.888-9.884 2.64 0 5.122 1.03 6.988 2.898a9.825 9.825 0 012.893 6.994c-.003 5.45-4.437 9.884-9.885 9.884m8.413-18.297A11.815 11.815 0 0012.05 0C5.495 0 .16 5.335.157 11.892c0 2.096.547 4.142 1.588 5.945L.057 24l6.305-1.654a11.882 11.882 0 005.683 1.448h.005c6.554 0 11.89-5.335 11.893-11.893a11.821 11.821 0 00-3.48-8.413Z"/></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fmudler.pm%2fposts%2flocalai-question-answering%2f&amp;t=Question%20Answering%20on%20Documents%20locally%20with%20LangChain%2c%20LocalAI%2c%20Chroma%2c%20and%20GPT4All" target="_blank" rel="noopener" aria-label="" title="Share on hacker news">
  <div class="resp-sharing-button resp-sharing-button--hackernews resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
			<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" stroke="none"><path d="M0 24V0h24v24H0zM6.951 5.896l4.112 7.708v5.064h1.583v-4.972l4.148-7.799h-1.749l-2.457 4.875c-.372.745-.688 1.434-.688 1.434s-.297-.708-.651-1.434L8.831 5.896h-1.88z"/></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://telegram.me/share/url?text=Question%20Answering%20on%20Documents%20locally%20with%20LangChain%2c%20LocalAI%2c%20Chroma%2c%20and%20GPT4All&amp;url=https%3a%2f%2fmudler.pm%2fposts%2flocalai-question-answering%2f" target="_blank" rel="noopener" aria-label="" title="Share on telegram">
  <div class="resp-sharing-button resp-sharing-button--telegram resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="22" y1="2" x2="11" y2="13"></line><polygon points="22 2 15 22 11 13 2 9 22 2"></polygon></svg>
    </div>
  </div>
</a>

      </div>


    

    

  </main>

            </div>

            
                <footer class="footer">
    
    
        <div class="footer__inner">
            <ul class="icons">
                    
        <li><a href="https://twitter.com/mudler_it" class="icon fa-brands fa-twitter" target="_blank" rel="noopener" title="Twitter"></a></li>
    
        <li><a href="mailto:mudler@mocaccino.org" class="icon fa-solid fa-envelope" target="_blank" rel="noopener" title="Email"></a></li>
    
        <li><a href="https://github.com/mudler" class="icon fa-brands fa-github" target="_blank" rel="noopener" title="Github"></a></li>
    
        <li><a href="https://www.linkedin.com/in/ettore-di-giacinto-211a4166/" class="icon fa-brands fa-linkedin" target="_blank" rel="noopener" title="Linkedin"></a></li>
    
                
                <li><a href="https://mudler.pm/posts/index.xml" target="_blank" title="rss" class="icon fa-solid fa-rss"></a></li>
                
            </ul>
        </div>
    
    
        <div class="footer__inner">
            <span>&copy;2023</span>&nbsp;
            <span><a href="https://mudler.pm"></a></span>&nbsp;
            <span><a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0</a></span>
        </div>
    
        <div class="footer__inner">
          
        </div>
</footer>

            
        </div>

        



<script type="text/javascript" src="/bundle.min.bb2c6bc3ed452ca4759660e4020811f248bc2320081559e8a32d8b0092773852941133639d35e8370d03d3ddaa750b1edd6b343c5bd22a55d5bdeae8f648f49b.js" integrity="sha512-uyxrw&#43;1FLKR1lmDkAggR8ki8IyAIFVnooy2LAJJ3OFKUETNjnTXoNw0D092qdQse3Ws0PFvSKlXVvero9kj0mw=="></script>



    </body>
</html>
