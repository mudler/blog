<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="">
<meta name="description" content="If you are a lucky(?) owner of the Jetson Nano Devkit (4GB), and you don&amp;rsquo;t know anymore on what to do with it, you can try to run LocalAI with llama.cpp on it.
The Jetson Nano Devkit is currently not supported anymore by Nvidia, and receives little to no attention, however, it can still do something, and if you are like me that recycles the board at home, you might want to have fun with it by running AI on top of it." />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="https://mudler.pm/posts/local-ai-jetson-nano-devkit/" />


    <title>
        
            LocalAI and llama.cpp on Jetson Nano Devkit :: Mudler blog  — Place where I write about stuff
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.5.0/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="/main.3d89495748aff88bbfbb2a665d8e1260b3aa63445cd77cc810fb6477aba343dd.css">




    <script src="https://kit.fontawesome.com/bdd86d0a63.js" crossorigin="anonymous"></script>

    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="">
    <link rel="shortcut icon" href="/favicon.ico">
    <meta name="msapplication-TileColor" content="">


<meta itemprop="name" content="LocalAI and llama.cpp on Jetson Nano Devkit">
<meta itemprop="description" content="If you are a lucky(?) owner of the Jetson Nano Devkit (4GB), and you don&rsquo;t know anymore on what to do with it, you can try to run LocalAI with llama.cpp on it.
The Jetson Nano Devkit is currently not supported anymore by Nvidia, and receives little to no attention, however, it can still do something, and if you are like me that recycles the board at home, you might want to have fun with it by running AI on top of it."><meta itemprop="datePublished" content="2024-05-30T00:00:00+00:00" />
<meta itemprop="dateModified" content="2024-05-30T00:00:00+00:00" />
<meta itemprop="wordCount" content="1491"><meta itemprop="image" content="https://mudler.pm"/>
<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://mudler.pm"/>

<meta name="twitter:title" content="LocalAI and llama.cpp on Jetson Nano Devkit"/>
<meta name="twitter:description" content="If you are a lucky(?) owner of the Jetson Nano Devkit (4GB), and you don&rsquo;t know anymore on what to do with it, you can try to run LocalAI with llama.cpp on it.
The Jetson Nano Devkit is currently not supported anymore by Nvidia, and receives little to no attention, however, it can still do something, and if you are like me that recycles the board at home, you might want to have fun with it by running AI on top of it."/>



    <meta property="og:title" content="LocalAI and llama.cpp on Jetson Nano Devkit" />
<meta property="og:description" content="If you are a lucky(?) owner of the Jetson Nano Devkit (4GB), and you don&rsquo;t know anymore on what to do with it, you can try to run LocalAI with llama.cpp on it.
The Jetson Nano Devkit is currently not supported anymore by Nvidia, and receives little to no attention, however, it can still do something, and if you are like me that recycles the board at home, you might want to have fun with it by running AI on top of it." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mudler.pm/posts/local-ai-jetson-nano-devkit/" /><meta property="og:image" content="https://mudler.pm"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-05-30T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-05-30T00:00:00+00:00" /><meta property="og:site_name" content="Mudler blog" />







    <meta property="article:published_time" content="2024-05-30 00:00:00 &#43;0000 UTC" />





<link href="https://mudler.pmfeed.json" rel="alternate"
    type="application/json" title="Mudler blog" />






    </head>

    
        <body>
    
    
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text">mudler blog</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <style>

  
.search-container {
    position: relative;
    display: flex;
    align-items: center;
    border-radius: 20px;
    padding: 5px;
    background-color: rgba(255, 255, 255, 0.5);
  }

  .search-box {
    border: none;
    background-color: transparent;
    color: #333;
    flex: 1;
    padding: 8px 8px 8px 28px;
    border-radius: 20px;
    font-size: 16px;
  }

  .search-button {
    border: none;
    background-color: #fff;
    color: #333;
    padding: 8px;
    border-radius: 20px;
  }

  .search-icon {
    position: absolute;
    left: 8px;
    top: 50%;
    transform: translateY(-50%);
    opacity: 0.7;
  }

  .search-icon i {
    font-size: 16px;
  }

  .search-button:hover {
    background-color: #555;
    color: #fff;
    cursor: pointer;
  }
  </style>

<nav class="menu">
    <ul class="menu__inner">
            
            <div class="submenu">
                <li class="dropdown">
                    <a href="/about">
                        About
                    </a>
                </li>
            </div>
            
        
            
            <div class="submenu">
                <li class="dropdown">
                    <a href="/posts">
                        Blog
                    </a>
                </li>
            </div>
            
        

    
        <div class="submenu">
            <form action="/search" method="GET">
                <div class="search-container">
                  <div class="search-icon">
                    <i class="fas fa-search"></i>
                  </div>
                  <input type="search" name="q" id="search-query" placeholder="Search..." class="search-box">
                  <button type="submit" class="search-button">
                    <i class="fas fa-search"></i>
                  </button>
                </div>
              </form>
        </div>
    </ul>

    
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            
                <span class="theme-toggle not-selectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
   <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
   3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
   13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
 </svg></span>
        </span>
    </span>
</header>


            <div class="content">
                
  <main class="post">

    <div class="post-info">
      <p>
        <i class="fa-solid fa-clock"></i>
        7 minutes
        <i class="fa-solid fa-clipboard"></i>
        1491 Words
        
      </p>
      <p>
        <i class="fa-solid fa-calendar-days"></i>
        
          2024-05-30 00:00
        

         
          
        
      </p>
      <p>
      
      
      </p>
    </div>

    <article>
      <h1 class="post-title">
        <a href="https://mudler.pm/posts/local-ai-jetson-nano-devkit/">LocalAI and llama.cpp on Jetson Nano Devkit</a>
      </h1>

      

      

      

      <div class="post-content">
        <p>If you are a lucky(?) owner of the Jetson Nano Devkit (4GB), and you don&rsquo;t know anymore on what to do with it, you can try to run LocalAI with llama.cpp on it.</p>
<p>The Jetson Nano Devkit is currently not supported anymore by Nvidia, and receives little to no attention, however, it can <em>still</em> do something, and if you are like me that recycles the board at home, you might want to have fun with it by running AI on top of it. It might be also good candidate for the workload distribution features that LocalAI has (see: <a href="https://localai.io/features/distribute/)">https://localai.io/features/distribute/)</a>, but leaving that for another post.</p>
<p>Disclaimer: you aren&rsquo;t going to run big models with it, but phi-2 runs and you can have fun with it!</p>
<p>I&rsquo;ll leave to another post for the setup I got with the Orin AGX, this covers for now only the Jetson Nano devkit as I recently did these steps to prepare a cluster to show-off on one of my upcoming talks!</p>
<h2 id="requirements">Requirements</h2>
<p>First of all, you need to get the latest Jetson Nano Devkit tooling that is - by now - it is 32.7.4.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>wget https://developer.nvidia.com/downloads/embedded/l4t/r32_release_v7.4/t210/jetson-210_linux_r32.7.4_aarch64.tbz2
</span></span><span style="display:flex;"><span>tar -xvf jetson-210_linux_r32.7.4_aarch64.tbz2
</span></span><span style="display:flex;"><span>cd Linux_for_Tegra
</span></span></code></pre></div><p>Then you need to flash the device with the latest image, so we need a rootfs:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>wget https://developer.nvidia.com/downloads/embedded/l4t/r32_release_v7.4/t210/tegra_linux_sample-root-filesystem_r32.7.4_aarch64.tbz2
</span></span><span style="display:flex;"><span>mkdir rootfs
</span></span><span style="display:flex;"><span>tar -xvf tegra_linux_sample-root-filesystem_r32.7.4_aarch64.tbz2 -C rootfs
</span></span></code></pre></div><p>Reminder: you need to prepare the rootfs by running (and requires QEMU with aarch64 support):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo ./apply_binaries.sh
</span></span></code></pre></div><p>Then you can flash the device with the following command:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo ./flash.sh jetson-nano-devkit-emmc mmcblk0p1
</span></span></code></pre></div><p>Make sure to put the device in recovery mode before running the flash command (notes below).</p>
<h3 id="enter-recovery-mode">Enter recovery mode</h3>
<ul>
<li>Jumper the J48 power select pin first and plug the power jack (optional, depend on devkit board)</li>
<li>Jumper the recovery pin and the reset pin. In my case it was on the edge of the Compute module.</li>
<li>Plug the MicroUSB cable to the PC</li>
<li>Plug the power to the board (DC on the board)</li>
</ul>
<h4 id="verify-you-are-in-recovery-mode">Verify you are in recovery mode</h4>
<p>You should be able to see the Nvidia device as <code>NVIDIA Corp. APX</code> by running “lsusb” command on your host.</p>
<pre tabindex="0"><code># lsusb
Bus 004 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub
Bus 003 Device 004: ID 0a5c:5842 Broadcom Corp. 58200
Bus 003 Device 003: ID 0bda:554c Realtek Semiconductor Corp. Integrated_Webcam_FHD
Bus 003 Device 079: ID 0955:7f21 NVIDIA Corp. APX
Bus 003 Device 005: ID 8087:0033 Intel Corp. 
Bus 003 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
Bus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub
Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
</code></pre><h2 id="cleanup">Cleanup</h2>
<p>We might need some extra space, especially if you are booting from the eMMC like me. We can remove all the graphical packages and docs like this on a booted system, but this might be a bit of a delicate process, beware and make sure you know what you are doing before running this command:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo apt-get remove ubuntu-desktop gnome-* libreoffice*
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">while</span> read p; <span style="color:#66d9ef">do</span>
</span></span><span style="display:flex;"><span>  sudo apt-get remove -y <span style="color:#e6db74">&#34;</span>$p<span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">done</span> &lt; &lt;<span style="color:#f92672">(</span>curl -L https://raw.githubusercontent.com/NVIDIA-AI-IOT/jetson-min-disk/main/assets/nvubuntu-bionic-packages_only-in-desktop.txt<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>sudo apt-get install network-manager
</span></span><span style="display:flex;"><span>sudo dpkg -r --force-depends <span style="color:#e6db74">&#34;cuda-documentation-10-2&#34;</span> <span style="color:#e6db74">&#34;cuda-samples-10-2&#34;</span> <span style="color:#e6db74">&#34;libnvinfer-samples&#34;</span> <span style="color:#e6db74">&#34;libvisionworks-samples&#34;</span> <span style="color:#e6db74">&#34;libnvinfer-doc&#34;</span> <span style="color:#e6db74">&#34;vpi1-samples&#34;</span>
</span></span></code></pre></div><h2 id="install-build-dependencies">Install Build dependencies</h2>
<p>The packages on the Jetson Nano are really old ( we are talking about gcc 7 ! ) as this is an Ubuntu 18.04.</p>
<p>We need then to install few things manually to go ahead:</p>
<h3 id="cmake">Cmake</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Install CMAKE: </span>
</span></span><span style="display:flex;"><span>wget https://github.com/Kitware/CMake/releases/download/v3.26.4/cmake-3.26.4.tar.gz
</span></span><span style="display:flex;"><span>tar xvf cmake-3.26.4.tar.gz
</span></span><span style="display:flex;"><span>cd cmake-3.26.4
</span></span><span style="display:flex;"><span>./configure
</span></span><span style="display:flex;"><span>make
</span></span><span style="display:flex;"><span>make install
</span></span></code></pre></div><h3 id="gcc-8">GCC-8</h3>
<p>We need gcc 8 for <code>-std=c++20</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo add-apt-repository ppa:ubuntu-toolchain-r/test
</span></span><span style="display:flex;"><span>sudo apt-get install -y gcc-8 g++-8
</span></span></code></pre></div><p>You might need to setup alternatives, just in case ( I didn&rsquo;t had to ):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e">#Remove the previous alternatives</span>
</span></span><span style="display:flex;"><span>sudo update-alternatives --remove-all gcc
</span></span><span style="display:flex;"><span>sudo update-alternatives --remove-all g++
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#Define the compiler</span>
</span></span><span style="display:flex;"><span>sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-8 <span style="color:#ae81ff">30</span>
</span></span><span style="display:flex;"><span>sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-8 <span style="color:#ae81ff">30</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sudo update-alternatives --install /usr/bin/cc cc /usr/bin/gcc <span style="color:#ae81ff">30</span>
</span></span><span style="display:flex;"><span>sudo update-alternatives --set cc /usr/bin/gcc
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sudo update-alternatives --install /usr/bin/c++ c++ /usr/bin/g++ <span style="color:#ae81ff">30</span>
</span></span><span style="display:flex;"><span>sudo update-alternatives --set c++ /usr/bin/g++
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#Confirm and update (You can use the default setting)</span>
</span></span><span style="display:flex;"><span>sudo update-alternatives --config gcc
</span></span><span style="display:flex;"><span>sudo update-alternatives --config g++
</span></span></code></pre></div><h3 id="install-cuda">Install cuda</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo apt-get install -y nvidia-cuda zlib1g-dev
</span></span></code></pre></div><h3 id="install-protoc">Install protoc</h3>
<p>Protoc in repositories is old! get the latest one:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl -L -s https://github.com/protocolbuffers/protobuf/releases/download/v26.1/protoc-26.1-linux-aarch_64.zip -o protoc.zip <span style="color:#f92672">&amp;&amp;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>unzip -j -d /usr/local/bin protoc.zip bin/protoc <span style="color:#f92672">&amp;&amp;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>rm protoc.zip
</span></span></code></pre></div><h3 id="install-golang">Install golang</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>wget https://go.dev/dl/go1.22.3.linux-arm64.tar.gz
</span></span><span style="display:flex;"><span>sudo tar -C /usr/local -xzf go1.22.3.linux-amd64.tar.gz
</span></span><span style="display:flex;"><span>echo <span style="color:#e6db74">&#34;export PATH=</span>$PATH<span style="color:#e6db74">:/usr/local/go/bin:</span>$HOME<span style="color:#e6db74">/go/bin:/usr/local/cuda/bin&#34;</span> &gt;&gt; ~/.bashrc
</span></span><span style="display:flex;"><span>source ~/.bashrc
</span></span></code></pre></div><h2 id="install-localai">Install LocalAI</h2>
<p>We are going to build LocalAI from source, as the prebuilt binaries are not available for the Jetson Nano and CUDA. The Jetson nano uses CUDA 10.2.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git clone https://github.com/mudler/LocalAI
</span></span><span style="display:flex;"><span>cd LocalAI
</span></span><span style="display:flex;"><span>BUILD_GRPC_FOR_BACKEND_LLAMA<span style="color:#f92672">=</span>true GRPC_BACKENDS<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;backend-assets/grpc/llama-cpp&#34;</span> BUILD_TYPE<span style="color:#f92672">=</span>cublas GO_TAGS<span style="color:#f92672">=</span>p2p make build
</span></span></code></pre></div><p>We are going to build only the llama-cpp backend (I did not try the others for now).</p>
<p>Its going to take a while, grab a coffee and wait for the build to finish.</p>
<p>&hellip; and boom, it fails compiling ?!.</p>
<p>At this point, you will need to patch few things out as llama.cpp with gcc 8 on aarch64 does not seem to compile see (upstream issue at <a href="https://github.com/ggerganov/llama.cpp/issues/7147">https://github.com/ggerganov/llama.cpp/issues/7147</a> ) :</p>
<pre tabindex="0"><code>diff --git a/ggml-cuda/fattn-common.cuh b/ggml-cuda/fattn-common.cuh
index 1dd519bd..c363bc9f 100644
--- a/ggml-cuda/fattn-common.cuh
+++ b/ggml-cuda/fattn-common.cuh
@@ -52,7 +52,7 @@ static __global__ void flash_attn_combine_results(
     dst       +=                 D * gridDim.y*blockIdx.x;
 
     const int tid = threadIdx.x;
-    __builtin_assume(tid &lt; D);
+    //__builtin_assume(tid &lt; D);
 
     __shared__ float2 meta[parallel_blocks];
     if (tid &lt; 2*parallel_blocks) {
diff --git a/ggml-cuda/fattn-vec-f16.cu b/ggml-cuda/fattn-vec-f16.cu
index 808e8f36..2bbb39bd 100644
--- a/ggml-cuda/fattn-vec-f16.cu
+++ b/ggml-cuda/fattn-vec-f16.cu
@@ -59,7 +59,7 @@ static __global__ void flash_attn_vec_ext_f16(
     static_assert(D % (2*WARP_SIZE) == 0, &#34;D not divisible by 2*WARP_SIZE == 64.&#34;);
     constexpr int nwarps = D / WARP_SIZE;
     const int tid = WARP_SIZE*threadIdx.y + threadIdx.x;
-    __builtin_assume(tid &lt; D);
+    //__assume(tid &lt; D);
 
     __shared__ half KQ[ncols*D];
 #pragma unroll
diff --git a/ggml-cuda/fattn-vec-f32.cu b/ggml-cuda/fattn-vec-f32.cu
index b4652301..500239b8 100644
--- a/ggml-cuda/fattn-vec-f32.cu
+++ b/ggml-cuda/fattn-vec-f32.cu
@@ -57,7 +57,7 @@ static __global__ void flash_attn_vec_ext_f32(
     static_assert(D % (2*WARP_SIZE) == 0, &#34;D not divisible by 2*WARP_SIZE == 64.&#34;);
     constexpr int nwarps = D / WARP_SIZE;
     const int tid = WARP_SIZE*threadIdx.y + threadIdx.x;
-    __builtin_assume(tid &lt; D);
+//#__builtin_assume(tid &lt; D);
 
     __shared__ float KQ[ncols*D];
 #pragma unroll
diff --git a/ggml-impl.h b/ggml-impl.h
index 5e77471f..1ad54af8 100644
--- a/ggml-impl.h
+++ b/ggml-impl.h
@@ -382,15 +382,15 @@ inline static uint8x16_t ggml_vqtbl1q_u8(uint8x16_t a, uint8x16_t b) {
 
 #define ggml_int16x8x2_t  int16x8x2_t
 #define ggml_uint8x16x2_t uint8x16x2_t
-#define ggml_uint8x16x4_t uint8x16x4_t
+#define ggml_uint8x16x4_t uint8x16x2_t
 #define ggml_int8x16x2_t  int8x16x2_t
-#define ggml_int8x16x4_t  int8x16x4_t
+#define ggml_int8x16x4_t  int8x16x2_t
 
 #define ggml_vld1q_s16_x2 vld1q_s16_x2
 #define ggml_vld1q_u8_x2  vld1q_u8_x2
-#define ggml_vld1q_u8_x4  vld1q_u8_x4
+#define ggml_vld1q_u8_x4  vld1q_u8_x2
 #define ggml_vld1q_s8_x2  vld1q_s8_x2
-#define ggml_vld1q_s8_x4  vld1q_s8_x4
+#define ggml_vld1q_s8_x4  vld1q_s8_x2
 #define ggml_vqtbl1q_s8   vqtbl1q_s8
 #define ggml_vqtbl1q_u8   vqtbl1q_u8
</code></pre><p>Apply the patch in <code>LocalAI/backend/cpp/llama-cpp/llama.cpp</code>.</p>
<h2 id="result">Result</h2>
<pre tabindex="0"><code>6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: freq_base_train  = 10000.0
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: freq_scale_train = 1
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: n_yarn_orig_ctx  = 2048
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: rope_finetuned   = unknown
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: ssm_d_conv       = 0
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: ssm_d_inner      = 0
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: ssm_d_state      = 0
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: ssm_dt_rank      = 0
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: model type       = 3B
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: model ftype      = Q4_K - Medium
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: model params     = 2.78 B
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: model size       = 1.62 GiB (5.00 BPW)
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: general.name     = Phi2
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: BOS token        = 50256 &#39;&lt;|endoftext|&gt;&#39;
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: EOS token        = 50256 &#39;&lt;|endoftext|&gt;&#39;
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: UNK token        = 50256 &#39;&lt;|endoftext|&gt;&#39;
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: PAD token        = 50256 &#39;&lt;|endoftext|&gt;&#39;
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: LF token         = 128 &#39;Ä&#39;
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: EOT token        = 50256 &#39;&lt;|endoftext|&gt;&#39;
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr ggml_cuda_init: found 1 CUDA devices:
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr   Device 0: NVIDIA Tegra X1, compute capability 5.3, VMM: no
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_tensors: ggml ctx size =    0.42 MiB
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_tensors: offloading 32 repeating layers to GPU
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_tensors: offloading non-repeating layers to GPU
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_tensors: offloaded 33/33 layers to GPU
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_tensors:        CPU buffer size =    70.31 MiB
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_tensors:      CUDA0 buffer size =  1585.10 MiB
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr ..........................................................................................
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llama_new_context_with_model: n_ctx      = 4096
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llama_new_context_with_model: n_batch    = 512
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llama_new_context_with_model: n_ubatch   = 512
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llama_new_context_with_model: flash_attn = 0
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llama_new_context_with_model: freq_base  = 10000.0
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llama_new_context_with_model: freq_scale = 1
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llama_kv_cache_init:      CUDA0 KV buffer size =  1280.00 MiB
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llama_new_context_with_model:  CUDA_Host  output buffer size =     0.20 MiB
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llama_new_context_with_model:      CUDA0 compute buffer size =   284.00 MiB
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llama_new_context_with_model:  CUDA_Host compute buffer size =    13.01 MiB
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llama_new_context_with_model: graph nodes  = 1161
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llama_new_context_with_model: graph splits = 2
</code></pre>
      </div>
    </article>
      <hr />
    <div class="post-info">
      
    <div class="pagination">
        
        <div class="pagination__title">
            <span class="pagination__title-h">Read other posts</span>
            <hr />
        </div>
        

        <div class="pagination__buttons">
            

            
            <span class="button next">
                <a href="https://mudler.pm/posts/smart-slackbot-for-teams/">
                    <span class="button__text">Create a Question answering bot for Slack on your data, that you can run locally</span>
                    <span class="button__icon">→</span>
                </a>
            </span>
            
        </div>
    </div>

    </div>
      <div class="sharing-buttons">
        
<a class="resp-sharing-button__link" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmudler.pm%2fposts%2flocal-ai-jetson-nano-devkit%2f" target="_blank" rel="noopener" aria-label="" title="Share on facebook">
  <div class="resp-sharing-button resp-sharing-button--facebook resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M18 2h-3a5 5 0 0 0-5 5v3H7v4h3v8h4v-8h3l1-4h-4V7a1 1 0 0 1 1-1h3z"></path></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://twitter.com/intent/tweet/?url=https%3a%2f%2fmudler.pm%2fposts%2flocal-ai-jetson-nano-devkit%2f" target="_blank" rel="noopener" aria-label="" title="Share on twitter">
  <div class="resp-sharing-button resp-sharing-button--twitter resp-sharing-button--small">
      <div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://www.tumblr.com/widgets/share/tool?posttype=link&amp;title=LocalAI%20and%20llama.cpp%20on%20Jetson%20Nano%20Devkit&amp;caption=LocalAI%20and%20llama.cpp%20on%20Jetson%20Nano%20Devkit&amp;canonicalUrl=https%3a%2f%2fmudler.pm%2fposts%2flocal-ai-jetson-nano-devkit%2f" target="_blank" rel="noopener" aria-label="" title="Share on tumblr">
  <div class="resp-sharing-button resp-sharing-button--tumblr resp-sharing-button--small">
    <div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" stroke="none" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M14.563 24c-5.093 0-7.031-3.756-7.031-6.411V9.747H5.116V6.648c3.63-1.313 4.512-4.596 4.71-6.469C9.84.051 9.941 0 9.999 0h3.517v6.114h4.801v3.633h-4.82v7.47c.016 1.001.375 2.371 2.207 2.371h.09c.631-.02 1.486-.205 1.936-.419l1.156 3.425c-.436.636-2.4 1.374-4.156 1.404h-.178l.011.002z"/></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="mailto:?subject=LocalAI%20and%20llama.cpp%20on%20Jetson%20Nano%20Devkit&amp;body=https%3a%2f%2fmudler.pm%2fposts%2flocal-ai-jetson-nano-devkit%2f" target="_self" rel="noopener" aria-label="" title="Share via email">
  <div class="resp-sharing-button resp-sharing-button--email resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path><polyline points="22,6 12,13 2,6"></polyline></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://pinterest.com/pin/create/button/?url=https%3a%2f%2fmudler.pm%2fposts%2flocal-ai-jetson-nano-devkit%2f&amp;media=https%3a%2f%2fmudler.pm%2fposts%2flocal-ai-jetson-nano-devkit%2f;description=LocalAI%20and%20llama.cpp%20on%20Jetson%20Nano%20Devkit" target="_blank" rel="noopener" aria-label="" title="Share on pinterest">
  <div class="resp-sharing-button resp-sharing-button--pinterest resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" stroke="none"><path d="M12.017 0C5.396 0 .029 5.367.029 11.987c0 5.079 3.158 9.417 7.618 11.162-.105-.949-.199-2.403.041-3.439.219-.937 1.406-5.957 1.406-5.957s-.359-.72-.359-1.781c0-1.663.967-2.911 2.168-2.911 1.024 0 1.518.769 1.518 1.688 0 1.029-.653 2.567-.992 3.992-.285 1.193.6 2.165 1.775 2.165 2.128 0 3.768-2.245 3.768-5.487 0-2.861-2.063-4.869-5.008-4.869-3.41 0-5.409 2.562-5.409 5.199 0 1.033.394 2.143.889 2.741.099.12.112.225.085.345-.09.375-.293 1.199-.334 1.363-.053.225-.172.271-.401.165-1.495-.69-2.433-2.878-2.433-4.646 0-3.776 2.748-7.252 7.92-7.252 4.158 0 7.392 2.967 7.392 6.923 0 4.135-2.607 7.462-6.233 7.462-1.214 0-2.354-.629-2.758-1.379l-.749 2.848c-.269 1.045-1.004 2.352-1.498 3.146 1.123.345 2.306.535 3.55.535 6.607 0 11.985-5.365 11.985-11.987C23.97 5.39 18.592.026 11.985.026L12.017 0z"/></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fmudler.pm%2fposts%2flocal-ai-jetson-nano-devkit%2f&amp;title=LocalAI%20and%20llama.cpp%20on%20Jetson%20Nano%20Devkit&amp;summary=LocalAI%20and%20llama.cpp%20on%20Jetson%20Nano%20Devkit&amp;source=https%3a%2f%2fmudler.pm%2fposts%2flocal-ai-jetson-nano-devkit%2f" target="_blank" rel="noopener" aria-label="" title="Share on linkedin">
  <div class="resp-sharing-button resp-sharing-button--linkedin resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect x="2" y="9" width="4" height="12"></rect><circle cx="4" cy="4" r="2"></circle></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://reddit.com/submit/?url=https%3a%2f%2fmudler.pm%2fposts%2flocal-ai-jetson-nano-devkit%2f&amp;resubmit=true&amp;title=LocalAI%20and%20llama.cpp%20on%20Jetson%20Nano%20Devkit" target="_blank" rel="noopener" aria-label="" title="Share on reddit">
  <div class="resp-sharing-button resp-sharing-button--reddit resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" stroke="none"><path d="M12 0A12 12 0 0 0 0 12a12 12 0 0 0 12 12 12 12 0 0 0 12-12A12 12 0 0 0 12 0zm5.01 4.744c.688 0 1.25.561 1.25 1.249a1.25 1.25 0 0 1-2.498.056l-2.597-.547-.8 3.747c1.824.07 3.48.632 4.674 1.488.308-.309.73-.491 1.207-.491.968 0 1.754.786 1.754 1.754 0 .716-.435 1.333-1.01 1.614a3.111 3.111 0 0 1 .042.52c0 2.694-3.13 4.87-7.004 4.87-3.874 0-7.004-2.176-7.004-4.87 0-.183.015-.366.043-.534A1.748 1.748 0 0 1 4.028 12c0-.968.786-1.754 1.754-1.754.463 0 .898.196 1.207.49 1.207-.883 2.878-1.43 4.744-1.487l.885-4.182a.342.342 0 0 1 .14-.197.35.35 0 0 1 .238-.042l2.906.617a1.214 1.214 0 0 1 1.108-.701zM9.25 12C8.561 12 8 12.562 8 13.25c0 .687.561 1.248 1.25 1.248.687 0 1.248-.561 1.248-1.249 0-.688-.561-1.249-1.249-1.249zm5.5 0c-.687 0-1.248.561-1.248 1.25 0 .687.561 1.248 1.249 1.248.688 0 1.249-.561 1.249-1.249 0-.687-.562-1.249-1.25-1.249zm-5.466 3.99a.327.327 0 0 0-.231.094.33.33 0 0 0 0 .463c.842.842 2.484.913 2.961.913.477 0 2.105-.056 2.961-.913a.361.361 0 0 0 .029-.463.33.33 0 0 0-.464 0c-.547.533-1.684.73-2.512.73-.828 0-1.979-.196-2.512-.73a.326.326 0 0 0-.232-.095z"/></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://www.xing.com/app/user?op=share;url=https%3a%2f%2fmudler.pm%2fposts%2flocal-ai-jetson-nano-devkit%2f;title=LocalAI%20and%20llama.cpp%20on%20Jetson%20Nano%20Devkit" target="_blank" rel="noopener" aria-label="" title="Share on xing">
  <div class="resp-sharing-button resp-sharing-button--xing resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" stroke="none"><path d="M18.188 0c-.517 0-.741.325-.927.66 0 0-7.455 13.224-7.702 13.657.015.024 4.919 9.023 4.919 9.023.17.308.436.66.967.66h3.454c.211 0 .375-.078.463-.22.089-.151.089-.346-.009-.536l-4.879-8.916c-.004-.006-.004-.016 0-.022L22.139.756c.095-.191.097-.387.006-.535C22.056.078 21.894 0 21.686 0h-3.498zM3.648 4.74c-.211 0-.385.074-.473.216-.09.149-.078.339.02.531l2.34 4.05c.004.01.004.016 0 .021L1.86 16.051c-.099.188-.093.381 0 .529.085.142.239.234.45.234h3.461c.518 0 .766-.348.945-.667l3.734-6.609-2.378-4.155c-.172-.315-.434-.659-.962-.659H3.648v.016z"/></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="whatsapp://send?text=LocalAI%20and%20llama.cpp%20on%20Jetson%20Nano%20Devkit%20https%3a%2f%2fmudler.pm%2fposts%2flocal-ai-jetson-nano-devkit%2f" target="_blank" rel="noopener" aria-label="" title="Share on whatsapp">
  <div class="resp-sharing-button resp-sharing-button--whatsapp resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" stroke="none" stroke-width="1" stroke-linecap="round" stroke-linejoin="round"><path d="M17.472 14.382c-.297-.149-1.758-.867-2.03-.967-.273-.099-.471-.148-.67.15-.197.297-.767.966-.94 1.164-.173.199-.347.223-.644.075-.297-.15-1.255-.463-2.39-1.475-.883-.788-1.48-1.761-1.653-2.059-.173-.297-.018-.458.13-.606.134-.133.298-.347.446-.52.149-.174.198-.298.298-.497.099-.198.05-.371-.025-.52-.075-.149-.669-1.612-.916-2.207-.242-.579-.487-.5-.669-.51-.173-.008-.371-.01-.57-.01-.198 0-.52.074-.792.372-.272.297-1.04 1.016-1.04 2.479 0 1.462 1.065 2.875 1.213 3.074.149.198 2.096 3.2 5.077 4.487.709.306 1.262.489 1.694.625.712.227 1.36.195 1.871.118.571-.085 1.758-.719 2.006-1.413.248-.694.248-1.289.173-1.413-.074-.124-.272-.198-.57-.347m-5.421 7.403h-.004a9.87 9.87 0 01-5.031-1.378l-.361-.214-3.741.982.998-3.648-.235-.374a9.86 9.86 0 01-1.51-5.26c.001-5.45 4.436-9.884 9.888-9.884 2.64 0 5.122 1.03 6.988 2.898a9.825 9.825 0 012.893 6.994c-.003 5.45-4.437 9.884-9.885 9.884m8.413-18.297A11.815 11.815 0 0012.05 0C5.495 0 .16 5.335.157 11.892c0 2.096.547 4.142 1.588 5.945L.057 24l6.305-1.654a11.882 11.882 0 005.683 1.448h.005c6.554 0 11.89-5.335 11.893-11.893a11.821 11.821 0 00-3.48-8.413Z"/></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fmudler.pm%2fposts%2flocal-ai-jetson-nano-devkit%2f&amp;t=LocalAI%20and%20llama.cpp%20on%20Jetson%20Nano%20Devkit" target="_blank" rel="noopener" aria-label="" title="Share on hacker news">
  <div class="resp-sharing-button resp-sharing-button--hackernews resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
			<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="currentColor" stroke="none"><path d="M0 24V0h24v24H0zM6.951 5.896l4.112 7.708v5.064h1.583v-4.972l4.148-7.799h-1.749l-2.457 4.875c-.372.745-.688 1.434-.688 1.434s-.297-.708-.651-1.434L8.831 5.896h-1.88z"/></svg>
    </div>
  </div>
</a>


<a class="resp-sharing-button__link" href="https://telegram.me/share/url?text=LocalAI%20and%20llama.cpp%20on%20Jetson%20Nano%20Devkit&amp;url=https%3a%2f%2fmudler.pm%2fposts%2flocal-ai-jetson-nano-devkit%2f" target="_blank" rel="noopener" aria-label="" title="Share on telegram">
  <div class="resp-sharing-button resp-sharing-button--telegram resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="22" y1="2" x2="11" y2="13"></line><polygon points="22 2 15 22 11 13 2 9 22 2"></polygon></svg>
    </div>
  </div>
</a>

      </div>


    

    

  </main>

            </div>

            
                <footer class="footer">
    
    
        <div class="footer__inner">
            <ul class="icons">
                    
        <li><a href="https://twitter.com/mudler_it" class="icon fa-brands fa-twitter" target="_blank" rel="noopener" title="Twitter"></a></li>
    
        <li><a href="mailto:mudler@mocaccino.org" class="icon fa-solid fa-envelope" target="_blank" rel="noopener" title="Email"></a></li>
    
        <li><a href="https://github.com/mudler" class="icon fa-brands fa-github" target="_blank" rel="noopener" title="Github"></a></li>
    
        <li><a href="https://www.linkedin.com/in/ettore-di-giacinto-211a4166/" class="icon fa-brands fa-linkedin" target="_blank" rel="noopener" title="Linkedin"></a></li>
    
                
                <li><a href="https://mudler.pm/posts/index.xml" target="_blank" title="rss" class="icon fa-solid fa-rss"></a></li>
                
            </ul>
        </div>
    
    
        <div class="footer__inner">
            <span>&copy;2024</span>&nbsp;
            <span><a href="https://mudler.pm"></a></span>&nbsp;
            <span><a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0</a></span>
        </div>
    
        <div class="footer__inner">
          
        </div>
</footer>

            
        </div>

        



<script type="text/javascript" src="/bundle.min.bb2c6bc3ed452ca4759660e4020811f248bc2320081559e8a32d8b0092773852941133639d35e8370d03d3ddaa750b1edd6b343c5bd22a55d5bdeae8f648f49b.js" integrity="sha512-uyxrw&#43;1FLKR1lmDkAggR8ki8IyAIFVnooy2LAJJ3OFKUETNjnTXoNw0D092qdQse3Ws0PFvSKlXVvero9kj0mw=="></script>



    </body>
</html>
