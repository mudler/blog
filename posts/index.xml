<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Mudler blog</title>
        <link>https://mudler.pm/posts/</link>
        <description>Recent content in Posts on Mudler blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
        <lastBuildDate>Thu, 22 Jun 2023 00:00:00 +0000</lastBuildDate>
        <atom:link href="https://mudler.pm/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Create a Question answering bot for Slack on your data, that you can run locally</title>
            <link>https://mudler.pm/posts/smart-slackbot-for-teams/</link>
            <pubDate>Thu, 22 Jun 2023 00:00:00 +0000</pubDate>
            
            <guid>https://mudler.pm/posts/smart-slackbot-for-teams/</guid>
            <description>There has been a lot of buzz around AI, Langchain, and the possibilities they offer nowadays. In this blog post, I will delve into the process of creating a small assistant for yourself or your team on Slack. This assistant will be able to provide answers related to your documentation.
The problem I work at Spectro Cloud, and we have an exciting open source project called Kairos (check it out at https://kairos.</description>
            <content type="html"><![CDATA[<p>There has been a lot of buzz around AI, Langchain, and the possibilities they offer nowadays. In this blog post, I will delve into the process of creating a small assistant for yourself or your team on Slack. This assistant will be able to provide answers related to your documentation.</p>
<p><img src="https://github.com/spectrocloud-labs/Slack-QA-bot/assets/2420543/6047e1ff-22d5-4b03-9d73-fcb7fb19a2c1" alt="Kairos-TPM-Slackbot"></p>
<h2 id="the-problem">The problem</h2>
<p>I work at <a href="https://www.spectrocloud.com/">Spectro Cloud</a>, and we have an exciting open source project called <a href="https://kairos.io">Kairos</a> (check it out at <a href="https://kairos.io">https://kairos.io</a> if you want to learn more about it!). Kairos is a Meta-Linux, immutable distribution designed for running Kubernetes at the Edge. One of the challenges we face, aside from creating good documentation, is making it easily accessible and consumable for our community. Documentation evolves rapidly, and it&rsquo;s easy to lose track.
Documentation is a critical part of any project. It&rsquo;s the first thing people see when they visit your website, and it&rsquo;s the first thing they look at when they want to learn more about your project, and when a project generates a large amount of documentation, it becomes difficult not only to navigate through it but also to find exactly what you&rsquo;re looking for.</p>
<p>Nowadays, there are several services that offer question answering to improve documentation. However, if you&rsquo;re like me and want to understand how things work behind the scenes, and perhaps build your own solution, then keep reading.</p>
<p>In this post, I will show you how to set up your own personal Slack bot that can answer questions based on documentation websites, GitHub issues, and code. By the end of this article, you will be able to deploy this bot using Docker or Kubernetes, either for yourself or for your team at work!</p>
<p>You can also try the bot live in our channel (#kairos) by joining <a href="https://join.slack.com/t/spectrocloudcommunity/shared_invite/zt-1k7wsz840-ugSsPKzZCP5gkasJ0kNpqw">the Kairos Slack channel</a> and opening a thread with <code>@LocalAI Bot (dev)</code> (for example, <code>@LocalAI Bot (dev) does Kairos use TPM?</code>. Keep in mind that we self-host this on a small instance, so answers can be <em>slow</em>, but typically in range of 1-2 minutes.).</p>
<h2 id="the-plan">The plan</h2>
<p>Here&rsquo;s how it works: our code will create a vector database that contains vector representations of different sections of the documentation, code snippets, and GitHub issues. To accomplish this, we will use Langchain and ChromaDB to create the vector database. Langchain is a powerful library that allows interaction with LLMs (Language Model Models), and ChromaDB is a local database that can store documents in the form of embeddings. Embeddings are vectors that represent strings. Embedding databases enable semantic searching within a dataset.</p>
<p>For LLM inference, we will also utilize LocalAI. LocalAI allows us to run LLMs and serves as a drop-in replacement for OpenAI. Although there are other ways to interact with LLMs locally, in this case, I want a clear separation between the model execution and the application logic. This separation enables me to focus more on the core functionality of my bot. It also makes maintenance and updates easier on the go. We can replace the underlying models behind the scenes without modifying our code. Additionally, we can leverage the existing OpenAI libraries, which is quite handy. We will simulate writing code that works with OpenAI, but we will actually test it locally. This approach also allows us to use the same code with OpenAI directly or Azure, if needed.</p>
<p>A summary of what we will need:</p>
<ul>
<li>Basic knowledge of Python and Docker to create a container image for our Slack bot.</li>
<li>LocalAI for running LLMs locally (no GPU required, just a modern CPU).</li>
<li>An LLM model of your choice (I personally found airoboros to be quite good for Q&amp;A).</li>
<li>No OpenAI API keys or external services are needed. We will host the bot on our own without relying on remote AI APIs.</li>
<li>If deploying on Kubernetes in the cloud, you will need a cluster. If running on bare metal, I&rsquo;ve tested this on Kairos (<a href="https://kairos.io">https://kairos.io</a>).</li>
</ul>
<h2 id="tools-we-will-use">Tools we will use</h2>
<p><strong>LocalAI</strong>: It&rsquo;s a project created by me and it is completely community-driven. I encourage you to help and contribute if you want! LocalAI lets you run LLM from different families and it has an OpenAI compatible API endpoint which allows to be used with exiting clients. You can learn more about LocalAI here <a href="https://github.com/go-skynet/LocalAI">https://github.com/go-skynet/LocalAI</a> and in the official website <a href="https://localai.io">https://localai.io</a>.</p>
<p><strong>Langchain</strong>: is a development framework created by Harrison Chase to build applications powered by language models. See: <a href="https://python.langchain.com/docs/get_started/introduction.html">https://python.langchain.com/docs/get_started/introduction.html</a></p>
<p><strong>Docker</strong>: we will run the slack bot with Docker to simplify configuration. A <code>docker-compose.yml</code> file is provided as an example on how to start the slack bot and LocalAI.</p>
<h2 id="how-the-bot-works">How the bot works</h2>
<p>If you&rsquo;re not interested in the details, you can skip directly to the Setup section below. In this section, I will explain how the bot works.</p>
<p>The bot is a generic Slack bot customized to provide answers using Langchain on datasets. You can view the full code of the bot here: <a href="https://github.com/spectrocloud-labs/Slack-QA-bot">https://github.com/spectrocloud-labs/Slack-QA-bot</a>. The interesting part of the bot lies in the <code>memory_ops.py</code> file (<a href="https://github.com/spectrocloud-labs/Slack-QA-bot/blob/main/app/memory_ops.py)">https://github.com/spectrocloud-labs/Slack-QA-bot/blob/main/app/memory_ops.py)</a>. Here&rsquo;s what we do in that file:</p>
<ul>
<li>Build a knowledge base for the bot to use for answering questions.</li>
<li>When asked questions, the bot utilizes the knowledge base to enhance its answers.</li>
</ul>
<h3 id="building-a-knowledge-base">Building a knowledge base</h3>
<p>The core of the bot lies in this Python function:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">build_knowledgebase</span>(sitemap):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Load environment variables</span>
</span></span><span style="display:flex;"><span>    repositories <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#34;REPOSITORIES&#34;</span>)<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34;,&#34;</span>)
</span></span><span style="display:flex;"><span>    issue_repos <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#34;ISSUE_REPOSITORIES&#34;</span>)<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34;,&#34;</span>)
</span></span><span style="display:flex;"><span>    embeddings <span style="color:#f92672">=</span> HuggingFaceEmbeddings(model_name<span style="color:#f92672">=</span>EMBEDDINGS_MODEL_NAME)
</span></span><span style="display:flex;"><span>    chunk_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">500</span>
</span></span><span style="display:flex;"><span>    chunk_overlap <span style="color:#f92672">=</span> <span style="color:#ae81ff">50</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    git_loaders <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> repo <span style="color:#f92672">in</span> repositories:
</span></span><span style="display:flex;"><span>        git_loader <span style="color:#f92672">=</span> GitLoader(
</span></span><span style="display:flex;"><span>            clone_url<span style="color:#f92672">=</span>os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>repo<span style="color:#e6db74">}</span><span style="color:#e6db74">_CLONE_URL&#34;</span>),
</span></span><span style="display:flex;"><span>            repo_path<span style="color:#f92672">=</span><span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;/tmp/</span><span style="color:#e6db74">{</span>repo<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>,
</span></span><span style="display:flex;"><span>            branch<span style="color:#f92672">=</span>os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>repo<span style="color:#e6db74">}</span><span style="color:#e6db74">_BRANCH&#34;</span>, <span style="color:#e6db74">&#34;main&#34;</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        git_loaders<span style="color:#f92672">.</span>append(git_loader)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> repo <span style="color:#f92672">in</span> issue_repos:
</span></span><span style="display:flex;"><span>        loader <span style="color:#f92672">=</span> GitHubIssuesLoader(
</span></span><span style="display:flex;"><span>            repo<span style="color:#f92672">=</span>repo,
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        git_loaders<span style="color:#f92672">.</span>append(loader)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    sitemap_loader <span style="color:#f92672">=</span> SitemapLoader(web_path<span style="color:#f92672">=</span>sitemap)
</span></span><span style="display:flex;"><span>    documents <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> git_loader <span style="color:#f92672">in</span> git_loaders:
</span></span><span style="display:flex;"><span>        documents<span style="color:#f92672">.</span>extend(git_loader<span style="color:#f92672">.</span>load())
</span></span><span style="display:flex;"><span>    documents<span style="color:#f92672">.</span>extend(sitemap_loader<span style="color:#f92672">.</span>load())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> doc <span style="color:#f92672">in</span> documents:
</span></span><span style="display:flex;"><span>        doc<span style="color:#f92672">.</span>metadata <span style="color:#f92672">=</span> fix_metadata(doc<span style="color:#f92672">.</span>metadata)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    text_splitter <span style="color:#f92672">=</span> RecursiveCharacterTextSplitter(chunk_size<span style="color:#f92672">=</span>chunk_size, chunk_overlap<span style="color:#f92672">=</span>chunk_overlap)
</span></span><span style="display:flex;"><span>    texts <span style="color:#f92672">=</span> text_splitter<span style="color:#f92672">.</span>split_documents(documents)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Creating embeddings. This may take a few minutes...&#34;</span>)
</span></span><span style="display:flex;"><span>    db <span style="color:#f92672">=</span> Chroma<span style="color:#f92672">.</span>from_documents(texts, embeddings, persist_directory<span style="color:#f92672">=</span>PERSIST_DIRECTORY, client_settings<span style="color:#f92672">=</span>CHROMA_SETTINGS)
</span></span><span style="display:flex;"><span>    db<span style="color:#f92672">.</span>persist()
</span></span><span style="display:flex;"><span>    db <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span></code></pre></div><p>We use the locally run <code>HuggingFaceEmbeddings</code> (<code>embeddings = HuggingFaceEmbeddings(model_name=EMBEDDINGS_MODEL_NAME)</code>) and Langchain to split the document into chunks. We then utilize Chroma to construct a vector database.</p>
<p>The code above utilizes the Github Loaders and GithubIssue loader from Langchain to retrieve information about code and GitHub issues from various GitHub repositories. The repositories can be defined via environment variables. We also use the <code>SitemapLoader</code> to ingest a <code>sitemap.xml</code> file and scrape an entire website. This is particularly useful if you already have documentation or a website.</p>
<h3 id="querying-the-knowledge-base">Querying the knowledge base</h3>
<p>Another crucial part of the code is how we interact with the AI and enhance the search results.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">ask_with_memory</span>(line) <span style="color:#f92672">-&gt;</span> str:
</span></span><span style="display:flex;"><span>    embeddings <span style="color:#f92672">=</span> HuggingFaceEmbeddings(model_name<span style="color:#f92672">=</span>EMBEDDINGS_MODEL_NAME)
</span></span><span style="display:flex;"><span>    db <span style="color:#f92672">=</span> Chroma(persist_directory<span style="color:#f92672">=</span>PERSIST_DIRECTORY, embedding_function<span style="color:#f92672">=</span>embeddings, client_settings<span style="color:#f92672">=</span>CHROMA_SETTINGS)
</span></span><span style="display:flex;"><span>    retriever <span style="color:#f92672">=</span> db<span style="color:#f92672">.</span>as_retriever()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    res <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    llm <span style="color:#f92672">=</span> ChatOpenAI(temperature<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, openai_api_base<span style="color:#f92672">=</span>BASE_PATH, model_name<span style="color:#f92672">=</span>OPENAI_MODEL)
</span></span><span style="display:flex;"><span>    qa <span style="color:#f92672">=</span> RetrievalQA<span style="color:#f92672">.</span>from_chain_type(llm<span style="color:#f92672">=</span>llm, chain_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;stuff&#34;</span>, retriever<span style="color:#f92672">=</span>retriever, return_source_documents<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Get the answer from the chain</span>
</span></span><span style="display:flex;"><span>    res <span style="color:#f92672">=</span> qa(<span style="color:#e6db74">&#34;---------------------</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74"> Given the context above, answer the following question: &#34;</span> <span style="color:#f92672">+</span> line)
</span></span><span style="display:flex;"><span>    answer, docs <span style="color:#f92672">=</span> res[<span style="color:#e6db74">&#39;result&#39;</span>], res[<span style="color:#e6db74">&#39;source_documents&#39;</span>]
</span></span><span style="display:flex;"><span>    res <span style="color:#f92672">=</span> answer <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n\n\n</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;Sources:</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Print the relevant sources used for the answer</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> document <span style="color:#f92672">in</span> docs:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#34;source&#34;</span> <span style="color:#f92672">in</span> document<span style="color:#f92672">.</span>metadata:
</span></span><span style="display:flex;"><span>            res <span style="color:#f92672">+=</span> <span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">---------------------</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">+</span> document<span style="color:#f92672">.</span>metadata[<span style="color:#e6db74">&#34;source&#34;</span>] <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">---------------------</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            res <span style="color:#f92672">+=</span> <span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">---------------------</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74"> No source available (sorry!) </span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">---------------------</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>        res <span style="color:#f92672">+=</span> <span style="color:#e6db74">&#34;```</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">+</span>document<span style="color:#f92672">.</span>page_content<span style="color:#f92672">+</span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">```&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> res
</span></span></code></pre></div><p>In this section, we load the previously created embedding database and configure the Langchain <code>RetrievalQA</code> object. Once the knowledge base has been built, we simply point to the embedding database and specify the embedding engine. In this case, we use local embeddings with HuggingFace, but other options could have been used as well (for example, LocalAI also has its own embedding mechanism).</p>
<p>We then configure the <code>llm</code> to use LocalAI. Note that we use ChatOpenAI and set <code>openai_api_base</code> to use <a href="https://github.com/go-skynet/LocalAI">LocalAI</a> instead.</p>
<h2 id="setup">Setup</h2>
<p>Now, let&rsquo;s proceed with setting up our bot! Here&rsquo;s what we need:</p>
<ul>
<li>Set up a Slack server and gain access to add new applications.</li>
<li>Create a GitHub repository (optional) and obtain a Personal Access Token to fetch issues from a repository.</li>
<li>Ensure your website has an accessible <code>sitemap.xml</code> file so that our bot can scrape the website content.</li>
<li>Install the <code>docker</code> and <code>docker-compose</code> applications locally if missing.</li>
<li>Choose a model for use with LocalAI (refer to <a href="https://github.com/go-skynet/LocalAI)">https://github.com/go-skynet/LocalAI)</a>.</li>
</ul>
<p>That&rsquo;s it! We don&rsquo;t need an OpenAI API key or any external services except GitHub, optionally, which we use to fetch the content we want to index.</p>
<h3 id="clone-the-required-files">Clone the required files</h3>
<p>We will run everything locally using Docker. At the end of this article, I will also provide a deployment file that works with Kubernetes.</p>
<p>To get started, clone the LocalAI repository locally:</p>
<pre tabindex="0"><code>git clone https://github.com/go-skynet/LocalAI
cd LocalAI/examples/slack-qa-bot
</code></pre><p>You will find a <code>docker-compose.yaml</code> file and a <code>.env.example</code> file. We need to edit the <code>.env</code> file and add the Slack tokens to allow the bot to connect.</p>
<h3 id="configuring-slack">Configuring Slack</h3>
<p>To install the bot, we need to create an application in the Slack workspace. Follow these steps:</p>
<ol>
<li>
<p>Go to <a href="https://api.slack.com/apps/">https://api.slack.com/apps/</a> and click on &ldquo;Create new App&rdquo;.
<img src="https://github.com/seratch/ChatGPT-in-Slack/assets/2420543/9e474872-0d24-4601-b453-679f3601de18" alt="Screenshot 1"></p>
</li>
<li>
<p>Select &ldquo;From an app Manifest&rdquo;.
<img src="https://github.com/seratch/ChatGPT-in-Slack/assets/2420543/962de606-a694-47ad-8fc0-cd096668e07f" alt="Screenshot 2"></p>
</li>
<li>
<p>Choose the workspace where you want to add the bot.
<img src="https://github.com/seratch/ChatGPT-in-Slack/assets/2420543/37573a42-87b6-4351-ad19-303e5ad610ed" alt="Screenshot 3"></p>
</li>
<li>
<p>Copy the content of the <a href="https://raw.githubusercontent.com/spectrocloud-labs/Slack-QA-bot/main/manifest-dev.yml">manifest-dev.yml</a> file from the repository and paste it into the app manifest.
<img src="https://github.com/seratch/ChatGPT-in-Slack/assets/2420543/f134d1c6-eded-4114-81a0-d5fa2f870baf" alt="Screenshot 4"></p>
</li>
<li>
<p>Install the app in your workspace.
<img src="https://github.com/seratch/ChatGPT-in-Slack/assets/2420543/cf84d743-6ccd-41a5-9b7e-41591b6d4939" alt="Screenshot 5"></p>
</li>
<li>
<p>Create an app level token with the <code>connection:write</code> scope. Save this token as <code>SLACK_APP_TOKEN</code>.
<img src="https://github.com/seratch/ChatGPT-in-Slack/assets/2420543/997573a6-8fb7-4357-b811-bdd01e52b158" alt="Screenshot 6">
<img src="https://github.com/seratch/ChatGPT-in-Slack/assets/2420543/763b1854-2b5a-4690-b5fa-a17fd720f40d" alt="Screenshot 7"></p>
</li>
<li>
<p>Obtain the OAuth token by going to OAuth &amp; Permissions and copying the OAuth Token. Use this token as <code>SLACK_BOT_TOKEN</code>.</p>
<p><img src="https://github.com/seratch/ChatGPT-in-Slack/assets/2420543/ce8eff39-305c-482c-98c6-ed9a03994b3b" alt="Screenshot 8">
<img src="https://github.com/seratch/ChatGPT-in-Slack/assets/2420543/96f2abe6-9bae-47f5-b08b-c569c138cf00" alt="Screenshot 9"></p>
</li>
</ol>
<h3 id="modifying-the-env-file">Modifying the .env File</h3>
<p>Follow these steps to modify the .env file:</p>
<ol>
<li>
<p>Copy the example env file using the following command:</p>
<pre tabindex="0"><code>cp -rfv .env.example .env
</code></pre></li>
<li>
<p>Open the .env file and update the values of <code>SLACK_APP_TOKEN</code> and <code>SLACK_BOT_TOKEN</code> with the tokens generated in the previous steps.</p>
</li>
<li>
<p>Additionally, if needed, modify the URL of the website to be indexed and set it as the value for <code>SITEMAP</code> in the .env file.</p>
</li>
</ol>
<h3 id="running-with-docker-compose">Running with Docker Compose</h3>
<p>To run the bot using Docker Compose, follow these steps.</p>
<p>Run the following command if you&rsquo;re using Docker and <code>docker-compose</code>:</p>
<pre tabindex="0"><code>docker-compose up
</code></pre><p>If you&rsquo;re running Docker with <code>docker compose</code>, use the following command:</p>
<pre tabindex="0"><code>docker compose up
</code></pre><p>By default, the local-ai setup will prepare and use the gpt4all-j model, which should work for most cases. However, if you want to change models, refer to the documentation or ask for assistance in the forums or Discord community.</p>
<h3 id="trying-it-out">Trying It Out!</h3>
<p>Once the bot starts successfully, you can ask it questions about the documentation in the designated channel. Check out this video for an example of how it works, including linking to the relevant sources in the documentation:</p>
<p><img src="https://github.com/spectrocloud-labs/Slack-QA-bot/assets/2420543/6047e1ff-22d5-4b03-9d73-fcb7fb19a2c1" alt="Kairos-TPM-Slackbot"></p>
<h3 id="bonus-setup-other-models">Bonus: Setup other models</h3>
<p>The <code>.env</code> file specifies to configure gpt4all automatically, however you can use other models by copying the manually in the models folder, or use the gallery:</p>
<pre tabindex="0"><code># See: https://github.com/go-skynet/model-gallery
PRELOAD_MODELS=[{&#34;url&#34;: &#34;github:go-skynet/model-gallery/gpt4all-j.yaml&#34;, &#34;name&#34;: &#34;gpt-3.5-turbo&#34;}]
</code></pre><p>The <code>PRELOAD_MODELS</code> environment variable in the <code>.env</code> file specifies the configuration for the <code>gpt-3.5-turbo</code> model. See also: <a href="https://github.com/go-skynet/model-gallery">https://github.com/go-skynet/model-gallery</a> in order to run other models from the gallery.</p>
<p>To run manually models, see the <code>chatbot-ui-manual</code> example in LocalAI, and comment the <code>PRELOAD_MODELS</code> environment variable.</p>
<h3 id="bonus-kubernetes-setup">Bonus: Kubernetes setup</h3>
<p>This is a manifest which can be used as a starting point:</p>
<pre tabindex="0"><code>apiVersion: v1
kind: Namespace
metadata:
  name: slack-bot
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: knowledgebase
  namespace: slack-bot
  labels:
    app: localai
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: localai
  namespace: slack-bot
  labels:
    app: localai
spec:
  selector:
    matchLabels:
      app: localai
  replicas: 1
  template:
    metadata:
      labels:
        app: localai
      name: localai
    spec:
      containers:
        - name: localai-slack
          env:
          - name: OPENAI_API_KEY
            value: &#34;x&#34;
          - name: SLACK_APP_TOKEN
            value: &#34;xapp-1-&#34;
          - name: SLACK_BOT_TOKEN
            value: &#34;xoxb-&#34;
          - name: OPENAI_MODEL
            value: &#34;gpt-3.5-turbo&#34;
          - name: OPENAI_TIMEOUT_SECONDS
            value: &#34;400&#34;
          - name: OPENAI_SYSTEM_TEXT
            value: &#34;&#34;
          - name: MEMORY_DIR
            value: &#34;/memory&#34;
          - name: TRANSLATE_MARKDOWN
            value: &#34;true&#34;
          - name: OPENAI_API_BASE
            value: &#34;http://local-ai.default.svc.cluster.local:8080&#34;
          - name: REPOSITORIES
            value: &#34;KAIROS,AGENT,SDK,OSBUILDER,PACKAGES,IMMUCORE&#34;
          - name: KAIROS_CLONE_URL
            value: &#34;https://github.com/kairos-io/kairos&#34;
          - name: KAIROS_BRANCH
            value: &#34;master&#34;
          - name: AGENT_CLONE_URL
            value: &#34;https://github.com/kairos-io/kairos-agent&#34;
          - name: AGENT_BRANCH
            value: &#34;main&#34;
          - name: SDK_CLONE_URL
            value: &#34;https://github.com/kairos-io/kairos-sdk&#34;
          - name: SDK_BRANCH
            value: &#34;main&#34;
          - name: OSBUILDER_CLONE_URL
            value: &#34;https://github.com/kairos-io/osbuilder&#34;
          - name: OSBUILDER_BRANCH
            value: &#34;master&#34;
          - name: PACKAGES_CLONE_URL
            value: &#34;https://github.com/kairos-io/packages&#34;
          - name: PACKAGES_BRANCH
            value: &#34;main&#34;
          - name: IMMUCORE_CLONE_URL
            value: &#34;https://github.com/kairos-io/immucore&#34;
          - name: IMMUCORE_BRANCH
            value: &#34;master&#34;
          - name: GITHUB_PERSONAL_ACCESS_TOKEN
            value: &#34;&#34;
          - name: ISSUE_REPOSITORIES
            value: &#34;kairos-io/kairos&#34;
          image: quay.io/spectrocloud-labs/slack-qa-local-bot:qa
          imagePullPolicy: Always
          volumeMounts:
            - mountPath: &#34;/memory&#34;
              name: knowledgebase
      volumes:
        - name: knowledgebase
          persistentVolumeClaim:
            claimName: knowledgebase
</code></pre><p>Note:</p>
<ul>
<li><code>OPENAI_API_BASE</code> is set to the default if installing the <code>local-ai</code> chart into the default namespace listening on 8080. Specify a different LocalAI url here.</li>
</ul>
<h2 id="stay-updated">Stay updated</h2>
<p>If you want to stay-up-to-date on my latest posts or what I am to follow me on Twitter at <a href="https://twitter.com/mudler_it/">@mudler</a> and on <a href="https://github.com/mudler">Github</a>.</p>
]]></content>
        </item>
        
        <item>
            <title>Question Answering on Documents locally with LangChain, LocalAI, Chroma, and GPT4All</title>
            <link>https://mudler.pm/posts/localai-question-answering/</link>
            <pubDate>Fri, 12 May 2023 00:00:00 +0000</pubDate>
            
            <guid>https://mudler.pm/posts/localai-question-answering/</guid>
            <description>Have you ever dreamed of building AI-native applications that can leverage the power of large language models (LLMs) without relying on expensive cloud services or complex infrastructure? If so, you’re not alone. Many developers are looking for ways to create and deploy AI-powered solutions that are fast, flexible, and cost-effective, or just experiment locally. In this blog post, I’m going to show you how you can use three amazing tools and a language model like gpt4all to : LangChain, LocalAI, and Chroma.</description>
            <content type="html"><![CDATA[<p>Have you ever dreamed of building AI-native applications that can leverage the power of large language models (LLMs) without relying on expensive cloud services or complex infrastructure? If so, you’re not alone. Many developers are looking for ways to create and deploy AI-powered solutions that are fast, flexible, and cost-effective, or just experiment locally. In this blog post, I’m going to show you how you can use three amazing tools and a language model like gpt4all to : LangChain, LocalAI, and Chroma.</p>
<ul>
<li>
<p><a href="https://www.trychroma.com/">Chroma</a> is a vector store and embedding database designed for AI workloads.</p>
</li>
<li>
<p><a href="https://gpt4all.io/index.html">Gpt4all</a> is an ecosystem of open-source chatbots trained on a massive collections of clean assistant data including code, stories and dialogue.</p>
</li>
<li>
<p><a href="https://github.com/go-skynet/LocalAI">LocalAI</a> is a self-hosted, community-driven, local OpenAI-compatible API that can run on CPU with consumer-grade hardware. It enables you to run models locally or on-prem without the need for internet connectivity or external servers.</p>
</li>
<li>
<p><a href="https://python.langchain.com/en/latest/">LangChain</a> is a modular and flexible framework for developing AI-native applications using LLMs.</p>
</li>
</ul>
<p>Together, these four tools form a powerful combination that can help you create and deploy AI-native applications with ease and efficiency. In the following sections, I’ll show you how to use them in practice to do question answering on a document.</p>
<h2 id="localai">LocalAI</h2>
<p><img src="https://user-images.githubusercontent.com/2420543/233147843-88697415-6dbf-4368-a862-ab217f9f7342.jpeg" alt=""></p>
<p><a href="https://github.com/go-skynet/LocalAI">LocalAI</a> is a drop-in replacement REST API compatible with OpenAI for local CPU inferencing. It allows you to run models locally or on-prem with consumer grade hardware, supporting multiple models families. LocalAI is a community-driven project, focused on making the AI accessible to anyone.</p>
<p>LocalAI uses C++ bindings for optimizing speed and performance. It is based on <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> gpt4all, rwkv.cpp, ggml, whisper.cpp for audio transcriptions, and bert.cpp for embedding. LocalAI also supports GPT4ALL-J which is licensed under Apache 2.0, and MosaicLM PT models which are also usable for commercial applications. (see <a href="https://github.com/go-skynet/LocalAI#model-compatibility-table">here the table of supported models</a>)</p>
<p>To use LocalAI, you need to install it on your machine and run it as a service, or either on the cloud or in a dedicated environment. You can then use the same API endpoints as OpenAI to interact with the models. For example, you can use the <code>/v1/models</code> endpoint to list the available models, or the <code>/v1/completions</code> endpoint to generate text completions, but computation is executed locally, with CPU-compatible models. You can download models from <a href="https://gpt4all.io/index.html">Gpt4all</a>.</p>
<p>LocalAI also supports various ranges of configuration and prompt templates, which are predefined prompts that can help you generate specific outputs with the models. For example, you can use the summarizer template to generate summaries of texts, or the sentiment-analyzer template to analyze the sentiment of texts. You can find more <a href="https://github.com/go-skynet/LocalAI/tree/master/examples">examples</a> and <a href="https://github.com/go-skynet/LocalAI/tree/master/prompt-templates">prompt templates</a> in the <a href="https://github.com/go-skynet/LocalAI">LocalAI</a> repository.</p>
<h2 id="langchain-and-chroma">Langchain and Chroma</h2>
<p><img src="https://blog.langchain.dev/content/images/size/w1000/2023/02/langchain-chroma-light.png" alt=""></p>
<p><a href="https://www.trychroma.com/">Chroma</a> is a vector store and embedding database designed for AI workloads. It allows you to store and work with embeddings, which are the AI-native way to represent any kind of data. It also offers high performance and flexibility for working with different types of embeddings and algorithms. Chroma was founded to build tools that leverage the power of embeddings. Embeddings are the perfect fit for working with all kinds of AI-powered tools and algorithms, such as LLMs, semantic search, example selection, and more.</p>
<p><a href="https://python.langchain.com/en/latest/">LangChain</a> is a modular and flexible framework for developing AI-native applications using LLMs. It allows you to easily prototype and experiment with different models, data sources, and use cases, such as chat bots, question answering services, and agents. LangChain also provides a rich ecosystem of integrations with other tools and platforms, such as Notion, PDFs, ClearML, CerebriumAI, and more.</p>
<p>Together, those are powerful and convenient tools that can help you store and work with embeddings in a simple and efficient way. It can also help you enhance your LLM applications with pluggable knowledge, facts, and skills. In the next section, I’ll show you how to use LangChain and Chroma together with LocalAI to create and deploy AI-native applications locally.</p>
<h2 id="question-answering-with-localai-chromadb-and-langchain">Question answering with LocalAI, ChromaDB and Langchain</h2>
<p>In this example, I&rsquo;ll show you how to use <code>LocalAI</code> with the <code>gpt4all</code> models with <code>LangChain</code> and <code>Chroma</code> to enable question answering on a set of documents. We’ll use the state of the union speeches from different US presidents as our data source, and we’ll use the <code>ggml-gpt4all-j</code> model served by LocalAI to generate answers. Note, you can use any model compatible with <code>LocalAI</code>.</p>
<p>To run this example, you’ll need to have LocalAI, LangChain, and Chroma installed on your machine. You’ll also need to download the models and the data files from the links provided below. Alternatively, you can use the docker-compose file to start the LocalAI API and the Chroma service with the models and data already loaded.</p>
<p>The example consists of two steps: creating a storage and querying the storage. In the first step, we’ll use LangChain and Chroma to create a local vector database from our document set. This will allow us to perform semantic search on the documents using embeddings. In the second step, we’ll use LangChain and LocalAI to query the storage using natural language questions. We’ll use the gpt4all model served by LocalAI using the <em>OpenAI</em> api and python client to generate answers based on the most relevant documents. The key aspect here is that we will configure the python client to use the LocalAI API endpoint instead of OpenAI.</p>
<h3 id="step-1-start-localai">Step 1: Start LocalAI</h3>
<p>To start LocalAI, we can either build it locally or use <code>docker-compose</code>. Note the steps here are for <code>Linux</code> machines. If you are on <code>Mac</code> you need to build the binary manually:</p>
<pre tabindex="0"><code># Clone LocalAI
git clone https://github.com/go-skynet/LocalAI

cd LocalAI/examples/langchain-chroma

# Download models
wget https://huggingface.co/skeskinen/ggml/resolve/main/all-MiniLM-L6-v2/ggml-model-q4_0.bin -O models/bert
wget https://gpt4all.io/models/ggml-gpt4all-j.bin -O models/ggml-gpt4all-j

docker-compose up
</code></pre><p>This will start the LocalAI server locally, with the models required for embeddings (<code>bert</code>) and for question answering (<code>gpt4all</code>).</p>
<p>Note: The example contains a <code>models</code> folder with the configuration for <code>gpt4all</code> and the <code>embeddings</code> models already prepared. LocalAI will map gpt4all to <code>gpt-3.5-turbo</code> model, and <code>bert</code> to the embeddings endpoints.</p>
<h3 id="step-2-create-a-vector-database">Step 2: Create a vector database</h3>
<p>To create a vectore database, we’ll use a script which uses LangChain and Chroma to create a collection of documents and their embeddings. The script takes a text file as input, where each line is a document. In our case, we’ll use the <code>state_of_the_union.txt</code> file, which we will use to ask it questions later for.</p>
<p>Download the data:</p>
<pre tabindex="0"><code># Download data used for training
wget https://raw.githubusercontent.com/hwchase17/chat-your-data/master/state_of_the_union.txt
</code></pre><p>The python OpenAI client allows to set an API key and a API target host with those two environment variables:</p>
<ul>
<li><code>OPENAI_API_BASE</code>: The base URL of the OpenAI API. If we run LocalAI locally, we set it to http://localhost:8080/v1</li>
<li><code>OPENAI_API_KEY</code>: The API key for the OpenAI API. We have to set it to something, but doesn&rsquo;t really matter</li>
</ul>
<p>Our script will do the following:</p>
<ul>
<li>It imports the necessary modules from the langchain library and the os module.</li>
<li>It sets the base_path variable to the OpenAI API endpoint, which is used to access the OpenAI embeddings model.</li>
<li>It creates a <code>TextLoader</code> object that loads the text file and returns a list of documents (each document is a string).</li>
<li>It creates a <code>CharacterTextSplitter</code> object that splits each document into smaller chunks of 300 characters with an overlap of 70 characters.
<ul>
<li>Alternatively, it can use a <code>TokenTextSplitter</code> object that splits each document into tokens (words or punctuation marks) or <code>RecursiveCharacterTextSplitter</code>.</li>
</ul>
</li>
<li>It creates an OpenAIEmbeddings object that uses the <code>text-embedding-ada-002</code> model to generate embeddings (numeric representations) for each chunk of text. Since we set <code>OPENAI_API_BASE</code> it will use LocalAI instead.</li>
<li>It creates a Chroma object that stores the embeddings in a vector database. It also specifies a persist_directory where the embeddings are saved on disk.</li>
<li>It calls the persist method to save the embeddings.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.vectorstores <span style="color:#f92672">import</span> Chroma
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.embeddings <span style="color:#f92672">import</span> OpenAIEmbeddings
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.text_splitter <span style="color:#f92672">import</span> RecursiveCharacterTextSplitter,TokenTextSplitter,CharacterTextSplitter
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.llms <span style="color:#f92672">import</span> OpenAI
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.chains <span style="color:#f92672">import</span> VectorDBQA
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.document_loaders <span style="color:#f92672">import</span> TextLoader
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>base_path <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>environ<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#39;OPENAI_API_BASE&#39;</span>, <span style="color:#e6db74">&#39;http://localhost:8080/v1&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load and process the text</span>
</span></span><span style="display:flex;"><span>loader <span style="color:#f92672">=</span> TextLoader(<span style="color:#e6db74">&#39;state_of_the_union.txt&#39;</span>)
</span></span><span style="display:flex;"><span>documents <span style="color:#f92672">=</span> loader<span style="color:#f92672">.</span>load()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>text_splitter <span style="color:#f92672">=</span> CharacterTextSplitter(chunk_size<span style="color:#f92672">=</span><span style="color:#ae81ff">300</span>, chunk_overlap<span style="color:#f92672">=</span><span style="color:#ae81ff">70</span>)
</span></span><span style="display:flex;"><span><span style="color:#75715e">#text_splitter = TokenTextSplitter()</span>
</span></span><span style="display:flex;"><span>texts <span style="color:#f92672">=</span> text_splitter<span style="color:#f92672">.</span>split_documents(documents)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Embed and store the texts</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Supplying a persist_directory will store the embeddings on disk</span>
</span></span><span style="display:flex;"><span>persist_directory <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;db&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>embedding <span style="color:#f92672">=</span> OpenAIEmbeddings(model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;text-embedding-ada-002&#34;</span>)
</span></span><span style="display:flex;"><span>vectordb <span style="color:#f92672">=</span> Chroma<span style="color:#f92672">.</span>from_documents(documents<span style="color:#f92672">=</span>texts, embedding<span style="color:#f92672">=</span>embedding, persist_directory<span style="color:#f92672">=</span>persist_directory)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>vectordb<span style="color:#f92672">.</span>persist()
</span></span><span style="display:flex;"><span>vectordb <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span></code></pre></div><p>To run the script, execute the following commands:</p>
<pre tabindex="0"><code># Set environment variables
export OPENAI_API_BASE=http://localhost:8080/v1
export OPENAI_API_KEY=sk-

# Run store.py script
python store.py
</code></pre><p>After it finishes, a directory named <code>db</code> will be created with the vector index database.</p>
<h3 id="step-3-query-the-storage">Step 3: Query the storage</h3>
<p>Now we can query our vector database with <code>gpt4all</code>.</p>
<p>To query the storage, we’ll use a python script which uses LangChain and LocalAI to generate answers from natural language questions.</p>
<p>Our script will do the following:</p>
<ul>
<li>It imports the necessary modules from LangChain and os.</li>
<li>It calculate embeds the question using the OpenAIEmbeddings class, which uses the OpenAI API to generate embeddings for each text chunk. - However, since we set <code>OPENAI_API_BASE</code> it will use <strong>LocalAI</strong> instead. It also uses the Chroma class, which is a vector store that can persist the embeddings on disk for later use.</li>
<li>It creates a question answering system using the VectorDBQA class, which can query the vector store using natural language questions. It also uses the OpenAI class, which is a wrapper for the OpenAI API that can specify parameters such as temperature and model name. In this case, it uses the <code>gpt-3.5-turbo</code> model, and LocalAI is configured to redirect requests to the <code>gpt4all</code> model instead.</li>
<li>Finally, it runs a sample query on the question answering system, asking “What the president said about taxes ?” and prints the answer.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.vectorstores <span style="color:#f92672">import</span> Chroma
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.embeddings <span style="color:#f92672">import</span> OpenAIEmbeddings
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.llms <span style="color:#f92672">import</span> OpenAI
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.chains <span style="color:#f92672">import</span> VectorDBQA
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>base_path <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>environ<span style="color:#f92672">.</span>get(<span style="color:#e6db74">&#39;OPENAI_API_BASE&#39;</span>, <span style="color:#e6db74">&#39;http://localhost:8080/v1&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load and process the text</span>
</span></span><span style="display:flex;"><span>embedding <span style="color:#f92672">=</span> OpenAIEmbeddings()
</span></span><span style="display:flex;"><span>persist_directory <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;db&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Now we can load the persisted database from disk, and use it as normal. </span>
</span></span><span style="display:flex;"><span>vectordb <span style="color:#f92672">=</span> Chroma(persist_directory<span style="color:#f92672">=</span>persist_directory, embedding_function<span style="color:#f92672">=</span>embedding)
</span></span><span style="display:flex;"><span>qa <span style="color:#f92672">=</span> VectorDBQA<span style="color:#f92672">.</span>from_chain_type(llm<span style="color:#f92672">=</span>OpenAI(temperature<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, model_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;gpt-3.5-turbo&#34;</span>, openai_api_base<span style="color:#f92672">=</span>base_path), chain_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;stuff&#34;</span>, vectorstore<span style="color:#f92672">=</span>vectordb)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>query <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;What the president said about taxes ?&#34;</span>
</span></span><span style="display:flex;"><span>print(qa<span style="color:#f92672">.</span>run(query))
</span></span></code></pre></div><p>To run the script, execute the following commands:</p>
<pre tabindex="0"><code># Set environment variables
export OPENAI_API_BASE=http://localhost:8080/v1
export OPENAI_API_KEY=sk-

# Run query.py
python query.py
# President Trump recently stated during a press conference regarding tax reform legislation that &#34;we&#39;re getting rid of all these loopholes.&#34; He also mentioned that he wants to simplify the system further through changes such as increasing the standard deduction amount and making other adjustments aimed at reducing taxpayers&#39; overall burden.    
</code></pre><h2 id="conclusions">Conclusions:</h2>
<p>In this blog post, we showed you how to use LangChain and Chroma together with LocalAI to enable question answering on a set of documents. We used the state of the union speeches from different US presidents as our data source, and we used the ggml-gpt4all-j model from LocalAI to generate answers. We also used Chroma as a vector store and embedding database to perform semantic search on the documents using embeddings.</p>
<p>This is just one example of how you can use these three amazing tools to create and deploy AI-native applications with ease and efficiency. You can also use them for other use cases, such as chat bots, agents, summarizers, sentiment analyzers, and more. You can also use different models, data sources, and integrations with other tools and platforms.</p>
<p>We hope you enjoyed this blog post and learned something new. If you want to try it out for yourself, you can find the code and the instructions in <a href="https://github.com/go-skynet/LocalAI/tree/master/examples/langchain-chroma">this GitHub repo</a>. You can also find more resources and documentation for LangChain, LocalAI, and Chroma in the links below.</p>
<ul>
<li>Chroma: <a href="https://docs.trychroma.com/">https://docs.trychroma.com/</a></li>
<li>GPT4all: <a href="https://github.com/nomic-ai/gpt4all">https://github.com/nomic-ai/gpt4all</a></li>
<li>LangChain: <a href="https://python.langchain.com/en/latest/">https://python.langchain.com/en/latest/</a></li>
<li>LocalAI: <a href="https://github.com/go-skynet/LocalAI">https://github.com/go-skynet/LocalAI</a></li>
</ul>
<p>We’d love to hear your feedback and suggestions on how to improve these tools and make them more useful for you. You can join our Discord channels and chat with us and other developers:</p>
<h2 id="links">Links</h2>
<ul>
<li>Full example code: <a href="https://github.com/go-skynet/LocalAI/tree/master/examples/langchain-chroma">https://github.com/go-skynet/LocalAI/tree/master/examples/langchain-chroma</a></li>
<li>LocalAI examples: <a href="https://github.com/go-skynet/LocalAI/tree/master/examples">https://github.com/go-skynet/LocalAI/tree/master/examples</a></li>
<li>Github: <a href="https://github.com/go-skynet/LocalAI">https://github.com/go-skynet/LocalAI</a></li>
<li>Follow us on Twitter: <a href="https://twitter.com/LocalAI_API">https://twitter.com/LocalAI_API</a></li>
<li>Upvote on Hacker news: <a href="https://news.ycombinator.com/item?id=35726934">https://news.ycombinator.com/item?id=35726934</a></li>
<li>Join our Discord: <a href="https://discord.gg/uJAeKSAGDy">https://discord.gg/uJAeKSAGDy</a></li>
</ul>
<p>Thank you for reading and happy coding!</p>
]]></content>
        </item>
        
        <item>
            <title>LocalAI updates</title>
            <link>https://mudler.pm/posts/localai/</link>
            <pubDate>Thu, 11 May 2023 00:00:00 +0000</pubDate>
            
            <guid>https://mudler.pm/posts/localai/</guid>
            <description>LocalAI has been a massive hit! Thanks to everyone who has shown support by following us. Your contribution is helping to democratize AI!
If you&amp;rsquo;re not familiar with LocalAI, it&amp;rsquo;s a self-hostable, free, and open-source alternative to the OpenAI API. It simplifies the AI development process and makes it accessible to everyone. You can check it out on Github: https://github.com/go-skynet/LocalAI.
News We have exciting news to share in the latest release (1.</description>
            <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/2420543/233147843-88697415-6dbf-4368-a862-ab217f9f7342.jpeg" alt=""></p>
<p>LocalAI has been a massive hit! Thanks to everyone who has shown support by following us. Your contribution is helping to democratize AI!</p>
<p>If you&rsquo;re not familiar with LocalAI, it&rsquo;s a self-hostable, free, and open-source alternative to the OpenAI API. It simplifies the AI development process and makes it accessible to everyone. You can check it out on Github: <a href="https://github.com/go-skynet/LocalAI">https://github.com/go-skynet/LocalAI</a>.</p>
<h2 id="news">News</h2>
<p>We have exciting news to share in the latest release (1.9.0).
The GPT4All binding family models have been updated, as we are upstreaming our implementations (<a href="https://github.com/nomic-ai/gpt4all/pull/534)">https://github.com/nomic-ai/gpt4all/pull/534)</a>, and now all models in <a href="https://gpt4all.io/index.html">https://gpt4all.io/index.html</a> should work, including token stream back to the client, for additional glamourous effects :) . Additionally, that means added support for MosaicML MPT models (<a href="https://huggingface.co/mosaicml/mpt-7b">https://huggingface.co/mosaicml/mpt-7b</a>) and I&rsquo;ve been looking at adding experimental support for bloomz models with <a href="https://github.com/NouamaneTazi/bloomz.cpp">bloomz.cpp</a>.</p>
<p>In the previous release (1.8.x), LocalAI gained support for audio transcription with <a href="https://github.com/ggerganov/whisper.cpp">whisper.cpp</a> and support for embeddings with <a href="https://github.com/skeskinen/bert.cpp">bert.cpp</a>. bert.cpp embedding extends support for any model, allowing you to implement question answering on large datasets with any model that doesn&rsquo;t support embedding by its own backend.</p>
<h2 id="localai-in-the-wild">LocalAI in the wild</h2>
<p>What you can do with LocalAI now?</p>
<ul>
<li>Question answering</li>
<li>Langchain</li>
<li>Chatbot UI</li>
</ul>
<p>and many more! Have a look at the examples <a href="https://github.com/go-skynet/LocalAI/tree/master/examples">https://github.com/go-skynet/LocalAI/tree/master/examples</a></p>
<h2 id="thank-you">Thank you</h2>
<p>I owe our success to our amazing community. I would like to thank llama.cpp, gpt4all, rwkv.cpp, ggml, whisper.cpp, and bert.cpp for providing us with excellent community software pieces.</p>
<h2 id="next">Next</h2>
<p>There are constantly new models out there. Some hot topics on our roadmap:</p>
<ul>
<li>Model gallery</li>
<li>RedPajama/Starcoder models</li>
<li>GPTNeoX</li>
<li>Dolly</li>
</ul>
<h2 id="call-for-maintainers">Call for maintainers</h2>
<p>LocalAI, and my mission is to make AI accessible for everyone - if you want to have fun with Golang and C++, or contribute to an open source project, feel free to reach out and become a maintainer!</p>
<h2 id="community-links">Community links</h2>
<ul>
<li>LocalAI examples: <a href="https://github.com/go-skynet/LocalAI/tree/master/examples">https://github.com/go-skynet/LocalAI/tree/master/examples</a></li>
<li>Github: <a href="https://github.com/go-skynet/LocalAI">https://github.com/go-skynet/LocalAI</a></li>
<li>Follow us on Twitter: <a href="https://twitter.com/LocalAI_API">https://twitter.com/LocalAI_API</a></li>
<li>Upvote on Hacker news: <a href="https://news.ycombinator.com/item?id=35726934">https://news.ycombinator.com/item?id=35726934</a></li>
<li>Join our Discord: <a href="https://discord.gg/uJAeKSAGDy">https://discord.gg/uJAeKSAGDy</a></li>
</ul>
<p>Join us in making LocalAI the go-to open-source, self-hostable AI API replacement for local inference with CPU. With your support, we can democratize AI and make it accessible to everyone!</p>
]]></content>
        </item>
        
        <item>
            <title>Whut?! Sabayon Community Repositories is here!</title>
            <link>https://mudler.pm/2016/04/whut-sabayon-community-repositories-is.html</link>
            <pubDate>Mon, 18 Apr 2016 06:02:00 -0700</pubDate>
            
            <guid>https://mudler.pm/2016/04/whut-sabayon-community-repositories-is.html</guid>
            <description>Maaaaaaaaaaaaany dependencies for enman… Still, he&amp;hellip; Tedel - Apr 1, 2016
Maaaaaaaaaaaaany dependencies for enman…
Still, here we go! :D</description>
            <content type="html"><![CDATA[<h4 id="maaaaaaaaaaaaany-dependencies-for-enman-still-he">Maaaaaaaaaaaaany dependencies for enman… Still, he&hellip;</h4>
<p><a href="https://www.blogger.com/profile/05988645683809592011" title="noreply@blogger.com">Tedel</a> - <!-- raw HTML omitted -->Apr 1, 2016<!-- raw HTML omitted --></p>
<p>Maaaaaaaaaaaaany dependencies for enman…<br>
Still, here we go! :D</p>
<!-- raw HTML omitted -->
]]></content>
        </item>
        
        <item>
            <title>Whut?! Sabayon Community Repositories is here!</title>
            <link>https://mudler.pm/2016/04/whut-sabayon-community-repositories-is.html</link>
            <pubDate>Mon, 18 Apr 2016 06:02:00 -0700</pubDate>
            
            <guid>https://mudler.pm/2016/04/whut-sabayon-community-repositories-is.html</guid>
            <description>Hello users,
Time passed when i first wrote the article &amp;ldquo;[Part 1] Building Gentoo and Sabayon packages in your machine locally or remotely with Docker and Cloud Services&amp;rdquo; that was going to be divided into parts. But things changed amazingly fast and there is better news, I wrote up a suite of tools that makes the whole operation much easier, thanks also to +Ben Roberts (optiz0r) who contributed in the development and he is also kindly sponsoring the buildserver.</description>
            <content type="html"><![CDATA[<p>Hello users,</p>
<p>Time passed when i first wrote the article &ldquo;<a href="https://blog.mudler.pm/2015/11/part-1-building-gentoo-and-sabayon.html">[Part 1] Building Gentoo and Sabayon packages in your machine locally or remotely with Docker and Cloud Services</a>&rdquo; that was going to be divided into parts. But things changed amazingly fast and there is better news, I wrote up a suite of tools that makes the whole operation much easier, thanks also to <a href="https://plus.google.com/104283756158326784792">+Ben Roberts</a> (optiz0r) who contributed in the development and he is also kindly sponsoring the buildserver.</p>
<p>Some of you probably already heard about Sabayon Community Repositories (SCR) that we announced  in the developers mailing list some weeks ago, if not, keep reading. The SCR Build System is an improvement of the methodology explicated <a href="https://blog.mudler.pm/2015/11/part-1-building-gentoo-and-sabayon.html">before</a>, but with a lot of extras and automated features. This is how SCR is born.</p>
<p><strong>TL;DR</strong><br>
Information and how-to use: <a href="https://wiki.sabayon.org/index.php?title=En:Sabayon_Community_Repositories">Wiki page</a> (under construction)<br>
Search available packages: <a href="https://sabayon.github.io/community-website/">SCR website</a><br>
Packages requests: <a href="https://bugs.sabayon.org/enter_bug.cgi?product=Community%20Repositories">Bugzilla section</a><br>
Contributing/setup a local development environment: <a href="https://wiki.sabayon.org/index.php?title=HOWTO:_Contributing_to_SCR">How-to contribute</a><br>
Guidelines and Bylaws (for SCR Devs): <a href="https://github.com/Sabayon/scr-docs">scr-docs Github repository</a></p>
<h2 id="sabayon-community-repositories">Sabayon Community Repositories</h2>
<p>SCR is a collection of repositories available to Sabayon users, that could be enabled/removed from the system by using <strong>enman</strong> &ndash; our layman equivalent &ndash; this ensures not only that a user can easily revert his mistakes ( SCR content <strong>is not</strong> as stable as main repos, and we put a lot of warnings in the website for this reason) but also that it&rsquo;s easier and faster than ever now having bleeding edge packages that are not in the official channels.<br>
Users can search packages in the <a href="https://sabayon.github.io/community-website/">SCR website</a>, that will display information on how to install them.</p>
<p>More information on how to use SCR is available in the <a href="https://wiki.sabayon.org/index.php?title=En:Sabayon_Community_Repositories">wiki page</a> (under construction). Meanwhile we are currently working on the project documentation: <a href="https://github.com/Sabayon/scr-docs/blob/master/scr-guidelines.md">Guidelines</a> and <a href="https://github.com/Sabayon/scr-docs/blob/master/scr-bylaws.md">Bylaws</a> can be found on the <a href="https://github.com/Sabayon/scr-docs">scr-docs Github repository</a>.</p>
<h2 id="usage">Usage</h2>
<p>Open a terminal and run the following as root!</p>
<p><strong>1)</strong> Disable weekly repositories and enable current:</p>
<pre tabindex="0"><code>sudo mv /etc/entropy/repositories.conf.d/entropy\_sabayonlinux.org{.example,}
</code></pre><p>equo repo disable sabayon-weekly</p>
<pre tabindex="0"><code>equo repo enable sabayonlinux.org
```

  

**2)** Update/upgrade your machine

  

```
 equo up &amp;&amp; sudo equo u
```

  

**3)** Install **enman**:

  

```
 equo i enman 
```

  

We are now ready to use SCR!  
  
  
  
**4)** Enable the community repository:  

  
```
 enman add community 
```  
Update just the community repository database:  
```
 equo up community
```  
Now we can search as usual packages with &#34;equo search&#34; or, you can list the repository content with &#34;equo query&#34;:  
```
 equo q list available community 
```  

Special Purpose Repositories
----------------------------

These repositories contain packages to satisfy a particular purpose and that are not suitable for inclusion in the &#34;community&#34; repository. These should be enabled with care, and only if you need the specific functionality they provide.  
  
Example special purpose repositories:  

*   &#34;**devel**&#34;: Contains live versions of core Sabayon packages, and can be used to develop future Sabayon improvements against upstream projects, such as the Calamares installer, or the Linux Kernel.
*   &#34;**gaming-live**&#34;: Contains bleeding edge graphics drivers which may add new features or hardware support but may contain bugs or cause crashes.
*   &#34;**kde-unstable**&#34;: Contains the very latest KDE packages, which haven&#39;t gone through the same level of QA as you would find for KDE versions made available via Entropy. 
*   &#34;**pentesting**&#34;: Contains various pentesting packages 

Development and Contributing
----------------------------

From a developer point of view, - feel free to skip that if you don&#39;t need to build your packages - the approach is more easier:  [we are using building specifications files that describes what the repository should contain](https://github.com/Sabayon/community-repositories) (and how to make that possible). The awesome side of this is that the entire infrastructure and the repository as well are replicable everytime. [In our wiki there is a page regarding on how to setup the infrastructure](https://wiki.sabayon.org/index.php?title=HOWTO:_Contributing_to_SCR) (the same we use on the Build Server) in your local machine, enabling you to locally-build repositories without having to care about setting up a chroot! just put in a configuration file the relevants bits and you are done!  
  

Package requests
----------------

If you don&#39;t mind on how all of this works, but you want a package to be added in the SCR repositories, feel free to open a bug request [in the specific section of our Bugzilla](https://bugs.sabayon.org/enter_bug.cgi?product=Community%20Repositories): be sure to specify if the ebuild is available in layman if you want it to get done faster.

  

Donations
---------

As you can see we are always busy to give you the Sabayon experience, but we really need your help now!  [Consider donating](https://www.sabayon.org/donate)!

[![](https://s-media-cache-ak0.pinimg.com/736x/72/e7/ff/72e7ff6ac7754f87122535431284090a.jpg)](https://s-media-cache-ak0.pinimg.com/736x/72/e7/ff/72e7ff6ac7754f87122535431284090a.jpg)

We don&#39;t use funds to buy alcohol, also if we&#39;d love to (remember Windows ME?). We need YOU to keep our systems operative and running!

Cheers!
</code></pre>]]></content>
        </item>
        
        <item>
            <title>Updates: Rpi2 joins to testing boards, Mate testing repository, Sabayon devkit</title>
            <link>https://mudler.pm/2016/02/updates-rpi2-joins-to-testing-boards.html</link>
            <pubDate>Sun, 07 Feb 2016 10:22:00 -0800</pubDate>
            
            <guid>https://mudler.pm/2016/02/updates-rpi2-joins-to-testing-boards.html</guid>
            <description>Updates, TMiD (This Month in the Dev world)
Something landed here! Thanks to +Ben Roberts for the donation!
ARM Marsala, the board that +Ben Roberts was so kind to donate has just landed here, and i&amp;rsquo;m very happy about it, because that means that i can test against the board again myself.
I&amp;rsquo;m glad also to announce that the ROS framework and some other goodies(among of them firefox and libreoffice) have been pushed into our ARM repo.</description>
            <content type="html"><![CDATA[<p>Updates, TMiD (This Month in the Dev world)</p>
<p><a href="http://the9gag.com/images/pictuers/thumb/eat_sleep_code_repeat.jpg"><img src="http://the9gag.com/images/pictuers/thumb/eat_sleep_code_repeat.jpg" alt=""></a></p>
<p>Something landed here! Thanks to <a href="https://plus.google.com/104283756158326784792">+Ben Roberts</a> for the donation!</p>
<h2 id="arm"> ARM</h2>
<p><a href="https://4.bp.blogspot.com/-2-6OxD55XIw/VreGFqrZAGI/AAAAAAAAOTg/QsV25IFm1dA/s1600/photo_2016-02-07_18-59-21.jpg"><img src="https://4.bp.blogspot.com/-2-6OxD55XIw/VreGFqrZAGI/AAAAAAAAOTg/QsV25IFm1dA/s320/photo_2016-02-07_18-59-21.jpg" alt=""></a></p>
<p>Marsala, the board that <a href="https://plus.google.com/104283756158326784792">+Ben Roberts</a> was so kind to donate has just landed here, and i&rsquo;m very happy about it, because that means that i can test against the board again myself.</p>
<p>I&rsquo;m glad also to announce that the ROS framework and some other goodies(among of them firefox and libreoffice) have been pushed into our ARM repo.</p>
<h2 id="devkit">Devkit</h2>
<p>Meanwhile in parallel i&rsquo;m also working on the Sabayon Devkit. It will contain tools that will be helpful to repository creation, management and so on, it will also hold together the common pieces that i use and see in the infra scripts.</p>
<p>I&rsquo;ll blog about it later, but as for now i wrote a <a href="https://github.com/Sabayon/devkit">small howto-readme in the github repository.</a></p>
<p>With the devkit i realized a small repository for testing Mate 1.10.2, if you want to help enable it and give us how much feedback as you can, because soon the upgrade will just happen:</p>
<h2 id="mate-in-testing">Mate in testing</h2>
<p>in <strong>/etc/entropy/repositories.conf.d</strong> createthe file <strong>entropy_mate</strong> with the following content:</p>
<blockquote>
<p>[mate]<br>
desc = Mate testing Sabayon repository<br>
repo=http://mirror1.mirror.garr.it/mirrors/spike/mate/#bz2<br>
enabled = true<br>
pkg = <a href="http://mirror1.mirror.garr.it/mirrors/spike/mate/">http://mirror1.mirror.garr.it/mirrors/spike/mate/</a></p>
</blockquote>
<h2 id="website">Website</h2>
<p>New website is in development, stay tuned for updates</p>
<p>Cheers and thank you for your support!</p>
]]></content>
        </item>
        
        <item>
            <title>Tech Preview: Sabayon on RaspberryPi2 </title>
            <link>https://mudler.pm/2016/01/tech-preview-sabayon-on-raspberrypi2.html</link>
            <pubDate>Mon, 11 Jan 2016 05:02:00 -0800</pubDate>
            
            <guid>https://mudler.pm/2016/01/tech-preview-sabayon-on-raspberrypi2.html</guid>
            <description>hi, this is a really good new are there chances o&amp;hellip; Unknown - Feb 6, 2016
hi, this is a really good new
are there chances of having sabayon on Raspberry PI (not PI2)?</description>
            <content type="html"><![CDATA[<h4 id="hi-this-is-a-really-good-new-are-there-chances-o">hi, this is a really good new are there chances o&hellip;</h4>
<p><a href="https://www.blogger.com/profile/14210313797077979642" title="noreply@blogger.com">Unknown</a> - <!-- raw HTML omitted -->Feb 6, 2016<!-- raw HTML omitted --></p>
<p>hi, this is a really good new</p>
<p>are there chances of having sabayon on Raspberry PI (not PI2)?</p>
<!-- raw HTML omitted -->
]]></content>
        </item>
        
        <item>
            <title>Tech Preview: Sabayon on RaspberryPi2 </title>
            <link>https://mudler.pm/2016/01/tech-preview-sabayon-on-raspberrypi2.html</link>
            <pubDate>Mon, 11 Jan 2016 05:02:00 -0800</pubDate>
            
            <guid>https://mudler.pm/2016/01/tech-preview-sabayon-on-raspberrypi2.html</guid>
            <description>As anticipated on Sabayon Linux site releases notes, that day was going to be close.
ARM meet Sabayon, Sabayon meet ARM.
The approach to the ARM(hfp) support will be different from the previous attempt, we are not going to support kernels for each different board we intend to build images, instead we will release cutted images with vendor-kernel to avoid incompatibilities and unexpected features. This is what almost every distribution does for tons of reasons, among them i want just to underline that in those years we have seen a lot of new ARM boards out of there and we can&amp;rsquo;t just support all of them, mantain a kernel branch for each one would results low QA (since our dev-team is small) and probably in hard decision to be made when support will be dropped from vendors (think also on how short is a board lifespan, and how projects dies quickly).</description>
            <content type="html"><![CDATA[<p>As anticipated on Sabayon Linux site releases notes, that day was going to be close.<br>
<strong>ARM</strong> meet <strong>Sabayon</strong>, <strong>Sabayon</strong> meet <strong>ARM</strong>.</p>
<p><a href="https://www.raspberrypi.org/wp-content/uploads/2015/01/Pi2ModB1GB_-comp.jpeg"><img src="https://www.raspberrypi.org/wp-content/uploads/2015/01/Pi2ModB1GB_-comp.jpeg" alt=""></a></p>
<p>The approach to the ARM(hfp) support will be different from the previous attempt, we are not going to support  kernels for each different board we intend to build images, instead we will release cutted images with vendor-kernel to avoid incompatibilities and unexpected features. This is what almost every distribution does for tons of reasons, among them i want just to underline that in those years we have seen a lot of new ARM boards out of there and we can&rsquo;t just support all of them, mantain a kernel branch for each one would results low QA (since our dev-team is small) and probably in hard decision to be made when support will be dropped from vendors (think also on how short is a board lifespan, and how projects dies quickly). In that way, we can still provide support also for legacy devices as well.</p>
<p>If someone intends to support the various kernels it&rsquo;s free to do it by forking the project or just creating a separate overlay with kernel ebuilds, there is no limitations at all, the Sabayon kernel eclass is ready to handle most of the task if you intend to dig in that deep ocean. If you want to help us to support other boards, head over <a href="https://github.com/Sabayon/docker-armhfp">the GitHub repository</a> that contains the Docker bits to assemble the final images.</p>
<p>To sum up: we will release board specific images, and generic images that will suit for most every board. The generic image won&rsquo;t always work out of the box, some of them need custom specific vendor bits (U-boot) to be in the right place, but we will document each board we can manage to put our hands on.</p>
<p>Upgrading the kernel or playing with it will be entirely all up to you, we will provide of course a bunch of scripts to help that process.</p>
<p>**Sabayon will support the ARM packages, built from Gentoo&rsquo;s portage, providing you a stable, rolling release rootfs with tons of tools you can install with Entropy. **<br>
Enough chitchat for now, let&rsquo;s get to the business, let me present you our Tech Preview of RaspberryPi2. (I consider this a Tech Preview because it is really barebone, doesn&rsquo;t include any helpers and because it is still in a early stage of development)</p>
<p>The images will be soon available in all Sabayon mirrors, for now you can download it from the official TOP-IX italian mirror:</p>
<p><a href="http://mirror.it.sabayon.org/testing/Sabayon_Linux_16_armv7l_Raspberry_Pi2_Base_8GB.img.xz">http://mirror.it.sabayon.org/testing/Sabayon_Linux_16_armv7l_Raspberry_Pi2_Base_8GB.img.xz</a><br>
<a href="http://mirror.it.sabayon.org/testing/Sabayon_Linux_16_armv7l_Raspberry_Pi2_Base_8GB.img.xz.md5">http://mirror.it.sabayon.org/testing/Sabayon_Linux_16_armv7l_Raspberry_Pi2_Base_8GB.img.xz.md5</a></p>
<p>you can however browse <a href="https://www.sabayon.org/download">our mirror list</a>, the image is in the <em>testing/</em> folder.</p>
<p>To write the image to the sdcard, you can do:</p>
<blockquote>
<p>xzcat <a href="http://mirror.it.sabayon.org/testing/Sabayon_Linux_16_armv7l_Raspberry_Pi2_Base_8GB.img.xz">Sabayon_Linux_16_armv7l_Raspberry_Pi2_Base_8GB.img.xz</a> &gt; /dev/mmcblk0</p>
</blockquote>
<p>Where mmcblk0 can differ.<br>
The root password is: root. There is a default user &ldquo;sabayon&rdquo; with password &ldquo;sabayon&rdquo; that can use sudo out-of-the-box.<br>
The OS is set to automatically boot and start eth0 and sshd (so you can connect to it via ssh).<br>
I strongly reccomend also to do a <em>&ldquo;equo up &amp;&amp; equo u&rdquo;</em> after boot to get the latest packages.</p>
<p>The System is already configured to allow serial login, and if you want put your hands on kodi (yeah, so you can do your HTPC for almost 30$ without pain) install it with equo:</p>
<blockquote>
<p>equo up</p>
</blockquote>
<blockquote>
<p>equo i kodi-raspberrypi</p>
</blockquote>
<p>and then start it from the terminal(without X):</p>
<blockquote>
<p>startkodi</p>
</blockquote>
<p>We have already ~2000 packages in the ARMhfp repository, including Docker already cutted for ARM boards (yay!), XFCE, LXDE and bunch of packages you might find interesting. For package requests, head over <a href="https://bugs.sabayon.org/">our bugzilla</a>.</p>
<p>There is also the handy <em>rpi-update</em> script that helps you thru the kernel upgrade process, downloading automatically the latest <em>raspberrypi</em> kernel from the Raspberrypi official repositories (the one that you can use in Raspbian as well).</p>
<p>Please, report any issue you find.</p>
<p>If you appreciate our efforts towards the ARM architecture, please consider to donate us either hardware or <a href="https://www.sabayon.org/donate">MONEY</a> to buy it!</p>
<p>Have fun!</p>
]]></content>
        </item>
        
        <item>
            <title>[Part 1] Building Gentoo and Sabayon packages in your machine locally or remotely with Docker and Cloud Services</title>
            <link>https://mudler.pm/2015/11/part-1-building-gentoo-and-sabayon.html</link>
            <pubDate>Sun, 15 Nov 2015 07:09:00 -0800</pubDate>
            
            <guid>https://mudler.pm/2015/11/part-1-building-gentoo-and-sabayon.html</guid>
            <description>With spawning a compilation, the second line (with&amp;hellip; fusion809 - Nov 4, 2015
With spawning a compilation, the second line (with the package you wish to merge) is it possible to point to a specific version of a package. For example, atm I&amp;rsquo;d like to merge app-editors/atom-1.2.0 from the Sabayon overlay, do I have to enter it as `app-editors/atom-1.2.0 &amp;ndash;overlay sabayon` or `=app-editors/atom-1.2.0 &amp;ndash;overlay sabayon`?
In section 3 I think you gave incorrect syntax, although it may be because of formatting limitations imposed by blogger.</description>
            <content type="html"><![CDATA[<h4 id="with-spawning-a-compilation-the-second-line-with">With spawning a compilation, the second line (with&hellip;</h4>
<p><a href="https://www.blogger.com/profile/14316408440182908236" title="noreply@blogger.com">fusion809</a> - <!-- raw HTML omitted -->Nov 4, 2015<!-- raw HTML omitted --></p>
<p>With spawning a compilation, the second line (with the package you wish to merge) is it possible to point to a specific version of a package. For example, atm I&rsquo;d like to merge app-editors/atom-1.2.0 from the Sabayon overlay, do I have to enter it as `app-editors/atom-1.2.0 &ndash;overlay sabayon` or `=app-editors/atom-1.2.0 &ndash;overlay sabayon`?</p>
<!-- raw HTML omitted -->
<p>In section 3 I think you gave incorrect syntax, although it may be because of formatting limitations imposed by blogger. I think the two lines:</p>
<p>sudo docker run -ti &ndash;rm -v &ldquo;$PWD&rdquo;/artifacts:/usr/portage/packages sabayon/builder-amd64<br>
app-text/tree</p>
<p>should instead be provided over a single line, like:</p>
<p>sudo docker run -ti &ndash;rm -v &ldquo;$PWD&rdquo;/artifacts:/usr/portage/packages sabayon/builder-amd64 app-text/tree</p>
<p>I say so because whenever I run it as two lines I get the error message:</p>
<p>-&gt; You should feed me with something</p>
<p>Examples:</p>
<p>/builder app-text/tree<br>
/builder plasma-meta &ndash;layman kde</p>
<p>**************************</p>
<p>You can supply multiple overlays as well: /builder plasma-meta &ndash;layman kde plab</p>
<p>Died at /builder line 18.</p>
<!-- raw HTML omitted -->
<p>Yes, i&rsquo;m sorry blogger returns the line anyway</p>
<!-- raw HTML omitted -->
<p>like while using emerge, you have to use the equal sign to specify an exact version</p>
<!-- raw HTML omitted -->
<p>Have ya considered using the code-formatting HTML code provided by codeformatter.blogspot.com? In the case of your first command mentioned in the &ldquo;3) Spawning&hellip;&rdquo; section the HTML code would be: <a href="http://paste2.org/NJ9st0ke">http://paste2.org/NJ9st0ke</a> (using a pastebin as inserting the code here directly gave a &ldquo;HTML cannot be accepted&rdquo; error).</p>
<!-- raw HTML omitted -->
]]></content>
        </item>
        
        <item>
            <title>[Part 1] Building Gentoo and Sabayon packages in your machine locally or remotely with Docker and Cloud Services</title>
            <link>https://mudler.pm/2015/11/part-1-building-gentoo-and-sabayon.html</link>
            <pubDate>Sun, 15 Nov 2015 07:09:00 -0800</pubDate>
            
            <guid>https://mudler.pm/2015/11/part-1-building-gentoo-and-sabayon.html</guid>
            <description>As previously anticipated in the previous blog post, here i explain what&amp;rsquo;s going on with Sabayon Docker images, and how can the Official Sabayon images help you in developing/deploying your application. In this article, i&amp;rsquo;ll show how to build Sabayon/Gentoo packages using Docker.
I&amp;rsquo;ll cover five cases and will be divided in differents articles:
You want to build packages locally You want to remotely build packages You want to host an overlay or a Sabayon repository and want your packages to get built on each push to the git repository You want to track remote repository changes and test (useful if you can&amp;rsquo;t setup a webhook) Setup an Entropy repo with a Docker image, and how to mantain packages In future, i&amp;rsquo;ll write also a post also on how to track the packages remotely with CircleCi.</description>
            <content type="html"><![CDATA[<p><a href="http://1.bp.blogspot.com/-s-2KEJ0FoF8/VkiGi_oCjBI/AAAAAAAAMfg/JkAeUmowseI/s1600/cloud-47581_1280.png"><img src="http://1.bp.blogspot.com/-s-2KEJ0FoF8/VkiGi_oCjBI/AAAAAAAAMfg/JkAeUmowseI/s640/cloud-47581_1280.png" alt=""></a></p>
<p>As previously anticipated in the previous blog post, here i explain what&rsquo;s going on with Sabayon Docker images, and how can the Official Sabayon images help you in developing/deploying your application. In this article, i&rsquo;ll show how to build Sabayon/Gentoo packages using Docker.</p>
<p>I&rsquo;ll cover five cases and will be divided in differents articles:</p>
<ol>
<li>You want to build packages locally</li>
<li>You want to remotely build packages</li>
<li>You want to host an overlay or a Sabayon repository and want your packages to get built on each push to the git repository</li>
<li>You want to track remote repository changes and test (useful if you can&rsquo;t setup a webhook)</li>
<li>Setup an Entropy repo with a Docker image, and how to mantain packages</li>
</ol>
<p>In future, i&rsquo;ll write also a post also on how to track the packages remotely with CircleCi.</p>
<h2 id="introduction">Introduction</h2>
<p>It has been a while that i&rsquo;m putting my head on how i could leverage Cloud provider&rsquo;s free services to build packages and improve the QA of the building process at all. Continous Integration applied to Linux Distribution Developing process can lead to quite awesome perspectives. [I will digress on this on a future article]</p>
<p>For those who are unfamiliar with Docker..<strong>What the heck is Docker?</strong> From their site:</p>
<blockquote>
<p><em>Docker allows you to package an application with all of its dependencies into a standardized unit for software development.Docker containers wrap up a piece of software in a complete filesystem that contains everything it needs to run: code, runtime, system tools, system libraries – anything you can install on a server. This guarantees that it will always run the same, regardless of the environment it is running in.</em></p>
</blockquote>
<p>We can look at Docker like a box which can be filled to be used like an entire OS, but at the same time share the kernel with the host (sorry to be much semplicistic, we are not discussing about the Docker technlogy here).</p>
<h2 id="differents-images-differents-purposes">Differents images, differents purposes</h2>
<h4 id="the-sabayon-images-provides-the-software-stack-present-on-the-entropy-repositories-thus-allowing-easily-to-use-emerge-gentoo-package-manager-equo-sabayon-package-manager-and-eit-sabayon-repository-manager-in-an-every-linux-distribution-so-also-aliens-can-use-it">The Sabayon images provides the software stack present on the Entropy repositories, thus allowing easily to use emerge (Gentoo package manager), equo (Sabayon package manager) and Eit (Sabayon repository manager) in an every linux distribution (So, also aliens can use it).</h4>
<p>At Sabayon, we currently are providing various flavors of Docker images, the most useful for developers/users/hobbist are: </p>
<ul>
<li><strong><a href="https://hub.docker.com/r/sabayon/base-amd64/">sabayon/base-amd64</a></strong>: A light image that ships the minimum Sabayon package set</li>
<li><strong><a href="https://hub.docker.com/r/sabayon/spinbase-amd64/">sabayon/spinbase-amd64</a></strong>: It is based on base-amd64, and includes kernel, software that is required to create the SpinBase.iso images that finally hits our mirrors</li>
<li><strong><a href="https://hub.docker.com/r/sabayon/builder-amd64/">sabayon/builder-amd64</a></strong>: This image have all the packages required to compile packages with emerge, includes an helper script to do most common tasks automatically</li>
<li><strong><a href="https://hub.docker.com/r/sabayon/molecules-amd64/">sabayon/molecules-amd64</a></strong>: This image features the Sabayon Molecule tool with the molecules git repository inside, it is just an aim to easily let people burn their custom spins.</li>
</ul>
<p>The other images that you can see in our public profile are used to convert a Gentoo image to Sabayon, everything translated in Docker steps.</p>
<p>While the <strong>sabayon/base-amd64</strong> image can be used to deploy your services, and be the base of your applications container, the <strong>sabayon/builder-amd64</strong> is a ready-to-use image to build package, it&rsquo;s all already setup for you.</p>
<p>As already said, i&rsquo;m going to cover four cases here, then you are free to pick up the solution that suits you well.</p>
<p>In this part i&rsquo;m going to talk on how to build packages locally, without having to setup your environment.</p>
<h2 id="building-the-packages-locally-with-docker">Building the packages locally with Docker</h2>
<p>I&rsquo;ll start from the easier task. <em>Building packages with your machine.</em></p>
<p>Let&rsquo;s say that there is an ebuild available in an overlay, which you are interested in and is not available in Entropy. Consider of course, that not all ebuilds will compile, some are not mantained and not work as expected.</p>
<p>We are going to use the sabayon/builder-amd64 image to build portage packages.</p>
<ol>
<li>Setup</li>
</ol>
<hr>
<p>Here we need docker to be installed in your machine, i&rsquo;m assuming that you are running Sabayon here, but this is merely optional,  docker can be used also on Debian, Gentoo, Fedora or whatever you want.</p>
<blockquote>
<blockquote>
<p>_sudo equo i docker _</p>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<p><em>sudo systemctl start docker</em></p>
</blockquote>
</blockquote>
<p>There we go, this is the unique software that we actually want.<br>
If you want the Docker service to be launched on boot:</p>
<blockquote>
<p>_        sudo systemctl enable docker_</p>
</blockquote>
<hr>
<ol start="2">
<li>Pull(download) the image</li>
</ol>
<hr>
<p>Now let&rsquo;s download the <strong>sabayon/builder-amd64</strong> image:</p>
<blockquote>
<blockquote>
<p><em>sudo docker pull sabayon/builder-amd64</em></p>
</blockquote>
</blockquote>
<p><em>Note: If you don&rsquo;t want to use sudo on each docker command, put your user into the &ldquo;docker&rdquo; group.</em></p>
<h2 id="_3-spawn-a-compilation_"><em>3) Spawn a compilation</em></h2>
<p>Let&rsquo;s say that now we want to compile app-text/tree. <br>
Now we can launch a new container from the sabayon/builder-amd64 image, giving it as argument the package we want to compile:</p>
<blockquote>
<blockquote>
<p><em>sudo docker run -ti &ndash;rm -v &ldquo;$PWD&rdquo;/artifacts:/usr/portage/packages sabayon/builder-amd64 app-text/tree</em></p>
</blockquote>
</blockquote>
<p>Now the builder machine will try to compile your package downloading first all the (available) dependencies it can find from Entropy and setting your container to face the compilation phase properly.</p>
<p>When docker run will exit, you will find then your package under the &ldquo;artifacts&rdquo; directory under the directory where you launched the command. </p>
<p>If your package was in another overlay, the builder machine supports to fetch automatically overlays, you just have to tell what are the overlays you need:</p>
<blockquote>
<blockquote>
<p><em>sudo docker run -ti &ndash;rm -v &ldquo;$PWD&rdquo;/artifacts:/usr/portage/packages sabayon/builder-amd64 app-text/tree &ndash;overlay <strong>NAME</strong></em></p>
</blockquote>
</blockquote>
<p>where:</p>
<ul>
<li>**--rm **tells docker to destroy the container as soon as it exits</li>
<li><strong>-v</strong> determines the mount of a directory between the host and the container. On the left side of the doublepoints &ldquo;:&rdquo; there is the host directory, on the right the container one. The host folder will be mounted on the directory specified on the container so we can share data.</li>
<li><strong>--overlay NAME:</strong> you can specify an overlay where your package is available, for example, we might want to compile the new gnome available in the gnome overlay. This command can be chained to list more than one overlay to fetch _(&ndash;_<em>overlay1 &ndash;overlay2, ecc.. )</em></li>
</ul>
<p>You might also want to save your changes to the image, meaning to mantain your building machine state. This can be accomplished with docker, <a href="https://docs.docker.com/v1.8/articles/basics/#committing-saving-a-container-state">using &ldquo;docker commit&rdquo;.</a> How to use this image is up to your imagination.</p>
<ol start="4">
<li>MOAR Options (Here be dragons)</li>
</ol>
<hr>
<p>Docker allows to share directories with the host system, gracefully mounting them on   start up. We can specify to Docker in it&rsquo;s definition file that we want some volumes to not be shipped with the image itself, but instead being mounted on the host system everytime we run it. As for example, we can tell to Docker to use the directory foo as our /usr/portage directory, allowing us to cache Portage files, including distfiles and packages.</p>
<blockquote>
<p><em>docker run -t -i sabayon/base-amd64 -v &ldquo;$PWD&rdquo;/foo:/usr/portage <!-- raw HTML omitted --></em></p>
</blockquote>
<ul>
<li><strong>-t</strong> tells to docker to allocate a pseudo-tty for us, allowing to run an interactive shell </li>
<li><strong>-i</strong> keep the STDIN attached also if not active</li>
<li><strong>-v</strong> sets the volume, so &ldquo;$PWD&rdquo;/foo will contain the /usr/portage content of the container and vice-versa (its mounted)</li>
<li>**<!-- raw HTML omitted --> **the last argument is the command that we want to launch inside the container, we could of course launch other commands here, this is just an example</li>
</ul>
<p>If you don&rsquo;t have already pulled the docker image (docker pull sabayon/base-amd64) this will be done automatically, leaving you to use just one command.</p>
<h2 id="example---advanced">Example - Advanced</h2>
<p><strong>The -v flag can furthermore exploited and chained to obtain more fine-grained tweaks</strong></p>
<p>_Going to cover a minor example regarding sabayon/builder-amd64 image here. _<br>
<em>You can of course customize it further, and replace all the configuration that&rsquo;s already been setup on the Docker Image.</em><br>
<em>Let&rsquo;s create 5 files in a directory, corresponding to the customization you might need from your building process.</em></p>
<ul>
<li><em>custom.unmask: will contain your unmasks</em></li>
<li><em>custom.mask: will contain your masks</em></li>
<li><em>custom.use: will contain your use flags</em></li>
<li><em>custom.env: will contain your env specifications</em></li>
<li><em>custom.keywords: will contain your keywords</em></li>
</ul>
<p>Exporting those files to your container is a matter of adding an argument to your docker run command.</p>
<h4 id="heading"></h4>
<h4 id="example-exporting-your-customunmask">Example. Exporting your custom.unmask:</h4>
<blockquote>
<p><em>-v /my/path/custom.unmask:/opt/sabayon-build/conf/intel/portage/package.unmask/custom.unmask</em></p>
</blockquote>
<h4 id="example-exporting-your-custommask">Example. Exporting your custom.mask:</h4>
<blockquote>
<p>_-v /my/path/custom.mask:/opt/sabayon-build/conf/intel/portage/package.mask/custom.mask _</p>
</blockquote>
<h4 id="example-exporting-your-customuse">Example. Exporting your custom.use:</h4>
<blockquote>
<p>_-v /my/path/custom.use:/opt/sabayon-build/conf/intel/portage/package.use/custom.use _</p>
</blockquote>
<h4 id="example-exporting-your-customenv">Example. Exporting your custom.env:</h4>
<blockquote>
<p><em>-v /my/path/custom.env:/opt/sabayon-build/conf/intel/portage/package.env/custom.env</em></p>
</blockquote>
<h4 id="example-exporting-your-customkeywords">Example. Exporting your custom.keywords:</h4>
<blockquote>
<p><em>-v /my/path/custom.keywords:/opt/sabayon-build/conf/intel/portage/package.keywords/custom.keywords</em></p>
</blockquote>
<p>In this way you tell to docker to mount your custom.* file inside /opt/sabayon-build/conf/intel/portage/package.*/custom.* inside the container. </p>
<p>Keep in mind that the container have the portage directory located at /opt/sabayon-build/conf/intel/portage/ ; the /etc/portage folder is then symlinked to it.</p>
<p>Attention! Remember also to use absolute paths or docker will fail to mount your files in the container.</p>
<p>The image then will call a script that provides to do most of the things automatically for you, but there are some environment variables that you can use to tweak it&rsquo;s default behavior:</p>
<ul>
<li><strong>BUILDER_PROFILE</strong>: Sets the profile for compilation, you can select it using the number or the name</li>
<li><strong>BUILDER_JOBS</strong>: How much jobs emerge will have assigned (-j option)</li>
<li><strong>USE_EQUO</strong>: 1/0 Enable/Disable equo for installing the package dependencies (if you plan to use a pure gentoo repository, set it to 0, but the compilation process would be much longer)</li>
<li><strong>PRESERVED_REBUILD</strong>: 1/0 to Enable/Disable preserved rebuild compilation</li>
<li><strong>EMERGE_DEFAULTS_ARGS</strong>: a list of commands that you might want to specify </li>
<li><strong>FEATURES</strong>: you can override default FEATURES (like in Portage&rsquo;s <strong>make.conf</strong>)</li>
</ul>
<p>To pass an environment variable to the docker container we set them like this:</p>
<blockquote>
<p>-e &ldquo;<strong>OPTION1</strong>=<em>VALUE1</em>&rdquo; -e &ldquo;<strong>OPTION2</strong>=<em>VALUE2</em>&rdquo;</p>
</blockquote>
<p>That&rsquo;s it for now!</p>
]]></content>
        </item>
        
        <item>
            <title>Sabayon: testers, testers, testers</title>
            <link>https://mudler.pm/2015/10/sabayon-testers-testers-testers.html</link>
            <pubDate>Wed, 07 Oct 2015 14:28:00 -0700</pubDate>
            
            <guid>https://mudler.pm/2015/10/sabayon-testers-testers-testers.html</guid>
            <description>I will give it a go installing new motherboard tod&amp;hellip; Unknown - Oct 4, 2015
I will give it a go installing new motherboard today will start testing after thate
Thanks for your efforts , please remember to file a bug for whatever problem you will face!
I have jut installed a new motherboard,it is a msi z97a the only problem so far is I cant install sabayon in uefi mode I am runing 15.</description>
            <content type="html"><![CDATA[<h4 id="i-will-give-it-a-go-installing-new-motherboard-tod">I will give it a go installing new motherboard tod&hellip;</h4>
<p><a href="https://www.blogger.com/profile/02509968321297586933" title="noreply@blogger.com">Unknown</a> - <!-- raw HTML omitted -->Oct 4, 2015<!-- raw HTML omitted --></p>
<p>I will give it a go installing new motherboard today will start testing after thate</p>
<!-- raw HTML omitted -->
<p>Thanks for your efforts , please remember to file a bug for whatever problem you will face!</p>
<!-- raw HTML omitted -->
<p>I have jut installed a new motherboard,it is a msi z97a the only problem so far is I cant install sabayon in uefi mode I am runing 15.10 on the other hard dive fedora 22 xfce installed in uefi mode no problem</p>
<!-- raw HTML omitted -->
<p>excuse my punctuation today please</p>
<!-- raw HTML omitted -->
<p>not in UEFI mode? there are some error messages?</p>
<!-- raw HTML omitted -->
<p>No no error messages just wont install in uefi mode.I will try putting 15.10 on a dvd and see if it installs ok</p>
<!-- raw HTML omitted -->
<p>15.10 will not install on this new wiz bang machine in uefi mode.motherboard msi z97a,cpu intel core i7-4770k CPU @ 3.50GHz,s,graphics card [AMD/ATI] Curacao XT [Radeon R7 370 / R9 270X/370 OEM],memory 31.4 GIB of ram.the strange part is fedora of any flavour installs in uefi mode so I am at a loss as to know why sabayon wont install in uefi mode,I have tried 2 different usb sticks still no luck</p>
<!-- raw HTML omitted -->
<p>What program are you using to burn the sticks? as far as i can test UEFI mode works on all my PC</p>
<!-- raw HTML omitted -->
<p>I use fedora live usb creator,I can get to the installion screen but it just hangs there</p>
<!-- raw HTML omitted -->
<p>I just tried unetbootin and a cd/dvd all doing the same thing hanging at the install screen.fedora 22 xfce and fedora 22 plasma both install no problem<br>
regards Ron</p>
<!-- raw HTML omitted -->
<p>Try to use dd to burn the image instead</p>
<!-- raw HTML omitted -->
<p>just did a new download of 15.10 from different mirror it boots in bios mode not uefi,it still hangs at install screen in uefi mode so will install in bios mode</p>
<!-- raw HTML omitted -->
<p>could my new graphics card be blocking uefi install</p>
<!-- raw HTML omitted -->
<p>just been in touch with msi they are lelling me its a prblem with uefi on sabayon is why it wont install in uefi mode so I wont be installing sabayon<br>
sorry regards Ron</p>
<!-- raw HTML omitted -->
<p>Hello Ettote just letting you know I am going back to a asrock extreme 4 motherboard so I can run sabayon I miss sabayon</p>
<!-- raw HTML omitted -->
<p>up and running with new asrock motherboard running sabayon 15.10 kde only problem sddm not working properly will file bug report</p>
<!-- raw HTML omitted -->
<p>replace sddm with lightdm, it is known to have bugs, new Sabayon KDE version will ship lightdm :)</p>
<!-- raw HTML omitted -->
]]></content>
        </item>
        
        <item>
            <title>Sabayon: testers, testers, testers</title>
            <link>https://mudler.pm/2015/10/sabayon-testers-testers-testers.html</link>
            <pubDate>Wed, 07 Oct 2015 14:28:00 -0700</pubDate>
            
            <guid>https://mudler.pm/2015/10/sabayon-testers-testers-testers.html</guid>
            <description>Sabayon, official call for testers (and well, developers are welcome too)
Testers, come to us Probably most of you already have seen Ballmer in show-mode
Also if controversial, it was a quite funny commercial / raindance move, balls included. But i can say that he is wrong, testers are as important as developers.
Consider helping Sabayon in a different way: testing it.
I am here, begging YOU for some help. The more the merrier!</description>
            <content type="html"><![CDATA[<p>Sabayon, official call for testers (and well, developers are welcome too)</p>
<p><a href="http://4.bp.blogspot.com/-aKxCHaqb3X4/VhTf8i951mI/AAAAAAAAMB0/1HrGxtWUY2Q/s1600/keep-calm-and-test-develop.png"><img src="http://4.bp.blogspot.com/-aKxCHaqb3X4/VhTf8i951mI/AAAAAAAAMB0/1HrGxtWUY2Q/s320/keep-calm-and-test-develop.png" alt=""></a></p>
<h2 id="testers-come-to-us">Testers, come to us</h2>
<p>Probably most of you already have seen Ballmer in <em>show-mode</em></p>
<p>Also if controversial, it was a quite funny commercial / raindance move, balls included. But i can say that he is wrong, <strong>testers are as important as developers.</strong><br>
Consider helping Sabayon in a <em>different</em> way: testing it.</p>
<p>I am here, begging YOU for some help. The more the merrier! What we need are brave folks proud to be bleeding edge, ready to install all the latest updates, capable observers that carries on using their machine normally BUT taking notes on functionality, installation problems, and any other weirdness or lack thereof.<br>
Grab a freshly built image or enable our testing repositories, but more importantly:_ report back_ to our <a href="https://bugs.sabayon.org/">bugzilla</a> or dropping by on our IRC channel (#sabayon on chat.freenode.net).</p>
<p><em>Please, when reporting, try to provide as much information as possible, for example, try to answering to those questions:</em></p>
<ul>
<li><em>What steps will reproduce the problem?</em></li>
<li><em>What is the expected output? What do you see instead?</em></li>
<li><em>What version of the product are you using? On what operating system?</em></li>
</ul>
<p><em>And of course, any additional info that comes up to you.</em></p>
<p>we @<a href="https://plus.google.com/115862676514799968405">Sabayon</a> are putting a lot of efforts to build the most awesome sexy lady among the other Linux Distributions.</p>
<p>No more chatting, let&rsquo;s see what&rsquo;s the news for the upcoming release.</p>
<h2 id="sabayon-server-edition">Sabayon Server Edition</h2>
<p><a href="http://2.bp.blogspot.com/-03zD33U3QR8/VhTVE42n8SI/AAAAAAAAMBA/XLMXUKbYows/s1600/sabayon-server.png"><img src="http://2.bp.blogspot.com/-03zD33U3QR8/VhTVE42n8SI/AAAAAAAAMBA/XLMXUKbYows/s320/sabayon-server.png" alt=""></a></p>
<p>Next release will ship back the Server edition, which is just perfect <strong>also</strong> for those who wants a not-X baked Sabayon version. The installer is still Calamares (just GUI install as for now), but running on a <em>adhoc</em> instance of X, that bloats the ISO size, but after install, all the additional components requested by Calamares are removed.</p>
<p>If you wants to put your hands on it before it&rsquo;s release date (28th of the current month), you can download it in our daily directory in one of our <a href="http://na.mirror.garr.it/mirrors/sabayonlinux/iso/daily/">mirrors</a> , or directly following <a href="http://na.mirror.garr.it/mirrors/sabayonlinux/iso/daily/Sabayon_Linux_DAILY_amd64_Server.iso">that</a> link. (be careful to download the image dated after 8th October)</p>
<p><em>Just a note:</em> if you wonder how you have to set network configuration after install, we ship NetworkManager that provides <em>nmtui</em> (the console frontend)</p>
<h2 id="important-packages-coming-up-and-now-in-testing">Important packages coming up, and now in testing</h2>
<p>On the testing repositories now are available the following &ldquo;critical&rdquo; packages in testing:</p>
<ul>
<li>Latest <strong>4.2.3</strong> kernel</li>
<li>AtiDrivers <em><strong>15.9</strong></em>(expanding supported cards)</li>
<li>Nvidia Drivers <em><strong>355.11</strong></em></li>
<li>systemd <em><strong>226</strong></em></li>
<li>VirtualBox <em><strong>5.0.6</strong></em></li>
<li>Calamares <em><strong>1.1.4.1</strong></em></li>
<li>Libreoffice <strong>5.0.1.2</strong></li>
</ul>
<p>If you encounter in any issue, please file a <a href="https://bugs.sabayon.org/">bug here.</a> </p>
<h3 id="help-us-also-enabling-the-testing-repository">Help us also enabling the testing repository</h3>
<p>If you are wondering how to put your hands on the shiny new testing packages here it is a small how-to:</p>
<blockquote>
<p>equo repo disable sabayon-weekly</p>
</blockquote>
<blockquote>
<p>equo repo enable sabayonlinux.org </p>
</blockquote>
<blockquote>
<p>equo repo enable sabayon-limbo</p>
</blockquote>
<p>taken from the <a href="https://wiki.sabayon.org/index.php?title=En:Repositories#Adding_Repositories">wiki.</a></p>
<h2 id="_and-as-we-work-meanwhile-in-the-world_"><em>And as we work, meanwhile in the world&hellip;</em></h2>
<p><a href="https://lkml.org/lkml/2015/9/3/428">Meanwhile our &ldquo;Sith Lord&rdquo; Linus Torvalds is screaming among Kernel devs</a>, a fancy meme appeared on my G+, i couldn&rsquo;t resist to share it to you</p>
<p><a href="http://4.bp.blogspot.com/-3Dwn6Qnb0-Y/VhTSmDHHUOI/AAAAAAAAMAw/ZVa0PI-MHxw/s1600/CPnK1CQWoAEru50.jpg-large.jpeg"><img src="http://4.bp.blogspot.com/-3Dwn6Qnb0-Y/VhTSmDHHUOI/AAAAAAAAMAw/ZVa0PI-MHxw/s320/CPnK1CQWoAEru50.jpg-large.jpeg" alt=""></a></p>
<p><em>by the way, I agree with his philosophy on that, people if wants REALLY learn things, most of the times needs to be beaten up a little bit, like your mom used to, and to be frank, it should be considered a honor to be yelled by Linus</em><br>
Cheers</p>
]]></content>
        </item>
        
        <item>
            <title>Sabayon 15.10 press release</title>
            <link>https://mudler.pm/2015/09/sabayon-1510-press-release.html</link>
            <pubDate>Mon, 28 Sep 2015 11:07:00 -0700</pubDate>
            
            <guid>https://mudler.pm/2015/09/sabayon-1510-press-release.html</guid>
            <description>Image: an ancient hen-foot print found on Mars (Mars Photo credit to NASA)
Sabayon 15.10 is a modern and easy to use Linux distribution based on Gentoo, following an extreme, yet reliable, rolling release model.
This is a monthly release generated, tested and published to mirrors by our build servers containing the latest and greatest collection of software available in the Entropy repositories.
The ChangeLog files related to this release are available on our mirrors.</description>
            <content type="html"><![CDATA[<p><a href="http://3.bp.blogspot.com/-AbWz1Wk7__U/Vgl_vSomVNI/AAAAAAAAL4I/rdcLxP0vxOE/s1600/mars-sabayon.jpg"><img src="http://3.bp.blogspot.com/-AbWz1Wk7__U/Vgl_vSomVNI/AAAAAAAAL4I/rdcLxP0vxOE/s400/mars-sabayon.jpg" alt=""></a></p>
<p><em>Image: an ancient hen-foot print found on Mars (Mars Photo credit to NASA)</em></p>
<p>Sabayon <strong>15.10</strong> is a modern and easy to use Linux distribution based on Gentoo, following an extreme, yet reliable, rolling release model.</p>
<p>This is a monthly release generated, tested and published to mirrors by our build servers containing the latest and greatest collection of software available in the Entropy repositories.</p>
<p>The ChangeLog files related to this release are available <a href="http://dl.sabayon.org/iso/ChangeLogs/">on our mirrors.</a></p>
<p>The list of packages included in each Sabayon flavor is available inside*.pkglist&quot; files. Our team is always busy packaging the latest and greatest stuff. If you want to have a look at what&rsquo;s inside our repositories, just go to our <a href="https://packages.sabayon.org/">packages website.</a></p>
<p>Please read on to know where to find the images and their torrent files on our mirrors.</p>
<h2 id="long-life-to-new-and-shiny-calamares">Long life to new and shiny Calamares!</h2>
<p>As already pre-announced we switched the default installer. Anaconda served us well, but we decided to embrace the community-baked Calamares, the Distribution-independent installer! If you don&rsquo;t know it, checkout their <a href="http://calamares.io/">website</a>. Also if it is young and some features are missing, it is lighter and bug-less wrt our old installer.</p>
<h2 id="plasma-5">Plasma 5</h2>
<p><a href="http://3.bp.blogspot.com/-jRJmCVvyYJ4/Vebu_7a86OI/AAAAAAAALnE/d8DCNFt13_w/s1600/plasma_sab.png"><img src="http://3.bp.blogspot.com/-jRJmCVvyYJ4/Vebu_7a86OI/AAAAAAAALnE/d8DCNFt13_w/s320/plasma_sab.png" alt=""></a></p>
<p>KDE releases now ships by default the new Plasma 5. Yes, you heard it well! </p>
<p><a href="http://4.bp.blogspot.com/-uUgqmkSUMKI/Vgl7IC832fI/AAAAAAAAL4A/MRewReUozJM/s1600/saba15.10.png"><img src="http://4.bp.blogspot.com/-uUgqmkSUMKI/Vgl7IC832fI/AAAAAAAAL4A/MRewReUozJM/s400/saba15.10.png" alt=""></a></p>
<p>Sabayon 15.10 KDE spin</p>
<h2 id="steam-big-picture-and-mce-mode-gone--for-now-">Steam Big Picture and MCE mode gone ( for now )</h2>
<p>Since we had to put much effort in making a working release with Calamares and along with other new packages, we had to ditch Steam Big Picture mode and Media Center installation options. Yeah, we are sorry about that: but this is just a sad story that we hope to fix soon.</p>
<h2 id="docker">Docker</h2>
<p>We now offer official docker releases as well. You can find them in our <a href="https://hub.docker.com/r/sabayon">official docker profile.</a></p>
<p>There also is a <a href="https://hub.docker.com/r/sabayon/builder-amd64/">Docker image</a> available to build Sabayon packages as well, without the need to have all the tools required in your existing machine.</p>
<h2 id="future-plans">Future plans</h2>
<p>You know that our habits are a bit evil-ish. We have a lot of stuff in the pipeline, just to spoiler a bit: you soon again should see a Sabayon Server Edition and hopefully also an ARM release.</p>
<h2 id="available-releases">Available releases</h2>
<p>As for now we offer 64bit images only.</p>
<p>But you are free to choose between the wonderful minimalism of GNOME, the eyecandy of KDE or the old fart called Xfce. If you are the kind of person who just needs Fluxbox/Openbox/whatever else, just get the Minimal image and you won&rsquo;t be hit by the &ldquo;OMG candies&rdquo; bloat that is in the other images.</p>
<h2 id="binary-vs-source-package-manager">Binary vs Source Package Manager</h2>
<p>It&rsquo;s up to you whether turn a newly Sabayon installation into a geeky Gentoo ~arch system or just camp on the lazy side and enjoy the power of our binary, dumbed down Applications Manager (a.k.a. Rigo). With Sabayon you are really in control of your system the way you really want. <a href="https://wiki.sabayon.org/index.php?title=HOWTO:_Safely_mix_Entropy_and_Portage">Read the wiki page if you plan mixing the two package managers.</a></p>
<h2 id="native-nvidia-and-amd-gpu-drivers-support">Native NVIDIA and AMD GPU drivers support</h2>
<p>All our releases natively support the latest and greatest GPU hardware from NVIDIA and AMD through their proprietary drivers. Whether you want to enjoy your Linux rig for gaming or video playback, you can. For AMD hardware though, we default to the Open Source implementation for the supported cards. Make sure to pass &ldquo;nomodeset&rdquo; to the boot command line to force the proprietary drivers to be used instead: <a href="https://wiki.sabayon.org/index.php?title=HOWTO:_Get_AMD/ATI_or_Nvidia_Video_Cards_working_in_Sabayon#AMD_-_Open_Source_to_FGLRX">head over the wiki for more details.</a></p>
<h2 id="ltsi-linux-kernels-310-312-offered">LTSI Linux Kernels, 3.10, 3.12 offered</h2>
<p>We are now tracking the 3.10, 3.12, 3.14 Long Term Stable Linux kernels, offering (almost) same-day updates to them. If you are using Sabayon in a server environment, you surely welcome this. However, if you&rsquo;re using Sabayon on your laptop, desktop workstation, switching between kernels or just moving to a new version has become a no-brainer operation through Rigo: just go to the preferences menu, select the kernel menu (LTS and regular kernels are listed in separate menus), pick a kernel and click &ldquo;Install&rdquo;. Rigo will take care of updating external modules in a reliable and safe way on your behalf.</p>
<h2 id="links"><strong>Links</strong></h2>
<ul>
<li><a href="http://www.sabayon.org/download">Sabayon Mirrors Page</a></li>
<li><a href="https://hub.docker.com/r/sabayon">Sabayon on Docker Hub</a></li>
<li><a href="http://dl.sabayon.org/iso/monthly/main.html">Sabayon Monthly Images Download Page</a></li>
<li><a href="http://torrents.sabayon.org/">Sabayon BitTorrent Tracker</a></li>
<li><a href="https://www.facebook.com/groups/36125411841">Join us on Facebook</a></li>
<li><a href="https://plus.google.com/+sabayon">Join us on Google+</a></li>
<li><a href="http://www.sabayon.org/donate">Donate to Sabayon!</a></li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>Sabayon is moving to KDE Plasma 5.4</title>
            <link>https://mudler.pm/2015/09/sabayon-is-moving-to-kde-plasma-54.html</link>
            <pubDate>Sat, 12 Sep 2015 10:36:00 -0700</pubDate>
            
            <guid>https://mudler.pm/2015/09/sabayon-is-moving-to-kde-plasma-54.html</guid>
            <description>Sabayon is officially moving to Plasma 5, please read if you are a KDE Sabayon user.
KDM is going to be removed! migrate to SDDM now!
Yes, finally we are moving on.
While KDE 4 was a pretty stable DM, Plasma 5 brings awesomeness and, if you didn&amp;rsquo;t noticed, it&amp;rsquo;s the future.
From KDM to SDDM We tried to do our best to make the transition as smooth as possible and automatic, but if your system just got broken here is why: the KDE team announced SDDM as the default KDE display manager.</description>
            <content type="html"><![CDATA[<p><a href="http://3.bp.blogspot.com/-jRJmCVvyYJ4/Vebu_7a86OI/AAAAAAAALnE/d8DCNFt13_w/s1600/plasma_sab.png"><img src="http://3.bp.blogspot.com/-jRJmCVvyYJ4/Vebu_7a86OI/AAAAAAAALnE/d8DCNFt13_w/s320/plasma_sab.png" alt=""></a></p>
<p>Sabayon is officially moving to Plasma 5, <strong>please read if you are a KDE Sabayon user.</strong></p>
<p><strong>KDM is going to be removed! migrate to SDDM now!</strong></p>
<p>Yes, finally we are moving on.</p>
<p>While KDE 4 was a pretty stable DM, Plasma 5 brings awesomeness and, if you didn&rsquo;t noticed, it&rsquo;s the future.</p>
<h2 id="from-kdm-to-sddm">From KDM to SDDM</h2>
<p>We tried to do our best to make the transition as smooth as possible and automatic, but if your system just got broken here is why: the KDE team announced SDDM as the default KDE display manager.</p>
<p>However to prevent annoyance we advice people that currently use KDM to replace that with SDDM (Simple Desktop Display Manager).</p>
<p><strong>KDM is going to be removed!</strong></p>
<p>Changing from KDM to SDDM is very straight forward.</p>
<ol>
<li>Open a root shell</li>
<li>equo install x11-misc/sddm x11-themes/sabayon-artwork-sddm-default</li>
<li>systemctl disable kdm; systemctl enable sddm﻿</li>
</ol>
<p>Oh yes, and the future KDE spins (Sabayon 15.10) will be automatically shipped with Plasma5!</p>
<blockquote>
</blockquote>
]]></content>
        </item>
        
        <item>
            <title>Tech Preview: Sabayon Plasma 5 LiveCD</title>
            <link>https://mudler.pm/2015/09/tech-preview-sabayon-plasma-5-livecd.html</link>
            <pubDate>Wed, 02 Sep 2015 05:46:00 -0700</pubDate>
            
            <guid>https://mudler.pm/2015/09/tech-preview-sabayon-plasma-5-livecd.html</guid>
            <description>Thanks, Unfortunately running in VirtualBox 4.3.28&amp;hellip; Unknown - Sep 3, 2015
Thanks, Unfortunately running in VirtualBox 4.3.28, NetworkManager is crashing and can&amp;rsquo;t access desktop. :(
Thanks for your feedback, i run the image on VirtualBox 4.3.28 as well without problems, can you try to change the Network Adaptor type under the image settings, Network-&amp;gt; Advanced -&amp;gt; Card type
Thanks, your tip solve the startup. Dolphin link from main menu is broken.</description>
            <content type="html"><![CDATA[<h4 id="thanks-unfortunately-running-in-virtualbox-4328">Thanks, Unfortunately running in VirtualBox 4.3.28&hellip;</h4>
<p><a href="https://www.blogger.com/profile/13048373511639941641" title="noreply@blogger.com">Unknown</a> - <!-- raw HTML omitted -->Sep 3, 2015<!-- raw HTML omitted --></p>
<p>Thanks, Unfortunately running in VirtualBox 4.3.28, NetworkManager is crashing and can&rsquo;t access desktop. :(</p>
<!-- raw HTML omitted -->
<p>Thanks for your feedback, i run the image on VirtualBox 4.3.28 as well without problems, can you try to change the Network Adaptor type under the image settings, Network-&gt; Advanced -&gt; Card type</p>
<!-- raw HTML omitted -->
<p>Thanks, your tip solve the startup. Dolphin link from main menu is broken. Can sugest to include kio-extras to enable to access some other resources in LAN (samba shares foe example).</p>
<!-- raw HTML omitted -->
]]></content>
        </item>
        
        <item>
            <title>Tech Preview: Sabayon Plasma 5 LiveCD</title>
            <link>https://mudler.pm/2015/09/tech-preview-sabayon-plasma-5-livecd.html</link>
            <pubDate>Wed, 02 Sep 2015 05:46:00 -0700</pubDate>
            
            <guid>https://mudler.pm/2015/09/tech-preview-sabayon-plasma-5-livecd.html</guid>
            <description>!! WARNING - HERE BE DRAGONS !!
Sabayon Plasma 5 LiveCD Tech preview is available for download
This release is just for testing only, it is just a preview of Plasma5 on Sabayon for those of you that wants just to snoop on what&amp;rsquo;s going on without touching your system.
If you want to install Plasma 5 with the community repository, follow those instructions.
Known issues: Dolphin Shortcut on favorites is not working Desktop isn&amp;rsquo;t in folderview so you can&amp;rsquo;t quickly access to the install icon, but the installer is present (search for &amp;ldquo;Calamares&amp;rdquo; or &amp;ldquo;Installer&amp;rdquo; in the Plasma menu) No Sabayon artwork is present, plasma is vanilla Please, if you encounter other issues let us know, Thank you!</description>
            <content type="html"><![CDATA[<p><strong>!!</strong> <strong>WARNING</strong> <strong>- HERE BE DRAGONS</strong> <strong>!!</strong></p>
<p><a href="http://1.bp.blogspot.com/-jRJmCVvyYJ4/Vebu_7a86OI/AAAAAAAALnA/tCSegvEXQDc/s1600/plasma_sab.png"><img src="http://1.bp.blogspot.com/-jRJmCVvyYJ4/Vebu_7a86OI/AAAAAAAALnA/tCSegvEXQDc/s320/plasma_sab.png" alt=""></a></p>
<p><strong><strong>Sabayon</strong> <strong>Plasma 5</strong> <strong>LiveCD</strong>  <strong>Tech</strong>  <strong>preview</strong> <strong>is available for</strong> <strong><a href="https://mirror.spike-pentesting.org/mirrors/spike/mirror/images/">download</a></strong></strong></p>
<p>This release is just for testing only, it is just a preview of Plasma5 on Sabayon for those of you that </p>
<p>wants just to snoop on what&rsquo;s going on without touching your system.</p>
<p>If you want to install Plasma 5 with the community repository, <a href="http://blog.mudler.pm/p/sabayon-plasma5-community-repository_27.html">follow those instructions.</a></p>
<h2 id="known-issues">Known issues:</h2>
<ul>
<li>Dolphin Shortcut on favorites is not working</li>
<li>Desktop isn&rsquo;t in folderview so you can&rsquo;t quickly access to the install icon, but the installer is present (search for &ldquo;Calamares&rdquo; or &ldquo;Installer&rdquo; in the Plasma menu)</li>
<li>No Sabayon artwork is present, plasma is vanilla</li>
</ul>
<p>Please, if you encounter other issues let us know, Thank you!</p>
<h2 id="download">Download</h2>
<p>You can find the images here: </p>
<p><strong>ISO</strong>: <a href="http://na.mirror.garr.it/mirrors/spike/mirror/images/Sabayon_Linux_LATEST_amd64_PLASMA-dev.iso">http://na.mirror.garr.it/mirrors/spike/mirror/images/Sabayon_Linux_LATEST_amd64_PLASMA-dev.iso</a></p>
<p><strong>MD5</strong>: <a href="http://na.mirror.garr.it/mirrors/spike/mirror/images/Sabayon_Linux_LATEST_amd64_PLASMA-dev.iso.md5">http://na.mirror.garr.it/mirrors/spike/mirror/images/Sabayon_Linux_LATEST_amd64_PLASMA-dev.iso.md5</a></p>
<p><strong>PKGLIST</strong>: <a href="http://na.mirror.garr.it/mirrors/spike/mirror/images/Sabayon_Linux_LATEST_amd64_PLASMA-dev.iso.pkglist">http://na.mirror.garr.it/mirrors/spike/mirror/images/Sabayon_Linux_LATEST_amd64_PLASMA-dev.iso.pkglist</a></p>
<p>NOTE: the Images are not meant to be installed on a physical machine.</p>
]]></content>
        </item>
        
        <item>
            <title>KDE Plasma 5.4, available on Plasma5 Sabayon community repository</title>
            <link>https://mudler.pm/2015/08/kde-plasma-54-available-on-plasma5.html</link>
            <pubDate>Thu, 27 Aug 2015 05:03:00 -0700</pubDate>
            
            <guid>https://mudler.pm/2015/08/kde-plasma-54-available-on-plasma5.html</guid>
            <description>I&amp;rsquo;m glad to announce that Plasma 5.4 is available on Sabayon Plasma5 community repository.
Plasma 5.4
The KDE team announced on 25 August the release of Plasma 5.4:
This release of Plasma brings many nice touches for our users such as much improved high DPI support, KRunner auto-completion and many new beautiful Breeze icons. It also lays the ground for the future with a tech preview of Wayland session available. We&amp;rsquo;re shipping a few new components such as an Audio Volume Plasma Widget, monitor calibration tool and the User Manager tool comes out beta.</description>
            <content type="html"><![CDATA[<p>I&rsquo;m glad to announce that Plasma 5.4 <strong>is available</strong> on Sabayon Plasma5 community repository.</p>
<p><a href="https://www.kde.org/announcements/plasma-5.4/plasma-screen-desktop-2-shadow.png"><img src="https://www.kde.org/announcements/plasma-5.4/plasma-screen-desktop-2-shadow.png" alt=""></a></p>
<p>Plasma 5.4</p>
<p>The KDE team announced on 25 August the release of Plasma 5.4:</p>
<blockquote>
<p>This release of Plasma brings many nice touches for our users such as much improved high DPI support, KRunner auto-completion and many new beautiful Breeze icons. It also lays the ground for the future with a tech preview of Wayland session available. We&rsquo;re shipping a few new components such as an Audio Volume Plasma Widget, monitor calibration tool and the User Manager tool comes out beta. <a href="https://www.kde.org/announcements/plasma-5.4.0.php?site_locale=en">Look at the official press release here</a></p>
</blockquote>
<p>Useless to say, the KDE team is doing a great work, and i&rsquo;m really excited to see what&rsquo;s <em>next()</em></p>
<p><a href="https://www.kde.org/announcements/plasma-5.4/plasma-screen-dashboard-2-shadow-wee.png"><img src="https://www.kde.org/announcements/plasma-5.4/plasma-screen-dashboard-2-shadow-wee.png" alt=""></a></p>
<p>The new dashboard</p>
<p>If you haven&rsquo;t added yet the repository to your Sabayon machine, and you want to install Plasma 5.4, follow <a href="http://blog.mudler.pm/p/sabayon-plasma5-community-repository_27.html">the instructions here</a>.</p>
<p>Keep in mind this is still experimental, If you encounter some issue, don&rsquo;t hesitate to reach us on IRC, G+, or leaving a comment here.</p>
]]></content>
        </item>
        
        <item>
            <title>on Calamares, Docker and Sabayon</title>
            <link>https://mudler.pm/2015/08/on-calamares-docker-and-sabayon.html</link>
            <pubDate>Mon, 10 Aug 2015 04:29:00 -0700</pubDate>
            
            <guid>https://mudler.pm/2015/08/on-calamares-docker-and-sabayon.html</guid>
            <description>Since i became Sabayon developer, i started to work on customizing it and giving some love to the entropy packages.
A lot as been done in those months, and you can see that picking up a &amp;ldquo;-dev&amp;rdquo; image from our mirrors.
After giving to the entropy packages a fullfill of love (systemd was bumped, ati-drivers and nvidia-drivers as well to be in line with upstream, implemented iptables saving rules for systemd, &amp;hellip; ) then we decided that the time of Anaconda installer was finished.</description>
            <content type="html"><![CDATA[<p>Since i became Sabayon developer, i started to work on customizing it and giving some love to the entropy packages.<br>
A lot as been done in those months, and you can see that picking up <a href="http://na.mirror.garr.it/mirrors/sabayonlinux/iso/daily/">a &ldquo;-dev&rdquo; image from our mirrors</a>.<br>
After giving to the entropy packages a fullfill of love (systemd was bumped, ati-drivers and nvidia-drivers as well to be in line with upstream, implemented iptables saving rules for systemd, &hellip; ) then we decided that the time of Anaconda installer was finished.<br>
Anaconda was buggy and most of the users couldn&rsquo;t even install Sabayon also if, for now, we loose a lot of  nice features like <strong>LUKS</strong> support and **LVM **we chose to swap it with the new and shiny <a href="http://calamares.io/">Calamares</a> .</p>
<p><a href="http://calamares.io/"><img src="http://2.bp.blogspot.com/-60fwITk4MNI/VciKWh9bpNI/AAAAAAAALNc/qJrU0t2cHRo/s320/sabayonandcalamares.png" alt=""></a></p>
<p>No, if you are like me you don&rsquo;t have to worry, <strong>LUKS</strong> and <strong>LVM</strong> support in Calamares are on their schedule, you just have to <em>wait</em>.</p>
<p>Unfortunately, this means that we also had to ditch the text installer, wich was included in Anaconda, but if someone will miss it&rsquo;s presence, probably will be replaced by an <em>adhoc</em> handmade bash script.</p>
<p>As for now, Sabayon is available only on amd64 platforms, suited for a Desktop out-of-the-box environment, but we are moving towards to ship server and <strong><em>arm</em></strong> images: after a week of work we finally have Docker images (yay!) available on the <a href="https://hub.docker.com/r/sabayon/">Sabayon public profile</a>, but i&rsquo;ll spend few words on it on a later post - <a href="https://www.docker.com/whatisdocker">if you don&rsquo;t know what Docker is check out this article</a>.<br>
Docker images finally gives to Sabayon the server flavor that was missing there, now it&rsquo;s easier to ship your application on the clouds using Sabayon! <a href="https://github.com/mudler/docker-sabayon-base">You can find the Github repository of the Docker images on my profile</a></p>
]]></content>
        </item>
        
        <item>
            <title>Google at Home installation [#1 Howto]</title>
            <link>https://mudler.pm/2014/10/google-at-home-installation-1-howto.html</link>
            <pubDate>Wed, 15 Oct 2014 03:04:00 -0700</pubDate>
            
            <guid>https://mudler.pm/2014/10/google-at-home-installation-1-howto.html</guid>
            <description>The first post of a series that will guide you to install and configure your domotic environment with Google@Home (following updates)
Introduction I&amp;rsquo;m starting in the blog a new series of posts illustrating the features of G@H as they are being developed, and how you can start to play with it.
This time we are going to see how to install it correctly, how to handle the basic configuration needed to start and we will install the Wikipedia plugin to allow to query wikipedia with our voice (you can lookup stuff on wikipedia just saying &amp;ldquo;wikipedia house&amp;rdquo;).</description>
            <content type="html"><![CDATA[<p>The first post of a series that will guide you to install and configure your domotic environment with Google@Home (following updates)</p>
<h2 id="introduction">Introduction</h2>
<p>I&rsquo;m starting in the blog a new series of posts illustrating the features of <a href="https://github.com/IntelliHome/Google-at-Home">G@H</a> as they are being developed, and how you can start to play with it.</p>
<p>This time we are going to see how to install it correctly, how to handle the basic configuration needed to start and we will install the Wikipedia plugin to allow to query wikipedia with our voice (you can lookup stuff on wikipedia just saying &ldquo;wikipedia house&rdquo;).</p>
<h2 id="plugin-system">Plugin system</h2>
<p>There are other plugins available as well <a href="https://github.com/IntelliHome">here</a> and there is also an <a href="https://github.com/IntelliHome/Google-at-Home-Remote-Controller">Android application</a> but it is out of scope for this article to guide you through them.</p>
<p>Plugins helps to scale the system even further and add new functionalities to your boards placed in different spots in your home (or whatever environment you plan to use it).</p>
<p>In this tutorial we are installing the master node and the client in the same machine. Basically we distinguish here 2 kind of nodes:<br>
masters (that represent the most capable hardware machine)<br>
agents or basics nodes. A basic node can either be an agent or a master.</p>
<p>The master takes the requests of agent&rsquo;s nodes, process them in a unique interface and send a reply back, so you will talk to the same Entity but you can ask things in parallel in different places on the house/infrastructure.</p>
<p>The Agents can be a PC or an embedded device and we plan to give also a display interface.</p>
<h3 id="1-prerequisites">1) Prerequisites</h3>
<ul>
<li>Git - <a href="http://git-scm.com/book/en/Getting-Started-Installing-Git">Installation how-to</a></li>
<li>SQLite - <a href="http://www.tutorialspoint.com/sqlite/sqlite_installation.htm">Installation how-to</a></li>
<li>SoX - <a href="http://linuxg.net/how-to-install-and-use-sox-on-ubuntu-13-10-13-04-12-10-12-04-and-linux-mint-15-14-13/">Installation ubuntu</a></li>
</ul>
<p>You can check if sqlite it&rsquo;s already installed issuing the following command: sqlite3</p>
<p>Same applies to git and sox.</p>
<h3 id="2-preparation">2) Preparation</h3>
<p>Let&rsquo;s fire up our terminal, and prepare a directory that will contain all the configuration files:</p>
<blockquote>
<p>mkdir ~/google_at_home cd ~/google_at_home</p>
</blockquote>
<h3 id="3-installing-appcpanminus">3) Installing App::cpanminus</h3>
<p>Let&rsquo;s be sure that we have installed also <a href="https://metacpan.org/pod/App::cpanminus">App::cpanminus</a></p>
<h4 id="a-debian">a) debian</h4>
<p>in debian we can just do:</p>
<blockquote>
<p>apt-get install cpanminus</p>
</blockquote>
<h4 id="b-general-install">b) general install</h4>
<p>otherwise, if your vendor doesn&rsquo;t ship it and you have cpan (you have it, trust me):</p>
<blockquote>
<p>cpan App::cpanminus</p>
</blockquote>
<h4 id="c-script-install">c) script install</h4>
<p>You can also install cpanminus by issuing that command in your terminal:</p>
<blockquote>
<p>curl -L <a href="http://cpanmin.us/">http://cpanmin.us/</a> | perl - &ndash;sudo App::cpanminus</p>
</blockquote>
<h3 id="4-installing-intellihome-libraries">4) Installing IntelliHome libraries</h3>
<p>Now let&rsquo;s install the libraries in the system (you can install it on the same user, if you intend to run google-at-home from there)</p>
<blockquote>
<p>cpanm <a href="mailto:git@github.com">git@github.com</a>:IntelliHome/Google-at-Home.git</p>
</blockquote>
<h3 id="5-clone-the-repository">5) Clone the repository</h3>
<p>Now we need to clone the repo as well locally to grab the configuration files</p>
<blockquote>
<p>git clone <a href="mailto:git@github.com">git@github.com</a>:IntelliHome/Google-at-Home.git cd Google-at-Home</p>
</blockquote>
<h3 id="6-basic-configuration-optional">6) Basic configuration (optional)</h3>
<p>By default, the database would be installed on /var/tmp/intellihome.db, to change it you just need to edit the config/database.yml. Here you can also set your language (e.g. for english language: &rsquo;en&rsquo;).</p>
<h3 id="7-plugin-installation">7) Plugin installation</h3>
<p>Now let&rsquo;s procede to install the Wikipedia plugin (that allows querying with voice wikipedia) and the Relay plugin (that enables to commands remote relay attached to your RaspberryPis)</p>
<blockquote>
<p>cpanm <a href="mailto:git@github.com">git@github.com</a>:IntelliHome/IntelliHome-Plugin-Wikipedia.git<br>
cpanm <a href="mailto:git@github.com">git@github.com</a>:IntelliHome/IntelliHome-Plugin-Relay.git</p>
</blockquote>
<p>In this article, we are just showing how to install them, since the relay plugin should be configured to work (such as registering the relays on the boards).</p>
<h3 id="8-database-deploy">8) Database deploy</h3>
<p>The database will keep record of the nodes on your network and the plugin installed on your system.</p>
<p>Now let&rsquo;s prepare the SQLite backend:</p>
<blockquote>
<p>perl intellihome-deployer -b SQLite -c prepare</p>
</blockquote>
<p>And deploy it:</p>
<blockquote>
<p>perl intellihome-deployer -b SQLite -c install</p>
</blockquote>
<h3 id="9-enable-your-plugins">9) Enable your plugins</h3>
<p>Now the master node is ready, we just need to register the plugins that we intend to use:</p>
<blockquote>
<p>perl intellihome-master -i Wikipedia<br>
perl intellihome-master -i Relay</p>
</blockquote>
<p>(to remove them use -r)</p>
<h3 id="10-start-the-software">10) Start the software</h3>
<h4 id="a-start-the-server">a) Start the server</h4>
<p>Let&rsquo;s start the master process now:</p>
<blockquote>
<p>perl intellihome-master</p>
</blockquote>
<p>This will spawn also the web interface, avaible now on <a href="http://localhost:8080/">http://localhost:8080</a>. It&rsquo;s under development, so you can just add/delete the nodes/gpio and tagging them for further use.</p>
<h4 id="b-start-the-local-node">b) Start the local node</h4>
<p>Since this tutorial aim is just to work on the local machine, now you can spawn the node process</p>
<blockquote>
<p>perl intellihome-node</p>
</blockquote>
<p>You are now ready to go, try to speak at your microphone and say &ldquo;wikipedia house&rdquo; to have your answer.</p>
<p>If you encounter issues or bugs, please fill a bug request <a href="https://github.com/IntelliHome/Google-at-Home/issues">here</a></p>
<p>A big thank you to Vytas and dgikiller for the help and the contributions to the project.</p>
]]></content>
        </item>
        
        <item>
            <title>GitInsight, predicting your github contribs calendar</title>
            <link>https://mudler.pm/2014/08/gitinsight-predicting-your-github.html</link>
            <pubDate>Sat, 09 Aug 2014 02:59:00 -0700</pubDate>
            
            <guid>https://mudler.pm/2014/08/gitinsight-predicting-your-github.html</guid>
            <description>Predicting github contrib calendar with Perl and PDL
Premise : this is my first time using PDL
Lately i had the chance to put my hands on PDL, i was glad to discover that it&amp;rsquo;s awesome!
I come from a matlab/octave and Mathematica background, at first was a bit difficult to dig thru the PDL equivalents functions and i have to admit that PyMC has some fancy stuff that require a lot of code to implement in PDL and i wanted to borrow in that case.</description>
            <content type="html"><![CDATA[<p>Predicting github contrib calendar with Perl and PDL</p>
<p>Premise : this is my first time using <strong>PDL</strong></p>
<p>Lately i had the chance to put my hands on PDL, i was glad to discover that it&rsquo;s awesome!</p>
<p>I come from a matlab/octave and Mathematica background, at first was a bit difficult to dig thru the PDL equivalents functions and i have to admit that PyMC has some fancy stuff that require a lot of code to implement in PDL and i wanted to borrow in that case.</p>
<p>But after all, i begun to love the syntax, and the perl integration with sci-libs is a great stuff: allows to pack up your result also in a web interface and it&rsquo;s damn fast.</p>
<p>The idea was to predict the future github contribution calendar of a user, since it&rsquo;s representation in the profile page is a CA where each box can be in 1 of 5 available states represented by the quartiles of the normal distribution over the range [0, max(c)] , where max(c) stands for the maximum commits ever done in a day (within the range of maximum a year).</p>
<p>A transition probability matrix is filled up, using Bayes inference for calculating the probabilities of jumping in each of the possible states and then the Markov chain is computed to give you the result of the prediction for the next week.</p>
<p>I was very glad to see how can be neat (thanks so much to PDL devs) to wrap a probability density function used for the inference in few lines of code:</p>
<pre tabindex="0"><code>sub prob {
    my $x = zeroes(100)-&gt;xlinvals( 0, 1 );
    return $x-&gt;index(maximum_ind(pdf_beta( $x, ( 1 + $_[1] ),( 1 + $_[0] - $_[1] ) ) ));
}
</code></pre><p>Instead of calculating the markov chain for the next states powering the matrix of the transition probabilities, by default the commits are split up by day and with a dedicated transition matrix for each one, since in my opinion developers habits are more likely to show up on the same days (but i added an option to calculate the probabilities also by powering and using just one transition matrix if you wish to compare the results).</p>
<p>Perl comes handy in every situation, and that said was pretty fast to set up a web interface for the algorithm using Mojolicious: <a href="http://gitinsight.mudler.pm/">link</a>.</p>
<p>More predictions that you folks are going to make, more you will help me to improve the algorithm to do better predictions. Suggestions, comments, issues, pull requests are welcome.</p>
<p>All the code is available on github: <a href="https://github.com/mudler/GitInsight/">the GitInsight module</a> and <a href="https://github.com/mudler/WebApp-GitInsight">the web interface</a>.</p>
<p>The future improvements would interests more fine grained quartile calculations( this implementation is just a beta, and probably not suitable for devs that have &gt;200/300 commits), and better prediction trying to detect when a user changes behaviour, highlighting that particular period of time. Oh and of course an option for specifying the starting day of the week.</p>
]]></content>
        </item>
        
        <item>
            <title>Introducing Deeme</title>
            <link>https://mudler.pm/2014/06/introducing-deeme.html</link>
            <pubDate>Sat, 21 Jun 2014 02:57:00 -0700</pubDate>
            
            <guid>https://mudler.pm/2014/06/introducing-deeme.html</guid>
            <description>Introducing Deeme a database-agnostic driven event emitter base-class.
Deeme is a database-agnostic driven event emitter base-class. Deeme allows you to define binding subs on different points in multiple applications, and execute them later, in another worker with a switchable backend database. It is handy if you have to attach subs to events that are delayed in time and must be fixed. It is strongly inspired by (and a rework of) Mojo::EventEmitter.</description>
            <content type="html"><![CDATA[<p><em>Introducing Deeme a database-agnostic driven event emitter base-class.</em></p>
<p>Deeme is a database-agnostic driven event emitter base-class. Deeme allows you to define binding subs on different points in multiple applications, and execute them later, in another worker with a switchable backend database. It is handy if you have to attach subs to events that are delayed in time and must be fixed. It is strongly inspired by (and a rework of) Mojo::EventEmitter.</p>
<p>Deeme was developed for handling the events notification on plugins of Google-at-Home, allowing to store the subroutines of the plugins needed to be executed later when receiving informations of the nodes.</p>
<pre tabindex="0"><code>package Cat;
use Mojo::Base &#39;Deeme&#39;;
use Deeme::Backend::Meerkat;

# app1.pl
package main;
# Subscribe to events in an application (thread, fork, whatever)
my $tiger = Cat-&gt;new(backend=&gt; Deeme::Backend::Meerkat-&gt;new(...) ); #or you can just do Deeme-&gt;new
$tiger-&gt;on(roar =&gt; sub {
  my ($tiger, $times) = @_;
  say &#39;RAWR!&#39; for 1 .. $times;
});

 ...

#then, later in another application
# app2.pl
my $tiger = Cat-&gt;new(backend=&gt; Deeme::Backend::Meerkat-&gt;new(...));
$tiger-&gt;emit(roar =&gt; 3); 
</code></pre><p>You can follow the development <a href="https://github.com/mudler/p5-Deeme">here</a>, for now there is <a href="https://github.com/mudler/p5-Deeme-Backend-Meerkat">the Meerkat backend</a>that supports MongoDB, later i&rsquo;ll write a backend for Mango too. There are examples: <a href="https://github.com/mudler/p5-Deeme/blob/master/ex/roar.pl">local memory example</a>, <a href="https://github.com/mudler/p5-Deeme-Backend-Meerkat/blob/master/ex/roar.pl">MongoDB using Meerkat</a>.</p>
]]></content>
        </item>
        
        <item>
            <title>G@H updates! (5)</title>
            <link>https://mudler.pm/2015/08/gh-updates-5.html</link>
            <pubDate>Mon, 16 Jun 2014 02:55:00 -0700</pubDate>
            
            <guid>https://mudler.pm/2015/08/gh-updates-5.html</guid>
            <description>This post is an update of the current status of the Google@Home project. Google@Home tries to bring domotic control in your home using Google Services for Speech Synthesis and Text-to-Speech. Finally a sketchup of the Android App!
That&amp;rsquo;s the current status of the project:
The GSoC student :
RPC server integration, fully pluggable with Mojolicious (better than the one was in the proposal, less dependencies since that framework will be used also for the UI) RPC Parser service, that forward request to the Parser that handles the actions to be made (by voice) Finished SQLite Schema with DBIx::Class, working on the SQLite Backend, Parser and deployer (using DBIx::Class::DeploymentHandler) facing some bugs encountered in his way This is what i have done so far:</description>
            <content type="html"><![CDATA[<p>This post is an update of the current status of the Google@Home project. Google@Home tries to bring domotic control in your home using Google Services for Speech Synthesis and Text-to-Speech. Finally a sketchup of the Android App!</p>
<p>That&rsquo;s the current status of the project:</p>
<p>The GSoC student :</p>
<ul>
<li>RPC server integration, fully pluggable with Mojolicious (better than the one was in the proposal, less dependencies since that framework will be used also for the UI)</li>
<li>RPC Parser service, that forward request to the Parser that handles the actions to be made (by voice)</li>
<li>Finished SQLite Schema with DBIx::Class, working on the SQLite Backend, Parser and deployer (using DBIx::Class::DeploymentHandler) facing some bugs encountered in his way</li>
</ul>
<p>This is what i have done so far:</p>
<ul>
<li>Adjustments on the core, using some features of Mojolicious, requiring less deps now.</li>
<li>Renamed namespaces of the Drivers, now all lays onIntelliHome::Drivers, a base class for all drivers was created and also a draft of the GPIO dual (like shutters, they are controlled by two GPIO ports)</li>
<li>Now the GPIO can have many pins associated with (needed for dual, but can be extended to other possible uses)</li>
<li>Trigger search now are tied by language</li>
<li>Plugins doesn&rsquo;t require to have a remove callback, it&rsquo;s handled by default, they just need to define install()</li>
<li>Searching on GPIO grep all results, this allow to create &ldquo;groups&rdquo; of controls having the same tag (i.e. open kitchen shutters, will open all the kitchen shutters)</li>
<li>Proceeded with the development of the Android application, since the student made the RPC service, now the app calls the RPC server and forward voice requests (acts like node for now)</li>
<li>Obviously, fixed and cleaned up some code along the way</li>
</ul>
<p>Here a screenshot of the Android app (thanks again to Lispeak mobile that made the recognize intent workout less bloody then expected):</p>
<p><img src="http://www.mudler.pm/img/google_at_home_remote.png" alt=""><br>
Well, the graphic is not really hot i know, but designers contributors are really welcome. In the future the android app would not only be a push-to-talk , but also integrates all the domotic controls (replicating/integrating the future webapp).</p>
<p>Next steps will be to develop a database deployer and configurator (for CLI right now), have a look at the issues on github if you wanna see what&rsquo;s going on.</p>
<p>If you wish to install and try you can just still follow <a href="https://github.com/mudler/Google-at-Home#quick-start">the quick start on the repository page</a>.</p>
]]></content>
        </item>
        
        <item>
            <title>G@H updates! (4)</title>
            <link>https://mudler.pm/2014/06/gh-updates-4.html</link>
            <pubDate>Wed, 04 Jun 2014 02:52:00 -0700</pubDate>
            
            <guid>https://mudler.pm/2014/06/gh-updates-4.html</guid>
            <description>This post is an update of the current status of the Google@Home project. Google@Home tries to bring domotic control in your home using Google Services for Speech Synthesis and Text-to-Speech. It is also planned a web interface to control the embedded nodes in the current environment. In this post i&amp;rsquo;ll report also the work by the GSoC student so far
During this period i haven&amp;rsquo;t much time to code, due to my knee surgery, but that&amp;rsquo;s the current status of the project:</description>
            <content type="html"><![CDATA[<p>This post is an update of the current status of the Google@Home project. Google@Home tries to bring domotic control in your home using Google Services for Speech Synthesis and Text-to-Speech. It is also planned a web interface to control the embedded nodes in the current environment. In this post i&rsquo;ll report also the work by the GSoC student so far</p>
<p>During this period i haven&rsquo;t much time to code, due to my knee surgery, but that&rsquo;s the current status of the project:</p>
<p>The GSoC student :</p>
<ul>
<li>ER scheme to port the current Mongo scheme into SQLite, with few modifications all was good so far</li>
<li>Created the modules using DBIxClass to reflect the SQLite schema</li>
<li>Looking ahead from the schedule, he started to code the RPC Server skeleton to avoid issues in the future (nice job, i&rsquo;m very glad about that)</li>
</ul>
<p>This is what i have done:</p>
<ul>
<li>Fixed the Google Synth api module to reflect the new api (well, that took a while..)</li>
<li>Created the <a href="https://github.com/IntelliHome/IntelliHome-Plugin-Relay">Relay plugin</a>, need some tests (this was a draft, that may eventually need some rework)</li>
<li>Added the driver attribute to the GPIOs schema</li>
<li>Started to develop the <a href="https://github.com/IntelliHome/Google-at-Home-Remote-Controller">Android application</a> to issue voice command from the mobile device, needs the RPC integration when the parsing services (that interfaces the RPC to the Parser) will be ready</li>
<li>Fixed and cleaned up some code</li>
</ul>
<p>If you wish to install and try you can just still follow <a href="https://github.com/mudler/Google-at-Home#quick-start">the quick start on the repository page</a>.</p>
]]></content>
        </item>
        
        <item>
            <title>love for pushbullet</title>
            <link>https://mudler.pm/2014/05/love-for-pushbullet.html</link>
            <pubDate>Thu, 22 May 2014 02:49:00 -0700</pubDate>
            
            <guid>https://mudler.pm/2014/05/love-for-pushbullet.html</guid>
            <description>Well, if you don&amp;rsquo;t know what pushbullet is, it&amp;rsquo;s a free-less-complicated version of what pusher.com does, unifying push&amp;rsquo;s across all the devices. In this article i&amp;rsquo;ll show how to implement pushbullet api in perl, in few lines of code.
Pushbullet enables to notify instantly to all your devices a message, a link, a file, an address or even a list of todo&amp;rsquo;s (and probably more) : that can be quite handy if you don&amp;rsquo;t want to setup an entire notification system, want to avoid the hassle of email alerts (that&amp;rsquo;s not a critique, but i hate having my mailbox full of short report status messages ) or if you want that all your clients are notified immediately.</description>
            <content type="html"><![CDATA[<p>Well, if you don&rsquo;t know what pushbullet is, it&rsquo;s a free-less-complicated version of what pusher.com does, unifying push&rsquo;s across all the devices. In this article i&rsquo;ll show how to implement pushbullet api in perl, in few lines of code.</p>
<p>Pushbullet enables to notify instantly to all your devices a message, a link, a file, an address or even a list of todo&rsquo;s (and probably more) : that can be quite handy if you don&rsquo;t want to setup an entire notification system, want to avoid the hassle of email alerts (that&rsquo;s not a critique, but i hate having my mailbox full of short report status messages ) or if you want that all your clients are notified immediately. I personally needed it to notify myself from simple status report by automatic programs.</p>
<p>However that service is free, you just need to install an extension to your browser and an handy app for your Android/iPhone device (or what the hell do you have, but i bet everyone at these days have at least one mobile device) to receive push messages.</p>
<p>Having a quick look at the <a href="https://docs.pushbullet.com/">Pushbullet&rsquo;s api page</a> we can see that all is about a single curl request:</p>
<pre tabindex="0"><code>curl https://api.pushbullet.com/v2/pushes \
      -u &lt;your_api_key_here&gt;: \
      -d device_iden=&#34;&lt;your_device_iden_here&gt;&#34; \
      -d type=&#34;note&#34; \
      -d title=&#34;Note title&#34; \
      -d body=&#34;note body&#34; \
      -X POST
</code></pre><p>where the most important parts are the api key, the type, the title and the body everything else it&rsquo;s just optional (have a look at the api if you wanna know more).</p>
<p>There is an handy module avaible on cpan it&rsquo;s <a href="https://metacpan.org/pod/WWW::PushBullet">WWW::PushBullet</a>, but if you want a independent solution, using only LWP and HTTP::Request::Common, read on.</p>
<p>Curl it&rsquo;s awesome but Perl it&rsquo;s handy, it&rsquo;s our swiss-army for that kinda of stuff, was easy to convert, so i&rsquo;ll keep the example as clean as possible.</p>
<pre tabindex="0"><code>use feature &#39;say&#39;;
use LWP::UserAgent;
use HTTP::Request::Common qw(POST);
my $ua      = LWP::UserAgent-&gt;new;
my $req = POST &#39;https://api.pushbullet.com/v2/pushes&#39;,
            [
            type  =&gt; &#39;note&#39;,
            title =&gt; &#39;Hello from perl&#39;,
            body   =&gt; &#39;with &lt;3&#39;
            ];
        $req-&gt;authorization_basic(&#39;&lt;API KEY&gt;&#39;);
        say $ua-&gt;request($req)-&gt;as_string;
</code></pre><p>And that&rsquo;s it! Perl saves the day once more, with no hassle at all. We are now lovely notified about Perl love across all our devices.</p>
<p>Cheers!</p>
]]></content>
        </item>
        
        <item>
            <title>Multiple issues in libXfont</title>
            <link>https://mudler.pm/2014/05/multiple-issues-in-libxfont.html</link>
            <pubDate>Thu, 15 May 2014 02:49:00 -0700</pubDate>
            
            <guid>https://mudler.pm/2014/05/multiple-issues-in-libxfont.html</guid>
            <description>_
These months are full of bugs to put hands on, there are multiple vulnerabilities (3) found in libXfont that trusts the font server protocol data, and not verifying that the values will not overflow or cause other damage._
From the Alan Coopersmith announce:
Ilja van Sprundel, a security researcher with IOActive, has discovered several issues in the way the libXfont library handles the responses it receives from xfs servers, and has worked with X.</description>
            <content type="html"><![CDATA[<p>_<br>
These months are full of bugs to put hands on, there are multiple vulnerabilities (3) found in libXfont that trusts the font server protocol data, and not verifying that the values will not overflow or cause other damage._</p>
<p>From the Alan Coopersmith announce:</p>
<p>Ilja van Sprundel, a security researcher with IOActive, has discovered several issues in the way the libXfont library handles the responses it receives from xfs servers, and has worked with X.Org&rsquo;s security team to analyze, confirm, and fix these issues.</p>
<p>Most of these issues stem from libXfont trusting the font server to send valid protocol data, and not verifying that the values will not overflow or cause other damage. This code is commonly called from the X server when an X Font Server is active in the font path, so may be running in a setuid-root process depending on the X server in use. Exploits of this path could be used by a local, authenticated user to attempt to raise privileges; or by a remote attacker who can control the font server to attempt to execute code with the privileges of the X server. (CVE-2014-0209 is the exception, as it does not involve communication with a font server)</p>
<p>Read more <a href="http://lists.freedesktop.org/archives/xorg/2014-May/056616.html">here</a></p>
<p>The three bugs actually doesn&rsquo;t have a real exploit in the wild.. so we can feel secure for now&hellip; but will be a matter of time! The remote attack needs the control of the font server, so shouldn&rsquo;t affect a lot of servers, but for the local priviledges escalation it&rsquo;s a cake :)</p>
]]></content>
        </item>
        
        <item>
            <title>CVE-2014-0196: Linux kernel pty layer race</title>
            <link>https://mudler.pm/2014/05/cve-2014-0196-linux-kernel-pty-layer.html</link>
            <pubDate>Thu, 15 May 2014 02:38:00 -0700</pubDate>
            
            <guid>https://mudler.pm/2014/05/cve-2014-0196-linux-kernel-pty-layer.html</guid>
            <description>Two days ago a temible exploit was released for local privilage escalation in the linux kernel. The bug was found on the 29th of April: memory corruption via a race in pty write handling , affected kernels are 2.6.31 -&amp;gt; 3.14rcX
The n_tty_write function in drivers/tty/n_tty.c in the Linux kernel through 3.14.3 does not properly manage tty driver access in the &amp;ldquo;LECHO &amp;amp; !OPOST&amp;rdquo; case, which allows local users to cause a denial of service (memory corruption and system crash) or gain privileges by triggering a race condition involving read and write operations with long strings.</description>
            <content type="html"><![CDATA[<p>Two days ago a temible exploit was released for local privilage escalation in the linux kernel. The bug was found on the <a href="https://bugzilla.novell.com/show_bug.cgi?id=875690">29th of April</a>: memory corruption via a race in pty write handling , affected kernels are 2.6.31 -&gt; 3.14rcX</p>
<p>The n_tty_write function in drivers/tty/n_tty.c in the Linux kernel through 3.14.3 does not properly manage tty driver access in the &ldquo;LECHO &amp; !OPOST&rdquo; case, which allows local users to cause a denial of service (memory corruption and system crash) or gain privileges by triggering a race condition involving read and write operations with long strings.</p>
<p>There is an exploit available for local privilage escalation, written by Matthew Daley and can be found <a href="http://bugzillafiles.novell.org/attachment.cgi?id=589445">here</a> . There was also an attempt before for local DOS thru the same bug, that can be found <a href="http://pastebin.com/yTSFUBgZ">here</a>.</p>
<p>Just waiting for a kernel release :)</p>
<p>For more information :</p>
<ul>
<li><a href="https://bugzilla.redhat.com/show_bug.cgi?id=1094232">RedHat bugzilla</a></li>
<li><a href="http://bugzilla.novell.com/show_bug.cgi?id=875690">Novell bugzilla</a></li>
<li><a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2014-0196">CVE-2014-0196</a></li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>G@H updates! (3)</title>
            <link>https://mudler.pm/2014/03/gh-updates-3.html</link>
            <pubDate>Sun, 11 May 2014 02:36:00 -0700</pubDate>
            
            <guid>https://mudler.pm/2014/03/gh-updates-3.html</guid>
            <description>This post is an update of the current status of the Google@Home project. Google@Home tries to bring domotic control in your home using Google Services for Speech Synthesis and Text-to-Speech. It is also planned a web interface to control the embedded nodes in the current environment.
During this period i focused on fixing bugs, microphone settings and some structural restyling
This is a small summary of the changes ( for a complete list, check commit diffs in github page) :</description>
            <content type="html"><![CDATA[<p><em>This post is an update of the current status of the Google@Home project. Google@Home tries to bring domotic control in your home using Google Services for Speech Synthesis and Text-to-Speech. It is also planned a web interface to control the embedded nodes in the current environment.</em></p>
<p>During this period i focused on fixing bugs, microphone settings and some structural restyling</p>
<p>This is a small summary of the changes ( for a complete list, check commit diffs in github page) :</p>
<ul>
<li>[structural enhancement] Changed design, main process moved from script/* to the Top module(for each type)</li>
<li>[enhancement] master and nodes start in background, give a -f as argument to mantain in foreground and provvisory -s to stop (will change soon to a normal start/stop)</li>
<li>[fix] Google synth api changes <a href="https://github.com/mudler/Google-at-Home/commit/b2bbbdf0688560327344a61aef1aa122e9b4396c">Commit</a>, switched to v2!</li>
<li>[enhancement] switched to pulseaudio+SoX for recording</li>
<li>[enhancement] Now microphone levels are adjusted automatically (you can disable this feature commenting out from nodes.yaml the mic_upper_threshold and mic_lower_threshold)</li>
<li>starting to work with my GSoC student to help him thru the code and defying a r scheme to follow for the SQLite porting.</li>
</ul>
<p>If you wish to install and try you can just still follow <a href="https://github.com/mudler/Google-at-Home#quick-start">the quick start on the repository page</a>.</p>
]]></content>
        </item>
        
        <item>
            <title>G@H updates! (2)</title>
            <link>https://mudler.pm/2014/04/this-post-is-update-of-current-status.html</link>
            <pubDate>Fri, 04 Apr 2014 02:30:00 -0700</pubDate>
            
            <guid>https://mudler.pm/2014/04/this-post-is-update-of-current-status.html</guid>
            <description>This post is an update of the current status of the Google@Home project. Google@Home tries to bring domotic control in your home using Google Services for Speech Synthesis and Text-to-Speech. It is also planned a web interface to control the embedded nodes in the current environment.
During this week i focused on fixing bug and few structural enhancements
This is a small summary of the changes:
[fix] Plugin can be removed safetly master now can load node configurations by their backends (e.</description>
            <content type="html"><![CDATA[<p>This post is an update of the current status of the Google@Home project. Google@Home tries to bring domotic control in your home using Google Services for Speech Synthesis and Text-to-Speech. It is also planned a web interface to control the embedded nodes in the current environment.</p>
<p>During this week i focused on fixing bug and few structural enhancements</p>
<p>This is a small summary of the changes:</p>
<ul>
<li>[fix] Plugin can be removed safetly</li>
<li>master now can load node configurations by their backends (e.g. Mongo)</li>
<li>a dump of an example of mongo db</li>
<li>added the model for the node</li>
<li>fixed <a href="https://github.com/mudler/Google-at-Home/issues/1">#1</a></li>
<li>tuned up SoX</li>
<li>started the prototyping of the listening agent that directly commands GPIO</li>
</ul>
<p>If you wish to install and try you can just still follow <a href="https://github.com/mudler/Google-at-Home#quick-start">the quick start on the repository page</a>.</p>
]]></content>
        </item>
        
        <item>
            <title>Google at Home updates</title>
            <link>https://mudler.pm/2014/03/google-at-home-updates.html</link>
            <pubDate>Sat, 29 Mar 2014 02:15:00 -0700</pubDate>
            
            <guid>https://mudler.pm/2014/03/google-at-home-updates.html</guid>
            <description>_
This post is an update of the current status of the Google@Home project. Google@Home tries to bring domotic control in your home using Google Services for Speech Synthesis and Text-to-Speech. It is also planned a web interface to control the embedded nodes in the current environment.
_
For now it&amp;rsquo;s still required to manually set-up the YAML file to configure which nodes belongs to the network, but if you set up some RaspberryPi on the network and a master node that has installed MongoDB the dispatchment of the commands and of the voice (so you can command everything by issuing commands on one node) works for now.</description>
            <content type="html"><![CDATA[<p>_<br>
This post is an update of the current status of the Google@Home project. Google@Home tries to bring domotic control in your home using Google Services for Speech Synthesis and Text-to-Speech. It is also planned a web interface to control the embedded nodes in the current environment.<br>
_</p>
<p>For now it&rsquo;s still required to manually set-up the YAML file to configure which nodes belongs to the network, but if you set up some RaspberryPi on the network and a master node that has installed MongoDB the dispatchment of the commands and of the voice (so you can command everything by issuing commands on one node) works for now.</p>
<p>During this week there weren&rsquo;t a lot of enhancements, i just focused on core structure, this is because i haven&rsquo;t so much time to work on it (some of my time was dedicated to the Nemesis project, a post will follow)</p>
<p>This is a small summary of the current state of the project:</p>
<ul>
<li>The MongoDB backend is ready for use </li>
<li>Plugins now can be triggered by the MongoDB Parser to answer to an user command </li>
<li>The Plugin system is divided so plugins can be developed as external repos </li>
<li>Started the developement of the Wikipedia plugin, few adjustments are still necessary </li>
<li>The Google TTS service had a limit of 100 chars, this was bypassed splitting the message output and joining the resulting mp3 (all done in a trasparent way) </li>
<li>Created the Hailo plugin (just for fun) :) </li>
</ul>
<p>If you wish to install and try you can just still follow <a href="https://github.com/mudler/Google-at-Home#quick-start">the quick start on the repository page</a>.</p>
<p>Note: If you want to install the master node on the RaspberryPi, for now only mongo is supported so refeer to that <a href="http://c-mobberley.com/wordpress/2013/10/14/raspberry-pi-mongodb-installation-the-working-guide/">tutorial</a>.</p>
]]></content>
        </item>
        
        <item>
            <title>Welcome!</title>
            <link>https://mudler.pm/2015/08/welcome.html</link>
            <pubDate>Sun, 02 Mar 2014 01:40:00 -0800</pubDate>
            
            <guid>https://mudler.pm/2015/08/welcome.html</guid>
            <description>This is my blog, it&amp;rsquo;s powered it was powered with Perl and Mojolicious. Enjoy! You will find here some interesting perl posts and updates for the projects where i am involved.
Unfortunately i had to move to blogger, due to time issues. Enjoy!</description>
            <content type="html"><![CDATA[<p>This is my blog, <del>it&rsquo;s powered</del> it was powered with Perl and Mojolicious. Enjoy! You will find here some interesting perl posts and updates for the projects where i am involved.</p>
<p>Unfortunately i had to move to blogger, due to time issues. Enjoy!</p>
]]></content>
        </item>
        
    </channel>
</rss>
