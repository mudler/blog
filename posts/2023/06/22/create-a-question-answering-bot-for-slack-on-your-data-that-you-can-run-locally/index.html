<!DOCTYPE html>
<html lang="en">
  <head>

    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
    <link rel="manifest" href="/images/site.webmanifest">

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Mudler blog - Place where I write about stuff">
    <title>Create a Question answering bot for Slack on your data, that you can run locally | Mudler blog</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link rel="stylesheet" href="https://mudler.pm/css/theme-override.css">
    
  </head>

  <body>
    <header>
      <nav>
        <ul>
          
          
          <li class="pull-left ">
            <a href="https://mudler.pm/">~/mudler blog</a>
          </li>
          
          
          <li class="pull-left ">
            <a href="/posts/">~/blog</a>
          </li>
          
          
          <li class="pull-left ">
            <a href="/about/">~/about</a>
          </li>
          
          
          <li class="pull-left ">
            <a href="/search/">~/search</a>
          </li>
          

          
          
          <li class="pull-right">
            <a href="/index.xml">~/subscribe</a>
          </li>
          

        </ul>
      </nav>
    </header>


<div class="article-meta">
<h1><span class="title">Create a Question answering bot for Slack on your data, that you can run locally</span></h1>

<h2 class="date">2023/06/22</h2>
<p class="terms">
  
  
  
  
  
</p>
</div>



<div class="content-wrapper">
  <main>
    <p>There has been a lot of buzz around AI, Langchain, and the possibilities they offer nowadays. In this blog post, I will delve into the process of creating a small assistant for yourself or your team on Slack. This assistant will be able to provide answers related to your documentation.</p>
<p><img src="https://github.com/spectrocloud-labs/Slack-QA-bot/assets/2420543/6047e1ff-22d5-4b03-9d73-fcb7fb19a2c1" alt="Kairos-TPM-Slackbot"></p>
<h2 id="the-problem">The problem</h2>
<p>I work at <a href="https://www.spectrocloud.com/">Spectro Cloud</a>, and we have an exciting open source project called <a href="https://kairos.io">Kairos</a> (check it out at <a href="https://kairos.io">https://kairos.io</a> if you want to learn more about it!). Kairos is a Meta-Linux, immutable distribution designed for running Kubernetes at the Edge.</p>
<p>One of the challenges we face, aside from creating good documentation, is making it easily accessible and consumable for our community. Documentation evolves rapidly, and it&rsquo;s easy to lose track.
Documentation is a critical part of any project - it&rsquo;s the first thing people see when they visit your website and when a project generates a large amount of documentation, it becomes difficult not only to navigate through it but also to find exactly what you&rsquo;re looking for.</p>
<p>Nowadays, there are several services that offer question answering to improve documentation and enhance this experience. However, if you&rsquo;re like me and want to understand how things work behind the scenes, and perhaps build your own solution, then keep reading.</p>
<p>In this post, I will show you how to set up your own personal Slack bot that can answer questions based on documentation websites, GitHub issues, and code. By the end of this article, you will be able to deploy this bot using Docker or Kubernetes, either for yourself or for your team at work!</p>
<p>You can also try the bot live in our channel (#kairos) by joining <a href="https://join.slack.com/t/spectrocloudcommunity/shared_invite/zt-1k7wsz840-ugSsPKzZCP5gkasJ0kNpqw">the Kairos Slack channel</a> and opening a thread with <code>@LocalAI Bot (dev)</code> (for example, <code>@LocalAI Bot (dev) does Kairos use TPM?</code>. Keep in mind that we self-host this on a small instance without GPU, so answers can be <em>slow</em>, but typically in range of 1-2 minutes.).</p>
<h2 id="the-plan">The plan</h2>
<p>Here&rsquo;s how it works: our code will create a vector database that contains vector representations of different sections of the documentation, code snippets, and GitHub issues. To accomplish this, we will use Langchain and ChromaDB to create the vector database. Langchain is a powerful library that allows interaction with LLMs (Large Language Models), and ChromaDB is a local database that can store documents in the form of embeddings. Embeddings are vectors that represent strings. Embedding databases enable semantic searching within a dataset.</p>
<p>For LLM inference, we will also utilize LocalAI. LocalAI allows us to run LLMs and serves as a drop-in replacement for OpenAI. Although there are other ways to interact with LLMs locally, in this case, I want a clear separation between the model execution and the application logic. This separation enables me to focus more on the core functionality of my bot. It also makes maintenance and updates easier on the go. We can replace the underlying models behind the scenes without modifying our code. Additionally, we can leverage the existing OpenAI libraries, which is quite handy. We will simulate writing code that works with OpenAI, but we will actually test it locally. This approach also allows us to use the same code with OpenAI directly or Azure, if needed.</p>
<p>A summary of what we will need:</p>
<ul>
<li>Basic knowledge of Python and Docker to create a container image for our Slack bot.</li>
<li>LocalAI for running LLMs locally (no GPU required, just a modern CPU).</li>
<li>An LLM model of your choice (I personally found airoboros to be quite good for Q&amp;A).</li>
<li>No OpenAI API keys or external services are needed. We will host the bot on our own without relying on remote AI APIs.</li>
<li>If deploying on Kubernetes in the cloud, you will need a cluster. If running on bare metal, I&rsquo;ve tested this on Kairos (<a href="https://kairos.io">https://kairos.io</a>).</li>
</ul>
<h2 id="tools-we-will-use">Tools we will use</h2>
<p><strong>LocalAI</strong>: It&rsquo;s a project created by me and it is completely community-driven. I encourage you to help and contribute if you want! LocalAI lets you run LLM from different families and it has an OpenAI compatible API endpoint which allows to be used with exiting clients. You can learn more about LocalAI here <a href="https://github.com/go-skynet/LocalAI">https://github.com/go-skynet/LocalAI</a> and in the official website <a href="https://localai.io">https://localai.io</a>.</p>
<p><strong>Langchain</strong>: is a development framework created by Harrison Chase to build applications powered by language models. See: <a href="https://python.langchain.com/docs/get_started/introduction.html">https://python.langchain.com/docs/get_started/introduction.html</a></p>
<p><strong>Docker</strong>: we will run the slack bot with Docker to simplify configuration. A <code>docker-compose.yml</code> file is provided as an example on how to start the slack bot and LocalAI.</p>
<h2 id="how-the-bot-works">How the bot works</h2>
<p>If you&rsquo;re not interested in the details, you can skip directly to the Setup section below. In this section, I will explain how the bot works.</p>
<p>The bot is a generic Slack bot customized to provide answers using Langchain on datasets. You can view the full code of the bot here: <a href="https://github.com/spectrocloud-labs/Slack-QA-bot">https://github.com/spectrocloud-labs/Slack-QA-bot</a>. The interesting part of the bot lies in the <code>memory_ops.py</code> file (<a href="https://github.com/spectrocloud-labs/Slack-QA-bot/blob/main/app/memory_ops.py%29">https://github.com/spectrocloud-labs/Slack-QA-bot/blob/main/app/memory_ops.py)</a>. Here&rsquo;s what we do in that file:</p>
<ul>
<li>Build a knowledge base for the bot to use for answering questions.</li>
<li>When asked questions, the bot utilizes the knowledge base to enhance its answers.</li>
</ul>
<h3 id="building-a-knowledge-base">Building a knowledge base</h3>
<p>The core of the bot lies in this Python function:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">build_knowledgebase</span>(sitemap):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Load environment variables</span>
</span></span><span style="display:flex;"><span>    repositories <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#34;REPOSITORIES&#34;</span>)<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34;,&#34;</span>)
</span></span><span style="display:flex;"><span>    issue_repos <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">&#34;ISSUE_REPOSITORIES&#34;</span>)<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34;,&#34;</span>)
</span></span><span style="display:flex;"><span>    embeddings <span style="color:#f92672">=</span> HuggingFaceEmbeddings(model_name<span style="color:#f92672">=</span>EMBEDDINGS_MODEL_NAME)
</span></span><span style="display:flex;"><span>    chunk_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">500</span>
</span></span><span style="display:flex;"><span>    chunk_overlap <span style="color:#f92672">=</span> <span style="color:#ae81ff">50</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    git_loaders <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> repo <span style="color:#f92672">in</span> repositories:
</span></span><span style="display:flex;"><span>        git_loader <span style="color:#f92672">=</span> GitLoader(
</span></span><span style="display:flex;"><span>            clone_url<span style="color:#f92672">=</span>os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>repo<span style="color:#e6db74">}</span><span style="color:#e6db74">_CLONE_URL&#34;</span>),
</span></span><span style="display:flex;"><span>            repo_path<span style="color:#f92672">=</span><span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;/tmp/</span><span style="color:#e6db74">{</span>repo<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>,
</span></span><span style="display:flex;"><span>            branch<span style="color:#f92672">=</span>os<span style="color:#f92672">.</span>getenv(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>repo<span style="color:#e6db74">}</span><span style="color:#e6db74">_BRANCH&#34;</span>, <span style="color:#e6db74">&#34;main&#34;</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        git_loaders<span style="color:#f92672">.</span>append(git_loader)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> repo <span style="color:#f92672">in</span> issue_repos:
</span></span><span style="display:flex;"><span>        loader <span style="color:#f92672">=</span> GitHubIssuesLoader(
</span></span><span style="display:flex;"><span>            repo<span style="color:#f92672">=</span>repo,
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        git_loaders<span style="color:#f92672">.</span>append(loader)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    sitemap_loader <span style="color:#f92672">=</span> SitemapLoader(web_path<span style="color:#f92672">=</span>sitemap)
</span></span><span style="display:flex;"><span>    documents <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> git_loader <span style="color:#f92672">in</span> git_loaders:
</span></span><span style="display:flex;"><span>        documents<span style="color:#f92672">.</span>extend(git_loader<span style="color:#f92672">.</span>load())
</span></span><span style="display:flex;"><span>    documents<span style="color:#f92672">.</span>extend(sitemap_loader<span style="color:#f92672">.</span>load())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> doc <span style="color:#f92672">in</span> documents:
</span></span><span style="display:flex;"><span>        doc<span style="color:#f92672">.</span>metadata <span style="color:#f92672">=</span> fix_metadata(doc<span style="color:#f92672">.</span>metadata)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    text_splitter <span style="color:#f92672">=</span> RecursiveCharacterTextSplitter(chunk_size<span style="color:#f92672">=</span>chunk_size, chunk_overlap<span style="color:#f92672">=</span>chunk_overlap)
</span></span><span style="display:flex;"><span>    texts <span style="color:#f92672">=</span> text_splitter<span style="color:#f92672">.</span>split_documents(documents)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Creating embeddings. This may take a few minutes...&#34;</span>)
</span></span><span style="display:flex;"><span>    db <span style="color:#f92672">=</span> Chroma<span style="color:#f92672">.</span>from_documents(texts, embeddings, persist_directory<span style="color:#f92672">=</span>PERSIST_DIRECTORY, client_settings<span style="color:#f92672">=</span>CHROMA_SETTINGS)
</span></span><span style="display:flex;"><span>    db<span style="color:#f92672">.</span>persist()
</span></span><span style="display:flex;"><span>    db <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span></code></pre></div><p>We use the locally run <code>HuggingFaceEmbeddings</code> (<code>embeddings = HuggingFaceEmbeddings(model_name=EMBEDDINGS_MODEL_NAME)</code>) and Langchain to split the document into chunks. We then utilize Chroma to construct a vector database.</p>
<p>The code above utilizes the Github Loaders and GithubIssue loader from Langchain to retrieve information about code and GitHub issues from various GitHub repositories. The repositories can be defined via environment variables. We also use the <code>SitemapLoader</code> to ingest a <code>sitemap.xml</code> file and scrape an entire website. This is particularly useful if you already have documentation or a website.</p>
<h3 id="querying-the-knowledge-base">Querying the knowledge base</h3>
<p>Another crucial part of the code is how we interact with the AI and enhance the search results.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">ask_with_memory</span>(line) <span style="color:#f92672">-&gt;</span> str:
</span></span><span style="display:flex;"><span>    embeddings <span style="color:#f92672">=</span> HuggingFaceEmbeddings(model_name<span style="color:#f92672">=</span>EMBEDDINGS_MODEL_NAME)
</span></span><span style="display:flex;"><span>    db <span style="color:#f92672">=</span> Chroma(persist_directory<span style="color:#f92672">=</span>PERSIST_DIRECTORY, embedding_function<span style="color:#f92672">=</span>embeddings, client_settings<span style="color:#f92672">=</span>CHROMA_SETTINGS)
</span></span><span style="display:flex;"><span>    retriever <span style="color:#f92672">=</span> db<span style="color:#f92672">.</span>as_retriever()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    res <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    llm <span style="color:#f92672">=</span> ChatOpenAI(temperature<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, openai_api_base<span style="color:#f92672">=</span>BASE_PATH, model_name<span style="color:#f92672">=</span>OPENAI_MODEL)
</span></span><span style="display:flex;"><span>    qa <span style="color:#f92672">=</span> RetrievalQA<span style="color:#f92672">.</span>from_chain_type(llm<span style="color:#f92672">=</span>llm, chain_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;stuff&#34;</span>, retriever<span style="color:#f92672">=</span>retriever, return_source_documents<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Get the answer from the chain</span>
</span></span><span style="display:flex;"><span>    res <span style="color:#f92672">=</span> qa(<span style="color:#e6db74">&#34;---------------------</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74"> Given the context above, answer the following question: &#34;</span> <span style="color:#f92672">+</span> line)
</span></span><span style="display:flex;"><span>    answer, docs <span style="color:#f92672">=</span> res[<span style="color:#e6db74">&#39;result&#39;</span>], res[<span style="color:#e6db74">&#39;source_documents&#39;</span>]
</span></span><span style="display:flex;"><span>    res <span style="color:#f92672">=</span> answer <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n\n\n</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;Sources:</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Print the relevant sources used for the answer</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> document <span style="color:#f92672">in</span> docs:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#34;source&#34;</span> <span style="color:#f92672">in</span> document<span style="color:#f92672">.</span>metadata:
</span></span><span style="display:flex;"><span>            res <span style="color:#f92672">+=</span> <span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">---------------------</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span> <span style="color:#f92672">+</span> document<span style="color:#f92672">.</span>metadata[<span style="color:#e6db74">&#34;source&#34;</span>] <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">---------------------</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            res <span style="color:#f92672">+=</span> <span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">---------------------</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74"> No source available (sorry!) </span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">---------------------</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>        res <span style="color:#f92672">+=</span> <span style="color:#e6db74">&#34;```</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">+</span>document<span style="color:#f92672">.</span>page_content<span style="color:#f92672">+</span><span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">```&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> res
</span></span></code></pre></div><p>In this section, we load the previously created embedding database and configure the Langchain <code>RetrievalQA</code> object. Once the knowledge base has been built, we simply point to the embedding database and specify the embedding engine. In this case, we use local embeddings with HuggingFace, but other options could have been used as well (for example, LocalAI also has its own embedding mechanism).</p>
<p>We then configure the <code>llm</code> to use LocalAI. Note that we use ChatOpenAI and set <code>openai_api_base</code> to use <a href="https://github.com/go-skynet/LocalAI">LocalAI</a> instead.</p>
<h2 id="setup">Setup</h2>
<p>Now, let&rsquo;s proceed with setting up our bot! Here&rsquo;s what we need:</p>
<ul>
<li>Set up a Slack server and gain access to add new applications.</li>
<li>Create a GitHub repository (optional) and obtain a Personal Access Token to fetch issues from a repository.</li>
<li>Ensure your website has an accessible <code>sitemap.xml</code> file so that our bot can scrape the website content.</li>
<li>Install the <code>docker</code> and <code>docker-compose</code> applications locally if missing.</li>
<li>Choose a model for use with LocalAI (refer to <a href="https://github.com/go-skynet/LocalAI%29">https://github.com/go-skynet/LocalAI)</a>.</li>
</ul>
<p>That&rsquo;s it! We don&rsquo;t need an OpenAI API key or any external services except GitHub, optionally, which we use to fetch the content we want to index.</p>
<h3 id="clone-the-required-files">Clone the required files</h3>
<p>We will run everything locally using Docker. At the end of this article, I will also provide a deployment file that works with Kubernetes.</p>
<p>To get started, clone the LocalAI repository locally:</p>
<pre tabindex="0"><code>git clone https://github.com/go-skynet/LocalAI
cd LocalAI/examples/slack-qa-bot
</code></pre><p>You will find a <code>docker-compose.yaml</code> file and a <code>.env.example</code> file. We need to edit the <code>.env</code> file and add the Slack tokens to allow the bot to connect.</p>
<h3 id="configuring-slack">Configuring Slack</h3>
<p>To install the bot, we need to create an application in the Slack workspace. Follow these steps:</p>
<ol>
<li>
<p>Go to <a href="https://api.slack.com/apps/">https://api.slack.com/apps/</a> and click on &ldquo;Create new App&rdquo;.
<img src="https://github.com/seratch/ChatGPT-in-Slack/assets/2420543/9e474872-0d24-4601-b453-679f3601de18" alt="Screenshot 1"></p>
</li>
<li>
<p>Select &ldquo;From an app Manifest&rdquo;.
<img src="https://github.com/seratch/ChatGPT-in-Slack/assets/2420543/962de606-a694-47ad-8fc0-cd096668e07f" alt="Screenshot 2"></p>
</li>
<li>
<p>Choose the workspace where you want to add the bot.
<img src="https://github.com/seratch/ChatGPT-in-Slack/assets/2420543/37573a42-87b6-4351-ad19-303e5ad610ed" alt="Screenshot 3"></p>
</li>
<li>
<p>Copy the content of the <a href="https://raw.githubusercontent.com/spectrocloud-labs/Slack-QA-bot/main/manifest-dev.yml">manifest-dev.yml</a> file from the repository and paste it into the app manifest.
<img src="https://github.com/seratch/ChatGPT-in-Slack/assets/2420543/f134d1c6-eded-4114-81a0-d5fa2f870baf" alt="Screenshot 4"></p>
</li>
<li>
<p>Install the app in your workspace.
<img src="https://github.com/seratch/ChatGPT-in-Slack/assets/2420543/cf84d743-6ccd-41a5-9b7e-41591b6d4939" alt="Screenshot 5"></p>
</li>
<li>
<p>Create an app level token with the <code>connection:write</code> scope. Save this token as <code>SLACK_APP_TOKEN</code>.
<img src="https://github.com/seratch/ChatGPT-in-Slack/assets/2420543/997573a6-8fb7-4357-b811-bdd01e52b158" alt="Screenshot 6">
<img src="https://github.com/seratch/ChatGPT-in-Slack/assets/2420543/763b1854-2b5a-4690-b5fa-a17fd720f40d" alt="Screenshot 7"></p>
</li>
<li>
<p>Obtain the OAuth token by going to OAuth &amp; Permissions and copying the OAuth Token. Use this token as <code>SLACK_BOT_TOKEN</code>.</p>
<p><img src="https://github.com/seratch/ChatGPT-in-Slack/assets/2420543/ce8eff39-305c-482c-98c6-ed9a03994b3b" alt="Screenshot 8">
<img src="https://github.com/seratch/ChatGPT-in-Slack/assets/2420543/96f2abe6-9bae-47f5-b08b-c569c138cf00" alt="Screenshot 9"></p>
</li>
</ol>
<h3 id="modifying-the-env-file">Modifying the .env File</h3>
<p>Follow these steps to modify the .env file:</p>
<ol>
<li>
<p>Copy the example env file using the following command:</p>
<pre tabindex="0"><code>cp -rfv .env.example .env
</code></pre></li>
<li>
<p>Open the .env file and update the values of <code>SLACK_APP_TOKEN</code> and <code>SLACK_BOT_TOKEN</code> with the tokens generated in the previous steps.</p>
</li>
<li>
<p>Additionally, if needed, modify the URL of the website to be indexed and set it as the value for <code>SITEMAP</code> in the .env file.</p>
</li>
</ol>
<h3 id="running-with-docker-compose">Running with Docker Compose</h3>
<p>To run the bot using Docker Compose, follow these steps.</p>
<p>Run the following command if you&rsquo;re using Docker and <code>docker-compose</code>:</p>
<pre tabindex="0"><code>docker-compose up
</code></pre><p>If you&rsquo;re running Docker with <code>docker compose</code>, use the following command:</p>
<pre tabindex="0"><code>docker compose up
</code></pre><p>By default, the local-ai setup will prepare and use the gpt4all-j model, which should work for most cases. However, if you want to change models, refer to the documentation or ask for assistance in the forums or Discord community.</p>
<h3 id="trying-it-out">Trying It Out!</h3>
<p>Once the bot starts successfully, you can ask it questions about the documentation in the designated channel. Check out this video for an example of how it works, including linking to the relevant sources in the documentation:</p>
<p><img src="https://github.com/spectrocloud-labs/Slack-QA-bot/assets/2420543/6047e1ff-22d5-4b03-9d73-fcb7fb19a2c1" alt="Kairos-TPM-Slackbot"></p>
<h3 id="bonus-setup-other-models">Bonus: Setup other models</h3>
<p>The <code>.env</code> file specifies to configure gpt4all automatically, however you can use other models by copying the manually in the models folder, or use the gallery:</p>
<pre tabindex="0"><code># See: https://github.com/go-skynet/model-gallery
PRELOAD_MODELS=[{&#34;url&#34;: &#34;github:go-skynet/model-gallery/gpt4all-j.yaml&#34;, &#34;name&#34;: &#34;gpt-3.5-turbo&#34;}]
</code></pre><p>The <code>PRELOAD_MODELS</code> environment variable in the <code>.env</code> file specifies the configuration for the <code>gpt-3.5-turbo</code> model. See also: <a href="https://github.com/go-skynet/model-gallery">https://github.com/go-skynet/model-gallery</a> in order to run other models from the gallery.</p>
<p>To run manually models, see the <code>chatbot-ui-manual</code> example in LocalAI, and comment the <code>PRELOAD_MODELS</code> environment variable.</p>
<h3 id="bonus-kubernetes-setup">Bonus: Kubernetes setup</h3>
<p>This is a manifest which can be used as a starting point:</p>
<pre tabindex="0"><code>apiVersion: v1
kind: Namespace
metadata:
  name: slack-bot
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: knowledgebase
  namespace: slack-bot
  labels:
    app: localai
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: localai
  namespace: slack-bot
  labels:
    app: localai
spec:
  selector:
    matchLabels:
      app: localai
  replicas: 1
  template:
    metadata:
      labels:
        app: localai
      name: localai
    spec:
      containers:
        - name: localai-slack
          env:
          - name: OPENAI_API_KEY
            value: &#34;x&#34;
          - name: SLACK_APP_TOKEN
            value: &#34;xapp-1-&#34;
          - name: SLACK_BOT_TOKEN
            value: &#34;xoxb-&#34;
          - name: OPENAI_MODEL
            value: &#34;gpt-3.5-turbo&#34;
          - name: OPENAI_TIMEOUT_SECONDS
            value: &#34;400&#34;
          - name: OPENAI_SYSTEM_TEXT
            value: &#34;&#34;
          - name: MEMORY_DIR
            value: &#34;/memory&#34;
          - name: TRANSLATE_MARKDOWN
            value: &#34;true&#34;
          - name: OPENAI_API_BASE
            value: &#34;http://local-ai.default.svc.cluster.local:8080&#34;
          - name: REPOSITORIES
            value: &#34;KAIROS,AGENT,SDK,OSBUILDER,PACKAGES,IMMUCORE&#34;
          - name: KAIROS_CLONE_URL
            value: &#34;https://github.com/kairos-io/kairos&#34;
          - name: KAIROS_BRANCH
            value: &#34;master&#34;
          - name: AGENT_CLONE_URL
            value: &#34;https://github.com/kairos-io/kairos-agent&#34;
          - name: AGENT_BRANCH
            value: &#34;main&#34;
          - name: SDK_CLONE_URL
            value: &#34;https://github.com/kairos-io/kairos-sdk&#34;
          - name: SDK_BRANCH
            value: &#34;main&#34;
          - name: OSBUILDER_CLONE_URL
            value: &#34;https://github.com/kairos-io/osbuilder&#34;
          - name: OSBUILDER_BRANCH
            value: &#34;master&#34;
          - name: PACKAGES_CLONE_URL
            value: &#34;https://github.com/kairos-io/packages&#34;
          - name: PACKAGES_BRANCH
            value: &#34;main&#34;
          - name: IMMUCORE_CLONE_URL
            value: &#34;https://github.com/kairos-io/immucore&#34;
          - name: IMMUCORE_BRANCH
            value: &#34;master&#34;
          - name: GITHUB_PERSONAL_ACCESS_TOKEN
            value: &#34;&#34;
          - name: ISSUE_REPOSITORIES
            value: &#34;kairos-io/kairos&#34;
          image: quay.io/spectrocloud-labs/slack-qa-local-bot:qa
          imagePullPolicy: Always
          volumeMounts:
            - mountPath: &#34;/memory&#34;
              name: knowledgebase
      volumes:
        - name: knowledgebase
          persistentVolumeClaim:
            claimName: knowledgebase
</code></pre><p>Note:</p>
<ul>
<li><code>OPENAI_API_BASE</code> is set to the default if installing the <code>local-ai</code> chart into the default namespace listening on 8080. Specify a different LocalAI url here.</li>
</ul>
<h2 id="about-the-author">About the Author</h2>
<p>I&rsquo;m the creator of LocalAI and I&rsquo;ve been contributing to Free Open Source software for almost 15 years, I&rsquo;ve been working at SUSE and now I&rsquo;m working at SpectroCloud.</p>
<h3 id="stay-updated">Stay updated</h3>
<p>If you want to stay-up-to-date on my latest posts or what I am to follow me on Twitter at <a href="https://twitter.com/mudler_it/">@mudler_it</a> and on <a href="https://github.com/mudler">Github</a>.</p>

    <a href="/"> >> Home</a>
  </main>
</div>
    <footer>
      
      
      <script>
      (function() {
        function center_el(tagName) {
          var tags = document.getElementsByTagName(tagName), i, tag;
          for (i = 0; i < tags.length; i++) {
            tag = tags[i];
            var parent = tag.parentElement;
            
            if (parent.childNodes.length === 1) {
              
              if (parent.nodeName === 'A') {
                parent = parent.parentElement;
                if (parent.childNodes.length != 1) continue;
              }
              if (parent.nodeName === 'P') parent.style.textAlign = 'center';
            }
          }
        }
        var tagNames = ['img', 'embed', 'object'];
        for (var i = 0; i < tagNames.length; i++) {
          center_el(tagNames[i]);
        }
      })();
      </script>

      
      <hr/>
      <a href="https://github.com/mudler">GitHub</a> | <a href="https://twitter.com/mudler_it">X/Twitter</a> | <a href="https://www.linkedin.com/in/ettore-di-giacinto-211a4166/">LinkedIn</a>
      
    </footer>
  </body>
</html>

