<!DOCTYPE html>
<html lang="en">
  <head>

    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
    <link rel="manifest" href="/images/site.webmanifest">

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Mudler blog - Place where I write about stuff">
    <title>LocalAI and llama.cpp on Jetson Nano Devkit | Mudler blog</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link rel="stylesheet" href="https://mudler.pm/css/theme-override.css">
    
  </head>

  <body>
    <header>
      <nav>
        <ul>
          
          
          <li class="pull-left ">
            <a href="https://mudler.pm/">~/mudler blog</a>
          </li>
          
          
          <li class="pull-left ">
            <a href="/posts/">~/blog</a>
          </li>
          
          
          <li class="pull-left ">
            <a href="/about/">~/about</a>
          </li>
          
          
          <li class="pull-left ">
            <a href="/search/">~/search</a>
          </li>
          

          
          
          <li class="pull-right">
            <a href="/index.xml">~/subscribe</a>
          </li>
          

        </ul>
      </nav>
    </header>


<div class="article-meta">
<h1><span class="title">LocalAI and llama.cpp on Jetson Nano Devkit</span></h1>

<h2 class="date">2024/05/30</h2>
<p class="terms">
  
  
  
  
  
</p>
</div>



<div class="content-wrapper">
  <main>
    <p>If you are a lucky(?) owner of the Jetson Nano Devkit (4GB), and you don&rsquo;t know anymore on what to do with it, you can try to run LocalAI with llama.cpp on it.</p>
<p>The Jetson Nano Devkit is currently not supported anymore by Nvidia, and receives little to no attention, however, it can <em>still</em> do something, and if you are like me that recycles the board at home, you might want to have fun with it by running AI on top of it. It might be also good candidate for the workload distribution features that LocalAI has (see: <a href="https://localai.io/features/distribute/%29">https://localai.io/features/distribute/)</a>, but leaving that for another post.</p>
<p>Disclaimer: you aren&rsquo;t going to run big models with it, but phi-3 runs and you can have fun with it!</p>
<p>I&rsquo;ll leave to another post for the setup I got with the Orin AGX, this covers for now only the Jetson Nano devkit as I recently did these steps to prepare a cluster to show-off on one of my upcoming talks!</p>
<p><img src="https://github.com/mudler/blog/assets/2420543/f7c050b6-3b08-4652-99d1-51438a1a7300" alt="Screenshot from 2024-05-31 19-16-18"></p>
<h2 id="requirements">Requirements</h2>
<p>First of all, you need to get the latest Jetson Nano Devkit tooling that is - by now - it is 32.7.4.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>wget https://developer.nvidia.com/downloads/embedded/l4t/r32_release_v7.4/t210/jetson-210_linux_r32.7.4_aarch64.tbz2
</span></span><span style="display:flex;"><span>tar -xvf jetson-210_linux_r32.7.4_aarch64.tbz2
</span></span><span style="display:flex;"><span>cd Linux_for_Tegra
</span></span></code></pre></div><p>Then you need to flash the device with the latest image, so we need a rootfs:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>wget https://developer.nvidia.com/downloads/embedded/l4t/r32_release_v7.4/t210/tegra_linux_sample-root-filesystem_r32.7.4_aarch64.tbz2
</span></span><span style="display:flex;"><span>mkdir rootfs
</span></span><span style="display:flex;"><span>tar -xvf tegra_linux_sample-root-filesystem_r32.7.4_aarch64.tbz2 -C rootfs
</span></span></code></pre></div><p>Reminder: you need to prepare the rootfs by running (and requires QEMU with aarch64 support):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo ./apply_binaries.sh
</span></span></code></pre></div><p>Then you can flash the device with the following command:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo ./flash.sh jetson-nano-devkit-emmc mmcblk0p1
</span></span></code></pre></div><p>Make sure to put the device in recovery mode before running the flash command (notes below).</p>
<h3 id="enter-recovery-mode">Enter recovery mode</h3>
<ul>
<li>Jumper the J48 power select pin first and plug the power jack (optional, depend on devkit board)</li>
<li>Jumper the recovery pin and the reset pin. In my case it was on the edge of the Compute module.</li>
<li>Plug the MicroUSB cable to the PC</li>
<li>Plug the power to the board (DC on the board)</li>
</ul>
<h4 id="verify-you-are-in-recovery-mode">Verify you are in recovery mode</h4>
<p>You should be able to see the Nvidia device as <code>NVIDIA Corp. APX</code> by running “lsusb” command on your host.</p>
<pre tabindex="0"><code># lsusb
Bus 004 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub
Bus 003 Device 004: ID 0a5c:5842 Broadcom Corp. 58200
Bus 003 Device 003: ID 0bda:554c Realtek Semiconductor Corp. Integrated_Webcam_FHD
Bus 003 Device 079: ID 0955:7f21 NVIDIA Corp. APX
Bus 003 Device 005: ID 8087:0033 Intel Corp. 
Bus 003 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
Bus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub
Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
</code></pre><h2 id="cleanup">Cleanup</h2>
<p>We might need some extra space, especially if you are booting from the eMMC like me. We can remove all the graphical packages and docs like this on a booted system, but this might be a bit of a delicate process, beware and make sure you know what you are doing before running this command:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo apt-get remove ubuntu-desktop gnome-* libreoffice*
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">while</span> read p; <span style="color:#66d9ef">do</span>
</span></span><span style="display:flex;"><span>  sudo apt-get remove -y <span style="color:#e6db74">&#34;</span>$p<span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">done</span> &lt; &lt;<span style="color:#f92672">(</span>curl -L https://raw.githubusercontent.com/NVIDIA-AI-IOT/jetson-min-disk/main/assets/nvubuntu-bionic-packages_only-in-desktop.txt<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>sudo apt-get install network-manager
</span></span><span style="display:flex;"><span>sudo dpkg -r --force-depends <span style="color:#e6db74">&#34;cuda-documentation-10-2&#34;</span> <span style="color:#e6db74">&#34;cuda-samples-10-2&#34;</span> <span style="color:#e6db74">&#34;libnvinfer-samples&#34;</span> <span style="color:#e6db74">&#34;libvisionworks-samples&#34;</span> <span style="color:#e6db74">&#34;libnvinfer-doc&#34;</span> <span style="color:#e6db74">&#34;vpi1-samples&#34;</span>
</span></span></code></pre></div><h2 id="install-build-dependencies">Install Build dependencies</h2>
<p>The packages on the Jetson Nano are really old ( we are talking about gcc 7 ! ) as this is an Ubuntu 18.04.</p>
<p>We need then to install few things manually to go ahead:</p>
<h3 id="cmake">Cmake</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Install CMAKE: </span>
</span></span><span style="display:flex;"><span>wget https://github.com/Kitware/CMake/releases/download/v3.26.4/cmake-3.26.4.tar.gz
</span></span><span style="display:flex;"><span>tar xvf cmake-3.26.4.tar.gz
</span></span><span style="display:flex;"><span>cd cmake-3.26.4
</span></span><span style="display:flex;"><span>./configure
</span></span><span style="display:flex;"><span>make
</span></span><span style="display:flex;"><span>make install
</span></span></code></pre></div><h3 id="gcc-8">GCC-8</h3>
<p>We need gcc 8 for <code>-std=c++20</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo add-apt-repository ppa:ubuntu-toolchain-r/test
</span></span><span style="display:flex;"><span>sudo apt-get install -y gcc-8 g++-8
</span></span></code></pre></div><p>You might need to setup alternatives, just in case ( I didn&rsquo;t had to ):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e">#Remove the previous alternatives</span>
</span></span><span style="display:flex;"><span>sudo update-alternatives --remove-all gcc
</span></span><span style="display:flex;"><span>sudo update-alternatives --remove-all g++
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#Define the compiler</span>
</span></span><span style="display:flex;"><span>sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-8 <span style="color:#ae81ff">30</span>
</span></span><span style="display:flex;"><span>sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-8 <span style="color:#ae81ff">30</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sudo update-alternatives --install /usr/bin/cc cc /usr/bin/gcc <span style="color:#ae81ff">30</span>
</span></span><span style="display:flex;"><span>sudo update-alternatives --set cc /usr/bin/gcc
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sudo update-alternatives --install /usr/bin/c++ c++ /usr/bin/g++ <span style="color:#ae81ff">30</span>
</span></span><span style="display:flex;"><span>sudo update-alternatives --set c++ /usr/bin/g++
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#Confirm and update (You can use the default setting)</span>
</span></span><span style="display:flex;"><span>sudo update-alternatives --config gcc
</span></span><span style="display:flex;"><span>sudo update-alternatives --config g++
</span></span></code></pre></div><h3 id="install-cuda">Install cuda</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo apt-get install -y nvidia-cuda zlib1g-dev
</span></span></code></pre></div><h3 id="install-protoc">Install protoc</h3>
<p>Protoc in repositories is old! get the latest one:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl -L -s https://github.com/protocolbuffers/protobuf/releases/download/v26.1/protoc-26.1-linux-aarch_64.zip -o protoc.zip <span style="color:#f92672">&amp;&amp;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>unzip -j -d /usr/local/bin protoc.zip bin/protoc <span style="color:#f92672">&amp;&amp;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>rm protoc.zip
</span></span></code></pre></div><h3 id="install-golang">Install golang</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>wget https://go.dev/dl/go1.22.3.linux-arm64.tar.gz
</span></span><span style="display:flex;"><span>sudo tar -C /usr/local -xzf go1.22.3.linux-amd64.tar.gz
</span></span><span style="display:flex;"><span>echo <span style="color:#e6db74">&#34;export PATH=</span>$PATH<span style="color:#e6db74">:/usr/local/go/bin:</span>$HOME<span style="color:#e6db74">/go/bin:/usr/local/cuda/bin&#34;</span> &gt;&gt; ~/.bashrc
</span></span><span style="display:flex;"><span>source ~/.bashrc
</span></span></code></pre></div><h2 id="install-localai">Install LocalAI</h2>
<p>We are going to build LocalAI from source, as the prebuilt binaries are not available for the Jetson Nano and CUDA. The Jetson nano uses CUDA 10.2.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git clone https://github.com/mudler/LocalAI
</span></span><span style="display:flex;"><span>cd LocalAI
</span></span><span style="display:flex;"><span>BUILD_GRPC_FOR_BACKEND_LLAMA<span style="color:#f92672">=</span>true GRPC_BACKENDS<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;backend-assets/grpc/llama-cpp&#34;</span> BUILD_TYPE<span style="color:#f92672">=</span>cublas GO_TAGS<span style="color:#f92672">=</span>p2p make build
</span></span></code></pre></div><p>We are going to build only the llama-cpp backend (I did not try the others for now).</p>
<p>Its going to take a while, grab a coffee and wait for the build to finish.</p>
<p>&hellip; and boom, it fails compiling ?!.</p>
<p>At this point, you will need to patch few things out as llama.cpp with gcc 8 on aarch64 does not seem to compile see (upstream issue at <a href="https://github.com/ggerganov/llama.cpp/issues/7147">https://github.com/ggerganov/llama.cpp/issues/7147</a> ) :</p>
<pre tabindex="0"><code>diff --git a/ggml-cuda/fattn-common.cuh b/ggml-cuda/fattn-common.cuh
index 1dd519bd..c363bc9f 100644
--- a/ggml-cuda/fattn-common.cuh
+++ b/ggml-cuda/fattn-common.cuh
@@ -52,7 +52,7 @@ static __global__ void flash_attn_combine_results(
     dst       +=                 D * gridDim.y*blockIdx.x;
 
     const int tid = threadIdx.x;
-    __builtin_assume(tid &lt; D);
+    //__builtin_assume(tid &lt; D);
 
     __shared__ float2 meta[parallel_blocks];
     if (tid &lt; 2*parallel_blocks) {
diff --git a/ggml-cuda/fattn-vec-f16.cu b/ggml-cuda/fattn-vec-f16.cu
index 808e8f36..2bbb39bd 100644
--- a/ggml-cuda/fattn-vec-f16.cu
+++ b/ggml-cuda/fattn-vec-f16.cu
@@ -59,7 +59,7 @@ static __global__ void flash_attn_vec_ext_f16(
     static_assert(D % (2*WARP_SIZE) == 0, &#34;D not divisible by 2*WARP_SIZE == 64.&#34;);
     constexpr int nwarps = D / WARP_SIZE;
     const int tid = WARP_SIZE*threadIdx.y + threadIdx.x;
-    __builtin_assume(tid &lt; D);
+    //__assume(tid &lt; D);
 
     __shared__ half KQ[ncols*D];
 #pragma unroll
diff --git a/ggml-cuda/fattn-vec-f32.cu b/ggml-cuda/fattn-vec-f32.cu
index b4652301..500239b8 100644
--- a/ggml-cuda/fattn-vec-f32.cu
+++ b/ggml-cuda/fattn-vec-f32.cu
@@ -57,7 +57,7 @@ static __global__ void flash_attn_vec_ext_f32(
     static_assert(D % (2*WARP_SIZE) == 0, &#34;D not divisible by 2*WARP_SIZE == 64.&#34;);
     constexpr int nwarps = D / WARP_SIZE;
     const int tid = WARP_SIZE*threadIdx.y + threadIdx.x;
-    __builtin_assume(tid &lt; D);
+//#__builtin_assume(tid &lt; D);
 
     __shared__ float KQ[ncols*D];
 #pragma unroll
diff --git a/ggml-impl.h b/ggml-impl.h
index 5e77471f..1ad54af8 100644
--- a/ggml-impl.h
+++ b/ggml-impl.h
@@ -382,15 +382,15 @@ inline static uint8x16_t ggml_vqtbl1q_u8(uint8x16_t a, uint8x16_t b) {
 
 #define ggml_int16x8x2_t  int16x8x2_t
 #define ggml_uint8x16x2_t uint8x16x2_t
-#define ggml_uint8x16x4_t uint8x16x4_t
+#define ggml_uint8x16x4_t uint8x16x2_t
 #define ggml_int8x16x2_t  int8x16x2_t
-#define ggml_int8x16x4_t  int8x16x4_t
+#define ggml_int8x16x4_t  int8x16x2_t
 
 #define ggml_vld1q_s16_x2 vld1q_s16_x2
 #define ggml_vld1q_u8_x2  vld1q_u8_x2
-#define ggml_vld1q_u8_x4  vld1q_u8_x4
+#define ggml_vld1q_u8_x4  vld1q_u8_x2
 #define ggml_vld1q_s8_x2  vld1q_s8_x2
-#define ggml_vld1q_s8_x4  vld1q_s8_x4
+#define ggml_vld1q_s8_x4  vld1q_s8_x2
 #define ggml_vqtbl1q_s8   vqtbl1q_s8
 #define ggml_vqtbl1q_u8   vqtbl1q_u8
</code></pre><p>Apply the patch in <code>LocalAI/backend/cpp/llama-cpp/llama.cpp/ggml/src</code>.</p>
<h2 id="result">Result</h2>
<p>Now you should have a binary, <code>local-ai</code>, and you can run phi-3 with:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>./local-ai run https://gist.githubusercontent.com/mudler/86dbff5fdf46e993b81dd366a679ea32/raw/20e6416719e86f99954b8ea7a26b1aa680db6f59/phi-3-mini-jetson.yaml
</span></span></code></pre></div><p>Example output:</p>
<pre tabindex="0"><code>6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: freq_base_train  = 10000.0
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: freq_scale_train = 1
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: n_yarn_orig_ctx  = 2048
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: rope_finetuned   = unknown
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: ssm_d_conv       = 0
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: ssm_d_inner      = 0
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: ssm_d_state      = 0
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: ssm_dt_rank      = 0
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: model type       = 3B
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: model ftype      = Q4_K - Medium
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: model params     = 2.78 B
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: model size       = 1.62 GiB (5.00 BPW)
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: general.name     = Phi2
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: BOS token        = 50256 &#39;&lt;|endoftext|&gt;&#39;
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: EOS token        = 50256 &#39;&lt;|endoftext|&gt;&#39;
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: UNK token        = 50256 &#39;&lt;|endoftext|&gt;&#39;
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: PAD token        = 50256 &#39;&lt;|endoftext|&gt;&#39;
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: LF token         = 128 &#39;Ä&#39;
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_print_meta: EOT token        = 50256 &#39;&lt;|endoftext|&gt;&#39;
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr ggml_cuda_init: found 1 CUDA devices:
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr   Device 0: NVIDIA Tegra X1, compute capability 5.3, VMM: no
6:43PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_tensors: ggml ctx size =    0.42 MiB
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_tensors: offloading 32 repeating layers to GPU
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_tensors: offloading non-repeating layers to GPU
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_tensors: offloaded 33/33 layers to GPU
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_tensors:        CPU buffer size =    70.31 MiB
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llm_load_tensors:      CUDA0 buffer size =  1585.10 MiB
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr ..........................................................................................
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llama_new_context_with_model: n_ctx      = 4096
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llama_new_context_with_model: n_batch    = 512
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llama_new_context_with_model: n_ubatch   = 512
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llama_new_context_with_model: flash_attn = 0
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llama_new_context_with_model: freq_base  = 10000.0
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llama_new_context_with_model: freq_scale = 1
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llama_kv_cache_init:      CUDA0 KV buffer size =  1280.00 MiB
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llama_new_context_with_model: KV self size  = 1280.00 MiB, K (f16):  640.00 MiB, V (f16):  640.00 MiB
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llama_new_context_with_model:  CUDA_Host  output buffer size =     0.20 MiB
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llama_new_context_with_model:      CUDA0 compute buffer size =   284.00 MiB
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llama_new_context_with_model:  CUDA_Host compute buffer size =    13.01 MiB
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llama_new_context_with_model: graph nodes  = 1161
6:45PM DBG GRPC(phi-2-layla-v1-chatml-Q4_K.gguf-127.0.0.1:40757): stderr llama_new_context_with_model: graph splits = 2
</code></pre>
    <a href="/"> >> Home</a>
  </main>
</div>
    <footer>
      
      
      <script>
      (function() {
        function center_el(tagName) {
          var tags = document.getElementsByTagName(tagName), i, tag;
          for (i = 0; i < tags.length; i++) {
            tag = tags[i];
            var parent = tag.parentElement;
            
            if (parent.childNodes.length === 1) {
              
              if (parent.nodeName === 'A') {
                parent = parent.parentElement;
                if (parent.childNodes.length != 1) continue;
              }
              if (parent.nodeName === 'P') parent.style.textAlign = 'center';
            }
          }
        }
        var tagNames = ['img', 'embed', 'object'];
        for (var i = 0; i < tagNames.length; i++) {
          center_el(tagNames[i]);
        }
      })();
      </script>

      
      <hr/>
      <a href="https://github.com/mudler">GitHub</a> | <a href="https://twitter.com/mudler_it">X/Twitter</a> | <a href="https://www.linkedin.com/in/ettore-di-giacinto-211a4166/">LinkedIn</a>
      
    </footer>
  </body>
</html>

